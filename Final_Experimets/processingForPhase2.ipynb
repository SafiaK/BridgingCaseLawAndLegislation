{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src folder to Python path\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Load environment variables from src/.env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(os.path.dirname(os.getcwd()), 'src', '.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#need to put all csvs in one place \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#final desitination is ../data/final_test/case_csvs\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#load the csv final\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df_final \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/final_test/combined_cases_final.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#need to put all csvs in one place \n",
    "i\n",
    "\n",
    "#final desitination is ../data/final_test/case_csvs\n",
    "\n",
    "#load the csv final\n",
    "df_final = pd.read_csv('../data/final_test/combined_cases_final.csv')\n",
    "\n",
    "#load the csv with the given url and the file path \n",
    "source_file = '/Users/apple/Documents/Swansea/Projects/combined_sampled_caselaws.csv'\n",
    "#from. sourcefile. makeadictionaryofcase_urisource_full_path\n",
    "#from source_file, get the file path and the url\n",
    "#for every unique case_uri in df_final, find the row in statsfile with the same case_uri\n",
    "#if found, copy the file path and the url to the final destination\n",
    "#if not found, print the case_uri\n",
    "\n",
    "#rename the files with the url removing the baseurl(https://caselaw.nationalarchives.gov.uk) for example https://caselaw.nationalarchives.gov.uk/ewhc/ch/2009/1229 will become ewhc_ch_2009_1229.csv\n",
    "#the case_uri thats final source is not found print them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading final CSV...\n",
      "Loading source CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/ylm2sy9s6ygbm9wjf__vgfqw0000gn/T/ipykernel_91320/1875538606.py:19: DtypeWarning: Columns (6,7,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,516) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_source = pd.read_csv(source_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating case_uri to file path mapping...\n",
      "Found 414 case URIs in source file\n",
      "Found 443 unique case URIs in final dataframe\n",
      "✓ Copied: ewhc_ch_2009_1229.csv\n",
      "✓ Copied: ewhc_ch_2022_2112.csv\n",
      "✓ Copied: ewhc_comm_2006_777.csv\n",
      "✓ Copied: ewhc_fam_2005_1832.csv\n",
      "✓ Copied: ewhc_admin_2024_3297.csv\n",
      "✓ Copied: ewhc_qb_2003_3555.csv\n",
      "✓ Copied: ewhc_fam_2018_3244.csv\n",
      "✓ Copied: ewhc_admin_2014_3699.csv\n",
      "✓ Copied: ewhc_admin_2017_3059.csv\n",
      "✓ Copied: ewhc_admin_2015_2868.csv\n",
      "✓ Copied: ewhc_admin_2015_1942.csv\n",
      "✓ Copied: ewhc_qb_2017_3087.csv\n",
      "✓ Copied: ewhc_costs_2003_9050.csv\n",
      "✓ Copied: ewhc_scco_2022_3354.csv\n",
      "✓ Copied: ewhc_admin_2005_2977.csv\n",
      "✓ Copied: ewhc_ch_2023_123.csv\n",
      "✓ Copied: ewhc_admin_2009_2348.csv\n",
      "✓ Copied: ewhc_admin_2003_1419.csv\n",
      "✓ Copied: ewhc_qb_2021_157.csv\n",
      "✓ Copied: ewhc_scco_2022_1778.csv\n",
      "✓ Copied: ewhc_admin_2009_2674.csv\n",
      "✓ Copied: ewhc_admin_2014_2900.csv\n",
      "✓ Copied: ewhc_pat_2005_2240.csv\n",
      "✓ Copied: ewhc_admin_2012_1033.csv\n",
      "✓ Copied: ewhc_admin_2013_1818.csv\n",
      "✓ Copied: ewhc_ch_2014_763.csv\n",
      "✓ Copied: ewhc_fam_2018_3795.csv\n",
      "✓ Copied: ewhc_admin_2024_1780.csv\n",
      "✓ Copied: ewhc_admin_2024_2661.csv\n",
      "✓ Copied: ewhc_admin_2020_1905.csv\n",
      "✓ Copied: ewhc_comm_2015_3248.csv\n",
      "✓ Copied: ewhc_ch_2011_3782.csv\n",
      "✓ Copied: ewhc_admin_2022_967.csv\n",
      "✓ Copied: ewhc_fam_2015_455.csv\n",
      "✓ Copied: ewhc_ch_2023_175.csv\n",
      "✓ Copied: ewhc_ch_2023_4.csv\n",
      "✓ Copied: ewhc_ch_2005_3438.csv\n",
      "✓ Copied: ewhc_admin_2023_62.csv\n",
      "✓ Copied: ewhc_admin_2007_1304.csv\n",
      "✓ Copied: ewhc_ch_2014_1688.csv\n",
      "✓ Copied: ewhc_ch_2020_3622.csv\n",
      "✓ Copied: ewhc_ch_2013_47.csv\n",
      "✓ Copied: ewhc_admin_2013_218.csv\n",
      "✓ Copied: ewhc_ipec_2024_2478.csv\n",
      "✓ Copied: ewhc_admin_2004_2240.csv\n",
      "✗ Error copying ewhc_kb_2023_1256.csv: stat: path should be string, bytes, os.PathLike or integer, not float\n",
      "✓ Copied: ewhc_ch_2016_876.csv\n",
      "✓ Copied: ewhc_ch_2016_1076.csv\n",
      "✓ Copied: ewhc_admin_2006_2643.csv\n",
      "✓ Copied: ewhc_comm_2020_3334.csv\n",
      "✓ Copied: ewhc_qb_2022_1968.csv\n",
      "✓ Copied: ewhc_pat_2015_1094.csv\n",
      "✗ Source file not found: /Users/apple/Documents/Swansea/Projects/Sampled_Caselaws_400/ewhc/Chedington Events Limited v Nihal Mohammed Kamal Brake & Anor (Quantum trial consequential orders).csv\n",
      "✓ Copied: ewhc_admin_2015_3084.csv\n",
      "✓ Copied: ewhc_ch_2017_1727.csv\n",
      "✓ Copied: ewhc_ch_2024_560.csv\n",
      "✓ Copied: ewhc_qb_2011_272.csv\n",
      "✓ Copied: ewhc_admin_2024_2864.csv\n",
      "✓ Copied: ewhc_qb_2020_1689.csv\n",
      "✓ Copied: ewhc_qb_2015_1760.csv\n",
      "✓ Copied: ewhc_ch_2018_2877.csv\n",
      "✓ Copied: ewhc_admin_2005_2363.csv\n",
      "✓ Copied: ewhc_ch_2012_858.csv\n",
      "✓ Copied: ewhc_comm_2020_2736.csv\n",
      "✓ Copied: ewhc_ch_2012_4090.csv\n",
      "✓ Copied: ewhc_ch_2022_1348.csv\n",
      "✓ Copied: ewhc_qb_2022_977.csv\n",
      "✓ Copied: ewhc_qb_2011_179.csv\n",
      "✓ Copied: ewhc_qb_2011_4.csv\n",
      "✓ Copied: ewhc_fam_2014_4643.csv\n",
      "✓ Copied: ewhc_admin_2009_771.csv\n",
      "✓ Copied: ewhc_qb_2009_1900.csv\n",
      "✓ Copied: ewhc_admin_2005_591.csv\n",
      "✓ Copied: ewhc_fam_2017_917.csv\n",
      "✓ Copied: ewhc_admin_2017_1818.csv\n",
      "✓ Copied: ewhc_ch_2009_74.csv\n",
      "✓ Copied: ewhc_admin_2014_1106.csv\n",
      "✓ Copied: ewhc_qb_2013_797.csv\n",
      "✓ Copied: ewhc_admin_2012_1867.csv\n",
      "✓ Copied: ewhc_ch_2006_2612.csv\n",
      "✓ Copied: ewhc_ch_2022_2033.csv\n",
      "✓ Copied: ewhc_ch_2022_740.csv\n",
      "✓ Copied: ewhc_qb_2018_2135.csv\n",
      "✓ Copied: ewhc_ch_2015_3172.csv\n",
      "✓ Copied: ewhc_admin_2011_754.csv\n",
      "✓ Copied: ewhc_ch_2019_704.csv\n",
      "✓ Copied: ewhc_comm_2020_2012.csv\n",
      "✓ Copied: ewhc_comm_2015_2748.csv\n",
      "✓ Copied: ewhc_admin_2009_1404.csv\n",
      "✓ Copied: ewhc_admin_2014_3627.csv\n",
      "✓ Copied: ewhc_admin_2009_634.csv\n",
      "✓ Copied: ewhc_admin_2018_1371.csv\n",
      "✓ Copied: ewhc_admin_2009_590.csv\n",
      "✓ Copied: ewhc_admin_2004_563.csv\n",
      "✓ Copied: ewhc_admin_2004_1069.csv\n",
      "✓ Copied: ewhc_admin_2013_1009.csv\n",
      "✓ Copied: ewhc_ch_2019_3339.csv\n",
      "✓ Copied: ewhc_fam_2005_402.csv\n",
      "✓ Copied: ewhc_ch_2021_1275.csv\n",
      "✓ Copied: ewhc_admin_2009_3614.csv\n",
      "✓ Copied: ewhc_ch_2017_769.csv\n",
      "✓ Copied: ewhc_tcc_2004_2991.csv\n",
      "✓ Copied: ewhc_admin_2020_2579.csv\n",
      "✓ Copied: ewhc_admin_2009_3412.csv\n",
      "✓ Copied: ewhc_kb_2023_3472.csv\n",
      "✓ Copied: ewhc_fam_2004_1066.csv\n",
      "✓ Copied: ewhc_admin_2003_1578.csv\n",
      "✓ Copied: ewhc_admin_2006_2784.csv\n",
      "✓ Copied: ewhc_comm_2005_2115.csv\n",
      "✓ Copied: ewhc_admin_2019_84.csv\n",
      "✓ Copied: ewhc_comm_2023_1889.csv\n",
      "✓ Copied: ewhc_admin_2020_801.csv\n",
      "✓ Copied: ewhc_admin_2013_240.csv\n",
      "✓ Copied: ewhc_admin_2019_3480.csv\n",
      "✓ Copied: ewhc_fam_2017_3164.csv\n",
      "✓ Copied: ewhc_admin_2019_62.csv\n",
      "✓ Copied: ewhc_fam_2014_3135.csv\n",
      "✓ Copied: ewhc_tcc_2019_2212.csv\n",
      "✓ Copied: ewhc_admin_2012_1098.csv\n",
      "✓ Copied: ewhc_comm_2018_330.csv\n",
      "✓ Copied: ewhc_admin_2020_1522.csv\n",
      "✓ Copied: ewhc_admin_2011_2962.csv\n",
      "✓ Copied: ewhc_admin_2006_3014.csv\n",
      "✓ Copied: ewhc_comm_2023_391.csv\n",
      "✓ Copied: ewhc_kb_2023_965.csv\n",
      "✓ Copied: ewhc_ch_2005_1075.csv\n",
      "✓ Copied: ewhc_ch_2020_3295.csv\n",
      "✓ Copied: ewhc_qb_2018_1948.csv\n",
      "✓ Copied: ewhc_qb_2015_1060.csv\n",
      "✓ Copied: ewhc_admin_2006_1346.csv\n",
      "✓ Copied: ewhc_admin_2008_2013.csv\n",
      "✓ Copied: ewhc_admin_2003_1321.csv\n",
      "✓ Copied: ewhc_comm_2012_50.csv\n",
      "✓ Copied: ewhc_comm_2006_134.csv\n",
      "✓ Copied: ewhc_admin_2008_1043.csv\n",
      "✓ Copied: ewhc_admin_2020_2352.csv\n",
      "✓ Copied: ewhc_admin_2011_2317.csv\n",
      "✓ Copied: ewhc_admin_2011_441.csv\n",
      "✓ Copied: ewhc_qb_2020_718.csv\n",
      "✓ Copied: ewhc_ch_2012_28.csv\n",
      "✓ Copied: ewhc_admin_2024_3016.csv\n",
      "✓ Copied: ewhc_comm_2021_286.csv\n",
      "✓ Copied: ewhc_admin_2012_882.csv\n",
      "✓ Copied: ewhc_ch_2005_1508.csv\n",
      "✓ Copied: ewhc_admin_2005_2278.csv\n",
      "✓ Copied: ewhc_admin_2003_1837.csv\n",
      "✓ Copied: ewhc_admin_2013_512.csv\n",
      "✓ Copied: ewhc_pat_2014_4242.csv\n",
      "✓ Copied: ewhc_admin_2013_1786.csv\n",
      "✓ Copied: ewhc_admin_2009_535.csv\n",
      "✓ Copied: ewhc_fam_2017_1782.csv\n",
      "✓ Copied: ewhc_qb_2017_294.csv\n",
      "✓ Copied: ewhc_tcc_2012_2593.csv\n",
      "✓ Copied: ewhc_admin_2019_1809.csv\n",
      "✓ Copied: ewhc_fam_2023_1096.csv\n",
      "✓ Copied: ewhc_admin_2006_3048.csv\n",
      "✓ Copied: ewhc_admin_2008_738.csv\n",
      "✓ Copied: ewhc_ch_2012_3266.csv\n",
      "✓ Copied: ewhc_ch_2003_2845.csv\n",
      "✓ Copied: ewhc_tcc_2009_1664.csv\n",
      "✓ Copied: ewhc_fam_2014_6.csv\n",
      "✓ Copied: ewhc_comm_2004_1752.csv\n",
      "✓ Copied: ewhc_ch_2010_1951.csv\n",
      "✓ Copied: ewhc_admin_2010_439.csv\n",
      "✓ Copied: ewhc_admin_2007_807.csv\n",
      "✓ Copied: ewhc_admin_2020_3243.csv\n",
      "✓ Copied: ewhc_qb_2015_926.csv\n",
      "✓ Copied: ewhc_comm_2011_1372.csv\n",
      "✓ Copied: ewhc_ch_2010_938.csv\n",
      "✓ Copied: ewhc_comm_2008_1785.csv\n",
      "✓ Copied: ewhc_ch_2025_46.csv\n",
      "✓ Copied: ewhc_ch_2018_2169.csv\n",
      "✓ Copied: ewhc_admin_2015_1603.csv\n",
      "✓ Copied: ewhc_admin_2012_2806.csv\n",
      "✓ Copied: ewhc_admin_2019_2233.csv\n",
      "✓ Copied: ewhc_admin_2013_195.csv\n",
      "✓ Copied: ewhc_admin_2004_611.csv\n",
      "✓ Copied: ewhc_qb_2010_100.csv\n",
      "✓ Copied: ewhc_admin_2012_3226.csv\n",
      "✓ Copied: ewhc_admin_2018_2651.csv\n",
      "✓ Copied: ewhc_admin_2014_2109.csv\n",
      "✓ Copied: ewhc_admin_2003_2698.csv\n",
      "✓ Copied: ewhc_ch_2017_1770.csv\n",
      "✓ Copied: ewhc_admin_2006_3346.csv\n",
      "✓ Copied: ewhc_admin_2016_2186.csv\n",
      "✓ Copied: ewhc_comm_2017_1430.csv\n",
      "✓ Copied: ewhc_ch_2023_568.csv\n",
      "✓ Copied: ewhc_ipec_2019_126.csv\n",
      "✓ Copied: ewhc_admin_2005_1393.csv\n",
      "✓ Copied: ewhc_ch_2023_264.csv\n",
      "✓ Copied: ewhc_tcc_2011_87.csv\n",
      "✓ Copied: ewhc_qb_2019_3340.csv\n",
      "✓ Copied: ewhc_admin_2018_1092.csv\n",
      "✓ Copied: ewhc_ch_2010_180.csv\n",
      "✓ Copied: ewhc_fam_2018_3841.csv\n",
      "✓ Copied: ewhc_fam_2021_1153.csv\n",
      "✓ Copied: ewhc_admin_2011_2943.csv\n",
      "✓ Copied: ewhc_ch_2016_2759.csv\n",
      "✓ Copied: ewhc_admin_2004_430.csv\n",
      "✓ Copied: ewhc_ch_2019_2517.csv\n",
      "✓ Copied: ewhc_admin_2008_470.csv\n",
      "✓ Copied: ewhc_ch_2018_2344.csv\n",
      "✓ Copied: ewhc_comm_2019_3292.csv\n",
      "✓ Copied: ewhc_qb_2003_1814.csv\n",
      "✓ Copied: ewhc_ch_2015_3910.csv\n",
      "✓ Copied: ewhc_qb_2017_2554.csv\n",
      "✗ Error copying ewhc_admin_2009_3189.csv: stat: path should be string, bytes, os.PathLike or integer, not float\n",
      "✓ Copied: ewhc_comm_2023_2866.csv\n",
      "✓ Copied: ewhc_fam_2010_1346.csv\n",
      "✓ Copied: ewhc_ch_2020_2624.csv\n",
      "✓ Copied: ewhc_admin_2015_1641.csv\n",
      "✓ Copied: ewhc_ch_2019_1839.csv\n",
      "✓ Copied: ewhc_admin_2011_2855.csv\n",
      "✓ Copied: ewhc_comm_2023_910.csv\n",
      "✓ Copied: ewhc_fam_2022_2677.csv\n",
      "✓ Copied: ewhc_admin_2016_3693.csv\n",
      "✓ Copied: ewhc_fam_2022_2120.csv\n",
      "✓ Copied: ewhc_comm_2017_3430.csv\n",
      "✓ Copied: ewhc_ch_2020_1363.csv\n",
      "✓ Copied: ewhc_ch_2017_2621.csv\n",
      "✓ Copied: ewhc_ch_2004_2947.csv\n",
      "✓ Copied: ewhc_ch_2012_731.csv\n",
      "✓ Copied: ewhc_admin_2013_3250.csv\n",
      "✓ Copied: ewhc_qb_2022_1917.csv\n",
      "✓ Copied: ewcop_2020_14.csv\n",
      "✓ Copied: ewcop_2019_10.csv\n",
      "✓ Copied: ewcop_2015_13.csv\n",
      "✓ Copied: ewcop_2014_2.csv\n",
      "✓ Copied: ewcop_2021_32.csv\n",
      "✓ Copied: ewcop_2020_23.csv\n",
      "✓ Copied: ewca_civ_2011_1172.csv\n",
      "✓ Copied: ewca_crim_2016_1941.csv\n",
      "✓ Copied: ewca_civ_2015_1419.csv\n",
      "✓ Copied: ewca_civ_2018_1105.csv\n",
      "✓ Copied: ewca_crim_2009_651.csv\n",
      "✓ Copied: ewca_civ_2023_652.csv\n",
      "✓ Copied: ewca_crim_2024_147.csv\n",
      "✓ Copied: ewca_crim_2005_1681.csv\n",
      "✓ Copied: ewca_civ_2023_1000.csv\n",
      "✓ Copied: ewca_crim_2009_469.csv\n",
      "✓ Copied: ewca_civ_2011_1081.csv\n",
      "✓ Copied: ewca_crim_2022_483.csv\n",
      "✓ Copied: ewca_civ_2016_1267.csv\n",
      "✓ Copied: ewca_crim_2008_854.csv\n",
      "✓ Copied: ewca_crim_2007_3021.csv\n",
      "✓ Copied: ewca_civ_2010_805.csv\n",
      "✓ Copied: ewca_crim_2010_591.csv\n",
      "✓ Copied: ewca_civ_2011_10.csv\n",
      "✓ Copied: ewca_crim_2010_1474.csv\n",
      "✓ Copied: ewca_civ_2014_826.csv\n",
      "✓ Copied: ewca_crim_2017_2314.csv\n",
      "✓ Copied: ewca_crim_2008_1745.csv\n",
      "✓ Copied: ewca_crim_2005_1881.csv\n",
      "✓ Copied: ewca_crim_2006_3335.csv\n",
      "✓ Copied: ewca_civ_2004_1269.csv\n",
      "✓ Copied: ewca_crim_2020_597.csv\n",
      "✓ Copied: ewca_civ_2014_163.csv\n",
      "✓ Copied: ewca_civ_2013_703.csv\n",
      "✓ Copied: ewca_civ_2008_1097.csv\n",
      "✓ Copied: ewca_crim_2019_281.csv\n",
      "✓ Copied: ewca_crim_2009_2204.csv\n",
      "✓ Copied: ewca_civ_2023_239.csv\n",
      "✓ Copied: ewca_civ_2017_438.csv\n",
      "✓ Copied: ewca_crim_2007_1165.csv\n",
      "✓ Copied: ewca_civ_2015_1230.csv\n",
      "✓ Copied: ewca_civ_2010_390.csv\n",
      "✓ Copied: ewca_crim_2007_3432.csv\n",
      "✓ Copied: ewca_civ_2022_1943.csv\n",
      "✓ Copied: ewca_crim_2013_2499.csv\n",
      "✓ Copied: ewca_crim_2007_3223.csv\n",
      "✓ Copied: ewca_crim_2012_1939.csv\n",
      "✓ Copied: ewca_civ_2006_140.csv\n",
      "✓ Copied: ewca_civ_2005_1570.csv\n",
      "✓ Copied: ewca_civ_2005_856.csv\n",
      "✓ Copied: ewca_civ_2012_51.csv\n",
      "✓ Copied: ewca_crim_2009_379.csv\n",
      "✓ Copied: ewca_civ_2009_454.csv\n",
      "✓ Copied: ewca_civ_2013_1116.csv\n",
      "✓ Copied: ewca_crim_2008_894.csv\n",
      "✓ Copied: ewca_civ_2017_2138.csv\n",
      "✓ Copied: ewca_crim_2022_1837.csv\n",
      "✓ Copied: ewca_crim_2018_1393.csv\n",
      "✓ Copied: ewca_crim_2017_2065.csv\n",
      "✓ Copied: ewca_civ_2020_620.csv\n",
      "✓ Copied: ewca_crim_2017_1971.csv\n",
      "✓ Copied: ewca_crim_2010_1450.csv\n",
      "✓ Copied: ewca_crim_2020_1455.csv\n",
      "✓ Copied: ewca_civ_2003_135.csv\n",
      "✓ Copied: ewca_civ_2018_162.csv\n",
      "✓ Copied: ewca_civ_2013_554.csv\n",
      "✓ Copied: ewca_civ_2006_529.csv\n",
      "✓ Copied: ewca_crim_2017_2063.csv\n",
      "✓ Copied: ewca_crim_2019_466.csv\n",
      "✓ Copied: ewca_crim_2006_2136.csv\n",
      "✓ Copied: ewca_crim_2007_2548.csv\n",
      "✓ Copied: ewca_crim_2024_313.csv\n",
      "✓ Copied: ewca_crim_2023_1121.csv\n",
      "✓ Copied: ewca_civ_2010_1468.csv\n",
      "✓ Copied: ewca_civ_2011_1539.csv\n",
      "✓ Copied: ewca_civ_2018_1841.csv\n",
      "✓ Copied: ewca_crim_2023_630.csv\n",
      "✓ Copied: ewca_civ_2012_1395.csv\n",
      "✓ Copied: ewca_crim_2013_1764.csv\n",
      "✓ Copied: ewca_crim_2006_757.csv\n",
      "✓ Copied: ewca_civ_2015_1085.csv\n",
      "✓ Copied: ewca_civ_2009_650.csv\n",
      "✓ Copied: ewca_crim_2009_201.csv\n",
      "✓ Copied: ewca_civ_2020_833.csv\n",
      "✓ Copied: ewca_crim_2017_1779.csv\n",
      "✓ Copied: ewca_civ_2015_1189.csv\n",
      "✓ Copied: ewca_civ_2014_935.csv\n",
      "✓ Copied: ewca_crim_2006_3301.csv\n",
      "✓ Copied: ewca_crim_2004_621.csv\n",
      "✓ Copied: ewca_crim_2010_1486.csv\n",
      "✓ Copied: ewca_civ_2015_515.csv\n",
      "✓ Copied: ewca_civ_2003_151.csv\n",
      "✓ Copied: ewca_civ_2017_20.csv\n",
      "✓ Copied: ewca_civ_2009_856.csv\n",
      "✓ Copied: ewca_crim_2005_1722.csv\n",
      "✓ Copied: ewca_crim_2013_2406.csv\n",
      "✓ Copied: ewca_crim_2006_707.csv\n",
      "✓ Copied: ewca_crim_2023_1520.csv\n",
      "✓ Copied: ewca_crim_2004_3092.csv\n",
      "✓ Copied: ewca_civ_2019_53.csv\n",
      "✓ Copied: ewca_civ_2016_775.csv\n",
      "✓ Copied: ewca_civ_2003_167.csv\n",
      "✓ Copied: ewca_crim_2022_50.csv\n",
      "✓ Copied: ewca_crim_2013_1026.csv\n",
      "✓ Copied: ewca_civ_2006_380.csv\n",
      "✓ Copied: ewca_crim_2015_1933.csv\n",
      "✓ Copied: ewca_crim_2007_36.csv\n",
      "✓ Copied: ewca_crim_2009_288.csv\n",
      "✓ Copied: ewca_crim_2008_2500.csv\n",
      "✓ Copied: ewca_civ_2021_763.csv\n",
      "✓ Copied: ewca_civ_2018_1808.csv\n",
      "✓ Copied: ewca_civ_2021_252.csv\n",
      "✓ Copied: ewca_civ_2005_1479.csv\n",
      "✓ Copied: ewca_crim_2004_492.csv\n",
      "✓ Copied: ewca_civ_2011_1515.csv\n",
      "✓ Copied: ewca_crim_2007_2847.csv\n",
      "✓ Copied: ewca_crim_2011_2651.csv\n",
      "✓ Copied: ewca_civ_2012_1660.csv\n",
      "✓ Copied: ewca_crim_2015_1256.csv\n",
      "✓ Copied: ewca_crim_2006_646.csv\n",
      "✓ Copied: ewca_civ_2005_385.csv\n",
      "✓ Copied: ewca_civ_2007_1175.csv\n",
      "✓ Copied: ewca_civ_2024_16.csv\n",
      "✓ Copied: ewca_civ_2014_1105.csv\n",
      "✓ Copied: ewca_crim_2003_190.csv\n",
      "✓ Copied: ewca_crim_2014_382.csv\n",
      "✓ Copied: ewca_civ_2021_113.csv\n",
      "✓ Copied: ewca_crim_2022_1428.csv\n",
      "✓ Copied: ewca_crim_2023_202.csv\n",
      "✓ Copied: ewca_crim_2017_1461.csv\n",
      "✓ Copied: ewca_crim_2023_1106.csv\n",
      "✓ Copied: ewca_civ_2015_718.csv\n",
      "✓ Copied: ewca_crim_2008_680.csv\n",
      "✓ Copied: ewca_crim_2018_2895.csv\n",
      "✓ Copied: ukftt_tc_2024_744.csv\n",
      "✓ Copied: ukftt_tc_2023_959.csv\n",
      "✓ Copied: ukftt_grc_2024_471.csv\n",
      "✓ Copied: ukftt_grc_2023_35.csv\n",
      "✓ Copied: ukftt_grc_2024_333.csv\n",
      "✓ Copied: ukftt_tc_2022_326.csv\n",
      "✓ Copied: ukftt_tc_2022_171.csv\n",
      "✓ Copied: ukftt_tc_2022_201.csv\n",
      "✓ Copied: ukftt_tc_2023_744.csv\n",
      "✓ Copied: ukftt_tc_2024_462.csv\n",
      "✓ Copied: ukftt_tc_2024_139.csv\n",
      "✓ Copied: ukftt_grc_2024_1087.csv\n",
      "✓ Copied: ukftt_grc_2022_415.csv\n",
      "✓ Copied: ewfc_2014_9.csv\n",
      "✓ Copied: ewfc_2022_34.csv\n",
      "✓ Copied: ewfc_b_2024_40.csv\n",
      "✓ Copied: ewfc_2024_6.csv\n",
      "✓ Copied: ewfc_2019_60.csv\n",
      "✓ Copied: ewfc_2016_25.csv\n",
      "✓ Copied: ewfc_b_2024_69.csv\n",
      "✓ Copied: uksc_2011_33.csv\n",
      "✓ Copied: uksc_2013_11.csv\n",
      "✓ Copied: uksc_2022_30.csv\n",
      "✓ Copied: uksc_2013_37.csv\n",
      "✓ Copied: uksc_2013_8.csv\n",
      "✓ Copied: ukut_tcc_2023_244.csv\n",
      "✓ Copied: ukut_lc_2022_342.csv\n",
      "✓ Copied: ukut_aac_2023_288.csv\n",
      "✗ Source file not found: /Users/apple/Documents/Swansea/Projects/Sampled_Caselaws_400/ukut/Aaron Shorr & Anor v London Borough of Camden.csv\n",
      "✓ Copied: ukut_aac_2016_355.csv\n",
      "✓ Copied: eat_2022_192.csv\n",
      "✓ Copied: eat_2023_162.csv\n",
      "✓ Copied: eat_2024_56.csv\n",
      "✓ Copied: eat_2023_70.csv\n",
      "✓ Copied: eat_2024_17.csv\n",
      "✓ Copied: ukpc_2023_36.csv\n",
      "✓ Copied: ukpc_2009_42.csv\n",
      "✓ Copied: ukpc_2013_21.csv\n",
      "✓ Copied: ukpc_2022_47.csv\n",
      "✓ Copied: ukpc_2011_16.csv\n",
      "✓ Copied: ukait_2009_41.csv\n",
      "✓ Copied: ukait_2009_54.csv\n",
      "✓ Copied: ukait_2009_43.csv\n",
      "✓ Copied: ukait_2009_40.csv\n",
      "✓ Copied: ukait_2008_1.csv\n",
      "✓ Copied: ewcc_2024_7.csv\n",
      "✓ Copied: ewhc_admin_2014_4153.csv\n",
      "✓ Copied: ewhc_admin_2004_3362.csv\n",
      "✓ Copied: ewca_crim_2008_1194.csv\n",
      "✓ Copied: ewca_crim_2005_3377.csv\n",
      "✓ Copied: ewca_civ_2005_1770.csv\n",
      "✓ Copied: ewca_crim_2006_229.csv\n",
      "✓ Copied: ukut_aac_2021_69.csv\n",
      "✓ Copied: ukut_iac_2015_268.csv\n",
      "\n",
      "==================================================\n",
      "SUMMARY\n",
      "==================================================\n",
      "Total unique case URIs in final CSV: 443\n",
      "Found in source mapping: 412\n",
      "Not found in source mapping: 31\n",
      "Successfully copied: 408\n",
      "Copy errors: 4\n",
      "\n",
      "Case URIs not found in source file (31):\n",
      "--------------------------------------------------\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/ch/2021/324\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/crim/2008/468\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2025/215\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/qb/2014/4729\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/scco/2025/374\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2006/4\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/admin/2017/576\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2005/647\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/qb/2016/2355\n",
      "  https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/287\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2004/988\n",
      "  https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/251\n",
      "  https://caselaw.nationalarchives.gov.uk/uksc/2011/41\n",
      "  https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/284\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/admin/2020/1850\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/ch/2013/4630\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2018/764\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/admin/2003/2779\n",
      "  https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/282\n",
      "  https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/283\n",
      "  https://caselaw.nationalarchives.gov.uk/ukut/aac/2022/263\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/crim/2019/2056\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2015/414\n",
      "  https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/289\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/admin/2025/462\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/admin/2010/2929\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2012/543\n",
      "  https://caselaw.nationalarchives.gov.uk/eat/2025/29\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2007/826\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/crim/2009/1942\n",
      "  https://caselaw.nationalarchives.gov.uk/ewfc/2025/41\n",
      "\n",
      "Files copied to: ../data/final_test/case_csvs\n",
      "\n",
      "Verifying copied files...\n",
      "Found 408 CSV files in destination\n",
      "Readable files: 408\n",
      "Error files: 0\n",
      "\n",
      "Process completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# File paths\n",
    "final_destination = '../data/final_test/case_csvs'\n",
    "final_df_path = '../data/final_test/combined_cases_final.csv'\n",
    "source_file = '/Users/apple/Documents/Swansea/Projects/combined_sampled_caselaws.csv'\n",
    "\n",
    "# Create destination directory if it doesn't exist\n",
    "os.makedirs(final_destination, exist_ok=True)\n",
    "\n",
    "# Load the final CSV\n",
    "print(\"Loading final CSV...\")\n",
    "df_final = pd.read_csv(final_df_path)\n",
    "\n",
    "# Load the source CSV with file paths\n",
    "print(\"Loading source CSV...\")\n",
    "df_source = pd.read_csv(source_file)\n",
    "\n",
    "# Create dictionary mapping case_uri to full file path\n",
    "print(\"Creating case_uri to file path mapping...\")\n",
    "case_uri_to_path = {}\n",
    "\n",
    "for idx, row in df_source.iterrows():\n",
    "    case_uri = row['case_uri']  # Adjust column name if different\n",
    "    file_path = row['source_full_path']  # Adjust column name if different\n",
    "    case_uri_to_path[case_uri] = file_path\n",
    "\n",
    "print(f\"Found {len(case_uri_to_path)} case URIs in source file\")\n",
    "\n",
    "# Get unique case URIs from final dataframe\n",
    "unique_case_uris = df_final['case_uri'].unique()\n",
    "print(f\"Found {len(unique_case_uris)} unique case URIs in final dataframe\")\n",
    "\n",
    "# Track statistics\n",
    "found_count = 0\n",
    "not_found_count = 0\n",
    "copy_success_count = 0\n",
    "copy_error_count = 0\n",
    "not_found_uris = []\n",
    "\n",
    "# Process each unique case URI\n",
    "for case_uri in unique_case_uris:\n",
    "    if case_uri in case_uri_to_path:\n",
    "        found_count += 1\n",
    "        source_path = case_uri_to_path[case_uri]\n",
    "        \n",
    "        # Generate destination filename by removing base URL and replacing / with _\n",
    "        base_url = 'https://caselaw.nationalarchives.gov.uk'\n",
    "        if case_uri.startswith(base_url):\n",
    "            # Remove base URL and leading slash\n",
    "            relative_path = case_uri[len(base_url):].lstrip('/')\n",
    "            # Replace slashes with underscores and add .csv extension\n",
    "            filename = relative_path.replace('/', '_') + '.csv'\n",
    "        else:\n",
    "            # Fallback: use the full URI with replacements\n",
    "            filename = case_uri.replace('https://', '').replace('http://', '').replace('/', '_') + '.csv'\n",
    "        \n",
    "        destination_path = os.path.join(final_destination, filename)\n",
    "        \n",
    "        # Copy the file\n",
    "        try:\n",
    "            if os.path.exists(source_path):\n",
    "                shutil.copy2(source_path, destination_path)\n",
    "                copy_success_count += 1\n",
    "                print(f\"✓ Copied: {filename}\")\n",
    "            else:\n",
    "                print(f\"✗ Source file not found: {source_path}\")\n",
    "                copy_error_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error copying {filename}: {str(e)}\")\n",
    "            copy_error_count += 1\n",
    "            \n",
    "    else:\n",
    "        not_found_count += 1\n",
    "        not_found_uris.append(case_uri)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total unique case URIs in final CSV: {len(unique_case_uris)}\")\n",
    "print(f\"Found in source mapping: {found_count}\")\n",
    "print(f\"Not found in source mapping: {not_found_count}\")\n",
    "print(f\"Successfully copied: {copy_success_count}\")\n",
    "print(f\"Copy errors: {copy_error_count}\")\n",
    "\n",
    "# Print not found URIs\n",
    "if not_found_uris:\n",
    "    print(f\"\\nCase URIs not found in source file ({len(not_found_uris)}):\")\n",
    "    print(\"-\" * 50)\n",
    "    for uri in not_found_uris:\n",
    "        print(f\"  {uri}\")\n",
    "\n",
    "print(f\"\\nFiles copied to: {final_destination}\")\n",
    "\n",
    "# Verify copied files\n",
    "print(\"\\nVerifying copied files...\")\n",
    "if os.path.exists(final_destination):\n",
    "    csv_files = [f for f in os.listdir(final_destination) if f.endswith('.csv')]\n",
    "    print(f\"Found {len(csv_files)} CSV files in destination\")\n",
    "    \n",
    "    readable_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for filename in csv_files:\n",
    "        filepath = os.path.join(final_destination, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            readable_count += 1\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"✗ {filename}: Error reading - {str(e)}\")\n",
    "    \n",
    "    print(f\"Readable files: {readable_count}\")\n",
    "    print(f\"Error files: {error_count}\")\n",
    "\n",
    "print(\"\\nProcess completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique case URIs in final dataframe: 443\n",
      "CSVs found: 408\n",
      "CSVs not found: 35\n",
      "Success rate: 92.10%\n",
      "\n",
      "Missing CSVs for these case URIs (35):\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/kb/2023/1256\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/ch/2024/11\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/admin/2009/3189\n",
      "  https://caselaw.nationalarchives.gov.uk/ukut/lc/2024/202\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/ch/2021/324\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/crim/2008/468\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2025/215\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/qb/2014/4729\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/scco/2025/374\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2006/4\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/admin/2017/576\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2005/647\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/qb/2016/2355\n",
      "  https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/287\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2004/988\n",
      "  https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/251\n",
      "  https://caselaw.nationalarchives.gov.uk/uksc/2011/41\n",
      "  https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/284\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/admin/2020/1850\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/ch/2013/4630\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2018/764\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/admin/2003/2779\n",
      "  https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/282\n",
      "  https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/283\n",
      "  https://caselaw.nationalarchives.gov.uk/ukut/aac/2022/263\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/crim/2019/2056\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2015/414\n",
      "  https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/289\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/admin/2025/462\n",
      "  https://caselaw.nationalarchives.gov.uk/ewhc/admin/2010/2929\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2012/543\n",
      "  https://caselaw.nationalarchives.gov.uk/eat/2025/29\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/civ/2007/826\n",
      "  https://caselaw.nationalarchives.gov.uk/ewca/crim/2009/1942\n",
      "  https://caselaw.nationalarchives.gov.uk/ewfc/2025/41\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the final dataframe\n",
    "df_final = pd.read_csv('../data/final_test/combined_cases_final.csv')\n",
    "final_destination = '../data/final_test/case_csvs'\n",
    "\n",
    "# Get unique case URIs from final dataframe\n",
    "unique_case_uris = df_final['case_uri'].unique()\n",
    "print(f\"Total unique case URIs in final dataframe: {len(unique_case_uris)}\")\n",
    "\n",
    "# Track statistics\n",
    "found_csvs = 0\n",
    "not_found_csvs = 0\n",
    "missing_case_uris = []\n",
    "\n",
    "# Check each case_uri\n",
    "for case_uri in unique_case_uris:\n",
    "    # Generate expected filename\n",
    "    base_url = 'https://caselaw.nationalarchives.gov.uk'\n",
    "    if case_uri.startswith(base_url):\n",
    "        relative_path = case_uri[len(base_url):].lstrip('/')\n",
    "        filename = relative_path.replace('/', '_') + '.csv'\n",
    "    else:\n",
    "        filename = case_uri.replace('https://', '').replace('http://', '').replace('/', '_') + '.csv'\n",
    "    \n",
    "    # Check if file exists\n",
    "    file_path = os.path.join(final_destination, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        found_csvs += 1\n",
    "    else:\n",
    "        not_found_csvs += 1\n",
    "        missing_case_uris.append(case_uri)\n",
    "\n",
    "# Print results\n",
    "print(f\"CSVs found: {found_csvs}\")\n",
    "print(f\"CSVs not found: {not_found_csvs}\")\n",
    "print(f\"Success rate: {(found_csvs/len(unique_case_uris)*100):.2f}%\")\n",
    "\n",
    "# Print missing case URIs\n",
    "if missing_case_uris:\n",
    "    print(f\"\\nMissing CSVs for these case URIs ({len(missing_case_uris)}):\")\n",
    "    for uri in missing_case_uris:\n",
    "        print(f\"  {uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Copied from alternative: ewhc_ch_2021_324.csv\n",
      "✓ Copied from alternative: ewca_crim_2008_468.csv\n",
      "✓ Copied from alternative: ewca_civ_2025_215.csv\n",
      "✓ Copied from alternative: ewhc_qb_2014_4729.csv\n",
      "✓ Copied from alternative: ewhc_scco_2025_374.csv\n",
      "✓ Copied from alternative: ewca_civ_2006_4.csv\n",
      "✓ Copied from alternative: ewhc_admin_2017_576.csv\n",
      "✓ Copied from alternative: ewca_civ_2005_647.csv\n",
      "✓ Copied from alternative: ewhc_qb_2016_2355.csv\n",
      "✓ Copied from alternative: ukftt_grc_2025_287.csv\n",
      "✓ Copied from alternative: ewca_civ_2004_988.csv\n",
      "✓ Copied from alternative: ukftt_grc_2025_251.csv\n",
      "✓ Copied from alternative: uksc_2011_41.csv\n",
      "✓ Copied from alternative: ukftt_grc_2025_284.csv\n",
      "✓ Copied from alternative: ewhc_admin_2020_1850.csv\n",
      "✓ Copied from alternative: ewhc_ch_2013_4630.csv\n",
      "✓ Copied from alternative: ewca_civ_2018_764.csv\n",
      "✓ Copied from alternative: ewhc_admin_2003_2779.csv\n",
      "✓ Copied from alternative: ukftt_grc_2025_282.csv\n",
      "✓ Copied from alternative: ukftt_grc_2025_283.csv\n",
      "✓ Copied from alternative: ukut_aac_2022_263.csv\n",
      "✓ Copied from alternative: ewca_crim_2019_2056.csv\n",
      "✓ Copied from alternative: ewca_civ_2015_414.csv\n",
      "✓ Copied from alternative: ukftt_grc_2025_289.csv\n",
      "✓ Copied from alternative: ewhc_admin_2025_462.csv\n",
      "✓ Copied from alternative: ewhc_admin_2010_2929.csv\n",
      "✓ Copied from alternative: ewca_civ_2012_543.csv\n",
      "✓ Copied from alternative: eat_2025_29.csv\n",
      "✓ Copied from alternative: ewca_civ_2007_826.csv\n",
      "✓ Copied from alternative: ewca_crim_2009_1942.csv\n",
      "✓ Copied from alternative: ewfc_2025_41.csv\n",
      "\n",
      "Found in alternative source: 31\n",
      "Successfully copied from alternative: 31\n",
      "Still missing: 4\n",
      "\n",
      "==================================================\n",
      "FINAL STATUS\n",
      "==================================================\n",
      "Total unique case URIs: 443\n",
      "CSVs found in destination: 443\n",
      "CSVs still missing: 0\n",
      "Final success rate: 100.00%\n"
     ]
    }
   ],
   "source": [
    "#other places to look for csv -- ../data/test2/csv_cases\n",
    "#csv cases would be named as they should be \n",
    "\n",
    "# Now look for missing ones in alternative source\n",
    "found_in_alternative = 0\n",
    "copied_from_alternative = 0\n",
    "still_missing = []\n",
    "alternative_source = '../data/test2/csv_cases'\n",
    "\n",
    "\n",
    "for case_uri in missing_case_uris:\n",
    "    # Generate expected filename\n",
    "    base_url = 'https://caselaw.nationalarchives.gov.uk'\n",
    "    if case_uri.startswith(base_url):\n",
    "        relative_path = case_uri[len(base_url):].lstrip('/')\n",
    "        filename = relative_path.replace('/', '_') + '.csv'\n",
    "    else:\n",
    "        filename = case_uri.replace('https://', '').replace('http://', '').replace('/', '_') + '.csv'\n",
    "    \n",
    "    # Check if file exists in alternative source\n",
    "    alternative_path = os.path.join(alternative_source, filename)\n",
    "    destination_path = os.path.join(final_destination, filename)\n",
    "    \n",
    "    if os.path.exists(alternative_path):\n",
    "        found_in_alternative += 1\n",
    "        try:\n",
    "            shutil.copy2(alternative_path, destination_path)\n",
    "            copied_from_alternative += 1\n",
    "            print(f\"✓ Copied from alternative: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error copying {filename}: {str(e)}\")\n",
    "            still_missing.append(case_uri)\n",
    "    else:\n",
    "        still_missing.append(case_uri)\n",
    "\n",
    "print(f\"\\nFound in alternative source: {found_in_alternative}\")\n",
    "print(f\"Successfully copied from alternative: {copied_from_alternative}\")\n",
    "print(f\"Still missing: {len(still_missing)}\")\n",
    "\n",
    "# Final check - recount what we have now\n",
    "final_found = 0\n",
    "final_missing = 0\n",
    "final_missing_uris = []\n",
    "\n",
    "for case_uri in unique_case_uris:\n",
    "    # Generate expected filename\n",
    "    base_url = 'https://caselaw.nationalarchives.gov.uk'\n",
    "    if case_uri.startswith(base_url):\n",
    "        relative_path = case_uri[len(base_url):].lstrip('/')\n",
    "        filename = relative_path.replace('/', '_') + '.csv'\n",
    "    else:\n",
    "        filename = case_uri.replace('https://', '').replace('http://', '').replace('/', '_') + '.csv'\n",
    "    \n",
    "    # Check if file exists in destination\n",
    "    file_path = os.path.join(final_destination, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        final_found += 1\n",
    "    else:\n",
    "        final_missing += 1\n",
    "        final_missing_uris.append(case_uri)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL STATUS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total unique case URIs: {len(unique_case_uris)}\")\n",
    "print(f\"CSVs found in destination: {final_found}\")\n",
    "print(f\"CSVs still missing: {final_missing}\")\n",
    "print(f\"Final success rate: {(final_found/len(unique_case_uris)*100):.2f}%\")\n",
    "\n",
    "# Print still missing case URIs\n",
    "if final_missing_uris:\n",
    "    print(f\"\\nStill missing CSVs for these case URIs ({len(final_missing_uris)}):\")\n",
    "    for uri in final_missing_uris:\n",
    "        print(f\"  {uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases with legislation references: 430\n",
      "\n",
      "Sample of loaded data (first 5 entries):\n",
      "\n",
      "Case: ukftt_tc_2022_201\n",
      "Legislation references: ['id/ukpga/1994/23']\n",
      "\n",
      "Case: ewhc_admin_2011_2317\n",
      "Legislation references: ['id/ukpga/1973/18', 'id/ukpga/1989/41', 'id/ukpga/1994/33', 'id/ukpga/1995/50', 'id/ukpga/1996/52', 'id/ukpga/2004/31', 'id/ukpga/2005/13', 'id/ukpga/2009/11']\n",
      "\n",
      "Case: ewcop_2014_2\n",
      "Legislation references: ['id/ukpga/1983/19', 'id/ukpga/1983/20', 'id/ukpga/1989/41', 'id/ukpga/2005/9']\n",
      "\n",
      "Case: ewca_civ_2009_856\n",
      "Legislation references: ['id/ukpga/2002/41']\n",
      "\n",
      "Case: ewhc_ch_2012_4090\n",
      "Legislation references: ['id/ukpga/1986/45', 'id/ukpga/Geo5/4-5/59']\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "# Load the cleaned case legislation map from pickle file\n",
    "with open('../data/final_test/case_csvs/cleaned_case_legislation_map.pkl', 'rb') as f:\n",
    "    cleaned_case_legislation_map = pkl.load(f)\n",
    "\n",
    "# Print some basic statistics about the loaded data\n",
    "print(f\"Number of cases with legislation references: {len(cleaned_case_legislation_map)}\")\n",
    "\n",
    "# Print a sample of the data (first 5 entries)\n",
    "print(\"\\nSample of loaded data (first 5 entries):\")\n",
    "for i, (case, legislation_list) in enumerate(cleaned_case_legislation_map.items()):\n",
    "    if i >= 5:  # Only show first 5 entries\n",
    "        break\n",
    "    print(f\"\\nCase: {case}\")\n",
    "    print(f\"Legislation references: {legislation_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ukftt_tc_2022_201', 'ewhc_admin_2011_2317', 'ewcop_2014_2', 'ewca_civ_2009_856', 'ewhc_ch_2012_4090', 'ewhc_admin_2019_2233', 'ewhc_ch_2009_1229', 'ewhc_ch_2021_324', 'ewca_civ_2014_1105', 'ewhc_ch_2022_740', 'ewhc_admin_2009_3614', 'ewhc_ch_2017_1727', 'ewca_crim_2008_468', 'ewhc_fam_2014_6', 'ewca_civ_2020_620', 'ewhc_admin_2011_441', 'ewca_crim_2012_1939', 'ewca_crim_2007_1165', 'ewhc_qb_2014_4729', 'ewca_crim_2019_281', 'ewhc_admin_2011_2855', 'ewhc_comm_2023_1889', 'ewhc_admin_2012_2806', 'ewhc_admin_2006_3346', 'ewca_crim_2023_1106', 'ewhc_admin_2015_1603', 'ewhc_qb_2019_3340', 'ewhc_admin_2013_195', 'ewhc_admin_2009_1404', 'ewca_civ_2012_51', 'ewca_civ_2010_1468', 'ewhc_ch_2018_2344', 'ewca_crim_2004_3092', 'ewhc_admin_2024_1780', 'ewhc_ipec_2024_2478', 'ewca_civ_2017_2138', 'ewhc_tcc_2019_2212', 'ewca_crim_2006_2136', 'ewhc_scco_2022_3354', 'ukut_lc_2024_202', 'ukpc_2011_16', 'ewca_crim_2009_651', 'ewca_crim_2024_147', 'ewhc_ch_2012_28', 'ewhc_admin_2003_1837', 'ewhc_admin_2014_2900', 'ewhc_ch_2016_876', 'ewca_civ_2006_4', 'ewhc_admin_2014_2109', 'ewca_crim_2013_2406', 'ewca_crim_2017_1461', 'ewhc_ch_2012_3266', 'ewca_civ_2018_1808', 'ewhc_ch_2019_3339', 'ewhc_admin_2005_2977', 'ewca_crim_2008_1194', 'ewhc_ch_2011_3782', 'ewhc_qb_2022_1968', 'uksc_2011_33', 'ewhc_scco_2022_1778', 'ukftt_tc_2022_171', 'ewhc_qb_2009_1900', 'ewca_civ_2005_1570', 'ewhc_ch_2024_11', 'ewhc_admin_2006_1346', 'ewca_crim_2008_1745', 'ewcop_2019_10', 'ewhc_admin_2016_3693', 'ewca_civ_2015_718', 'ewhc_admin_2013_218', 'ewhc_ch_2023_568', 'ewca_crim_2022_50', 'ewca_crim_2005_1681', 'ewhc_ch_2016_2759', 'ewhc_admin_2009_535', 'ewhc_admin_2017_576', 'ewca_crim_2005_1722', 'uksc_2013_37', 'ewca_civ_2023_652', 'ewca_civ_2011_10', 'ewca_civ_2003_167', 'ewhc_fam_2015_455', 'ewca_civ_2011_1081', 'ewhc_ch_2025_46', 'ewhc_qb_2010_100', 'ewhc_qb_2018_1948', 'ewcop_2015_13', 'ewhc_ch_2013_47', 'ewhc_ch_2020_1363', 'ewhc_admin_2023_62', 'ewhc_comm_2023_910', 'ewca_crim_2003_190', 'ewhc_tcc_2004_2991', 'ewca_civ_2006_140', 'ewhc_admin_2014_4153', 'ewhc_ch_2015_3172', 'ewhc_admin_2009_3189', 'ewca_civ_2005_1770', 'ewhc_qb_2011_272', 'ewhc_ch_2018_2169', 'ewca_crim_2008_2500', 'ewca_civ_2006_380', 'ewca_crim_2009_469', 'ewhc_comm_2020_2736', 'ewhc_fam_2018_3244', 'ewhc_admin_2012_1098', 'ewhc_admin_2009_3412', 'ewhc_ch_2005_1075', 'ewhc_qb_2021_157', 'ewhc_fam_2021_1153', 'ewhc_admin_2024_2661', 'ewca_civ_2005_647', 'ewhc_admin_2019_62', 'ewca_crim_2011_2651', 'ewhc_ch_2014_1688', 'ewhc_ch_2017_769', 'ewca_civ_2018_1841', 'ukpc_2022_47', 'ewhc_ch_2020_2624', 'ewhc_qb_2016_2355', 'ukut_tcc_2023_244', 'ukftt_grc_2025_287', 'ewca_crim_2013_1026', 'ewhc_admin_2014_3699', 'ewhc_admin_2013_240', 'ewhc_admin_2011_2943', 'ewca_civ_2013_1116', 'ewca_civ_2004_988', 'ewhc_comm_2006_134', 'ukftt_grc_2025_251', 'ewhc_admin_2004_2240', 'ewca_crim_2013_2499', 'ewhc_comm_2021_286', 'ewhc_admin_2011_754', 'ewhc_admin_2004_611', 'ewca_crim_2006_3301', 'ewhc_ch_2005_3438', 'eat_2023_162', 'ewhc_admin_2004_3362', 'ukftt_grc_2022_415', 'ewhc_admin_2014_1106', 'ewhc_ch_2023_123', 'ewhc_comm_2017_1430', 'ewhc_ch_2022_2112', 'ewhc_comm_2015_3248', 'uksc_2011_41', 'ewhc_admin_2019_1809', 'ewca_crim_2015_1256', 'ukftt_grc_2025_284', 'ewca_crim_2010_591', 'ewhc_admin_2020_2579', 'ewca_crim_2018_2895', 'ewhc_qb_2020_718', 'ewca_crim_2018_1393', 'eat_2024_17', 'ewhc_admin_2009_590', 'ewhc_pat_2015_1094', 'ewca_crim_2022_1428', 'ewca_civ_2005_385', 'ewhc_fam_2022_2120', 'ewhc_admin_2020_1850', 'ewhc_admin_2024_3297', 'ukftt_grc_2024_333', 'ewcop_2020_23', 'ewhc_admin_2020_2352', 'ewca_crim_2023_202', 'ewhc_kb_2023_1256', 'ewhc_qb_2022_977', 'ewhc_costs_2003_9050', 'ewfc_2022_34', 'ewhc_comm_2017_3430', 'ewhc_qb_2003_1814', 'ewhc_fam_2014_3135', 'ewhc_ch_2013_4630', 'ewhc_ch_2019_704', 'ewca_civ_2013_703', 'ewhc_qb_2003_3555', 'ewhc_qb_2017_294', 'ewca_civ_2018_764', 'ewfc_2016_25', 'ewhc_qb_2017_2554', 'ukftt_tc_2024_744', 'ewfc_2019_60', 'ewhc_comm_2023_391', 'ewca_civ_2018_162', 'ewhc_admin_2009_634', 'ukut_lc_2022_342', 'ewca_civ_2021_252', 'ewca_civ_2018_1105', 'ewca_crim_2010_1474', 'ewhc_admin_2013_1818', 'ewhc_admin_2003_2779', 'ewhc_admin_2005_2278', 'ewhc_admin_2004_563', 'ewca_crim_2006_707', 'ewca_crim_2009_379', 'ukftt_grc_2025_282', 'ewhc_admin_2018_2651', 'ewca_crim_2009_2204', 'ewhc_fam_2018_3795', 'ewca_civ_2012_1660', 'ewca_crim_2016_1941', 'ewhc_ch_2018_2877', 'ewhc_admin_2018_1371', 'ewhc_ch_2006_2612', 'ewhc_ch_2024_560', 'ewhc_ch_2021_1275', 'ewca_civ_2015_1189', 'ewhc_fam_2017_3164', 'ukut_aac_2016_355', 'ewca_crim_2020_1455', 'ukut_iac_2015_268', 'ewca_crim_2022_1837', 'ewhc_fam_2017_917', 'ukut_aac_2022_263', 'ewca_crim_2006_3335', 'ewca_crim_2010_1450', 'ewhc_kb_2023_3472', 'ewhc_admin_2012_3226', 'ewhc_ch_2019_2517', 'eat_2022_192', 'ewca_crim_2009_201', 'ewhc_admin_2020_1522', 'ewhc_comm_2020_3334', 'ewca_civ_2017_20', 'ewhc_admin_2013_1009', 'ewhc_admin_2011_2962', 'ewhc_admin_2013_512', 'ewca_crim_2017_1971', 'ewhc_ch_2017_1770', 'ewhc_admin_2005_1393', 'ewhc_admin_2003_1321', 'ewhc_tcc_2009_1664', 'ewca_civ_2014_163', 'ewca_civ_2015_1230', 'ewca_crim_2010_1486', 'ewfc_2014_9', 'ewca_crim_2019_2056', 'ukftt_grc_2024_471', 'ewca_civ_2003_135', 'ukftt_grc_2024_1087', 'ewhc_admin_2015_1641', 'ewhc_admin_2004_430', 'ewhc_ch_2022_1348', 'ewhc_ch_2020_3295', 'ewhc_ch_2010_938', 'ewca_civ_2015_414', 'ewca_crim_2017_2063', 'ewca_crim_2014_382', 'ewhc_admin_2008_2013', 'ewhc_fam_2017_1782', 'ewhc_admin_2009_771', 'ewca_civ_2011_1172', 'ewhc_comm_2005_2115', 'ewhc_admin_2020_1905', 'ewhc_admin_2008_1043', 'ewhc_qb_2011_4', 'ewhc_admin_2010_439', 'ewca_civ_2023_239', 'ewhc_fam_2014_4643', 'ewca_crim_2024_313', 'ewhc_admin_2013_1786', 'ewhc_ipec_2019_126', 'ewhc_admin_2020_3243', 'ewhc_comm_2023_2866', 'ewca_civ_2006_529', 'ewca_civ_2007_1175', 'ewca_crim_2017_2314', 'ewhc_admin_2015_3084', 'ewhc_admin_2009_2348', 'ewhc_fam_2022_2677', 'ewhc_admin_2017_1818', 'ewhc_admin_2012_882', 'ewhc_qb_2017_3087', 'ewhc_comm_2004_1752', 'ewhc_comm_2015_2748', 'ewhc_ch_2019_1839', 'ewhc_qb_2015_1760', 'ewhc_admin_2012_1033', 'ewhc_admin_2007_1304', 'ewca_civ_2010_805', 'ewca_crim_2017_1779', 'ewca_civ_2015_1419', 'ewca_crim_2004_621', 'ewhc_admin_2022_967', 'ewhc_ch_2023_264', 'ewhc_fam_2005_1832', 'ukftt_grc_2025_289', 'ewca_civ_2004_1269', 'ewca_civ_2021_113', 'ewca_civ_2010_390', 'ewhc_qb_2022_1917', 'ewhc_qb_2011_179', 'ewca_crim_2007_2548', 'ewca_civ_2024_16', 'ewca_civ_2016_1267', 'uksc_2013_8', 'ewca_crim_2005_3377', 'ukut_aac_2021_69', 'ewca_civ_2014_826', 'ewca_crim_2006_646', 'ukut_aac_2023_288', 'ewca_civ_2005_1479', 'ewhc_admin_2006_3048', 'ewca_crim_2017_2065', 'ewhc_tcc_2011_87', 'ewca_crim_2023_630', 'ewca_crim_2013_1764', 'ukait_2008_1', 'ukftt_grc_2023_35', 'ewhc_admin_2005_591', 'ewhc_pat_2005_2240', 'ewhc_pat_2014_4242', 'ewca_crim_2015_1933', 'ukpc_2023_36', 'ewhc_comm_2020_2012', 'ewhc_ch_2009_74', 'ewhc_admin_2006_2784', 'ewhc_ch_2015_3910', 'ewcop_2020_14', 'ewhc_fam_2005_402', 'ewhc_admin_2017_3059', 'ewhc_comm_2011_1372', 'ewhc_admin_2019_84', 'ewhc_ch_2005_1508', 'ewfc_2024_6', 'uksc_2013_11', 'ewhc_admin_2016_2186', 'ewhc_admin_2024_3016', 'ewhc_ch_2003_2845', 'ukait_2009_41', 'ewfc_b_2024_69', 'ewhc_ch_2022_2033', 'ewhc_admin_2003_1419', 'ewhc_admin_2005_2363', 'ewca_crim_2023_1520', 'ewhc_ch_2014_763', 'ewhc_admin_2010_2929', 'ewca_civ_2020_833', 'ewca_crim_2020_597', 'ewca_crim_2007_36', 'eat_2024_56', 'ewca_civ_2017_438', 'ukftt_tc_2024_462', 'ewca_crim_2007_3223', 'ewca_civ_2015_515', 'ewhc_ch_2010_1951', 'ewca_civ_2019_53', 'ewca_crim_2005_1881', 'ewhc_qb_2018_2135', 'ukait_2009_40', 'ukait_2009_54', 'ewfc_b_2024_40', 'ewca_crim_2007_3021', 'ewcop_2021_32', 'ewca_civ_2012_1395', 'ewca_civ_2014_935', 'ewca_civ_2008_1097', 'ewhc_admin_2003_1578', 'ewca_civ_2023_1000', 'ewca_crim_2009_288', 'ewhc_admin_2009_2674', 'uksc_2022_30', 'ewhc_admin_2013_3250', 'ukftt_tc_2023_959', 'ewca_civ_2016_775', 'ukpc_2009_42', 'ewhc_admin_2019_3480', 'ewhc_fam_2018_3841', 'ewhc_admin_2015_2868', 'ewhc_ch_2023_175', 'ewca_civ_2022_1943', 'ewca_civ_2011_1515', 'ewca_crim_2006_757', 'ewhc_ch_2023_4', 'ewhc_qb_2015_1060', 'ewhc_admin_2006_2643', 'ewca_civ_2012_543', 'ewhc_admin_2014_3627', 'ewhc_ch_2012_731', 'ewhc_comm_2008_1785', 'ewhc_comm_2019_3292', 'ewhc_ch_2016_1076', 'ewca_crim_2019_466', 'ukftt_tc_2024_139', 'ukait_2009_43', 'ewca_crim_2004_492', 'ewca_civ_2015_1085', 'ewhc_ch_2017_2621', 'ewhc_qb_2013_797', 'ewca_crim_2007_3432', 'ewhc_admin_2008_470', 'ewhc_comm_2018_330', 'ewhc_qb_2015_926', 'ewca_civ_2009_454', 'ewhc_fam_2010_1346', 'ewca_civ_2021_763', 'ewhc_comm_2006_777', 'ewhc_admin_2006_3014', 'ewhc_ch_2010_180', 'ewca_civ_2007_826', 'eat_2023_70', 'ewhc_admin_2007_807', 'ewca_crim_2007_2847', 'ewca_crim_2009_1942', 'ewca_crim_2008_894', 'ewhc_admin_2020_801', 'ewca_civ_2011_1539', 'ewhc_admin_2024_2864', 'ewhc_ch_2004_2947', 'ewhc_ch_2012_858', 'ewhc_tcc_2012_2593', 'ewca_crim_2023_1121', 'ewhc_fam_2004_1066', 'ewhc_admin_2004_1069', 'ewhc_qb_2020_1689', 'ewhc_admin_2003_2698', 'ewca_civ_2013_554', 'ewca_crim_2022_483', 'ewhc_kb_2023_965', 'ewhc_ch_2020_3622', 'ewcc_2024_7', 'ewhc_fam_2023_1096', 'ewhc_admin_2008_738', 'ewca_crim_2008_854', 'ewhc_admin_2018_1092', 'ewfc_2025_41', 'ewhc_admin_2012_1867', 'ukpc_2013_21', 'ewca_civ_2003_151'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_case_legislation_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "430"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_case_legislation_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CSV files: 443\n",
      "Cases in legislation map: 430\n",
      "Missing cases: 13\n",
      "\n",
      "CSV files without legislation map entries:\n",
      "  ewca_crim_2008_680.csv\n",
      "  ewca_civ_2025_215.csv\n",
      "  ewhc_scco_2025_374.csv\n",
      "  ewhc_comm_2012_50.csv\n",
      "  ewhc_admin_2015_1942.csv\n",
      "  ewca_crim_2006_229.csv\n",
      "  ukftt_grc_2025_283.csv\n",
      "  ewhc_admin_2025_462.csv\n",
      "  ukftt_tc_2022_326.csv\n",
      "  ewca_civ_2009_650.csv\n",
      "  eat_2025_29.csv\n",
      "  ewca_civ_2005_856.csv\n",
      "  ukftt_tc_2023_744.csv\n"
     ]
    }
   ],
   "source": [
    "# Get list of CSV files in the directory\n",
    "csv_files = [f for f in os.listdir('../data/final_test/case_csvs') if f.endswith('.csv')]\n",
    "\n",
    "# Get list of case names from the cleaned_case_legislation_map\n",
    "map_cases = list(cleaned_case_legislation_map.keys())\n",
    "\n",
    "# Find CSV files that don't have corresponding entries in the map\n",
    "missing_cases = []\n",
    "for csv_file in csv_files:\n",
    "    case_name = csv_file.replace('.csv', '')\n",
    "    if case_name not in map_cases:\n",
    "        missing_cases.append(csv_file)\n",
    "\n",
    "# Print results\n",
    "print(f\"Total CSV files: {len(csv_files)}\")\n",
    "print(f\"Cases in legislation map: {len(map_cases)}\")\n",
    "print(f\"Missing cases: {len(missing_cases)}\")\n",
    "\n",
    "if missing_cases:\n",
    "    print(\"\\nCSV files without legislation map entries:\")\n",
    "    for case in missing_cases:\n",
    "        print(f\"  {case}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: ['Case1', 'Case2', 'Case4']\n",
      "Cluster 2: ['Case3', 'Case5']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def intelligent_grouping_with_kmeans(case_dict, n_clusters,max_cases_per_group=100):\n",
    "    \"\"\"Use k-means clustering to group cases and split groups that exceed the max_cases_per_group.\"\"\"\n",
    "    \n",
    "    # Step 1: Prepare the matrix of acts for each case law\n",
    "    case_laws = list(case_dict.keys())\n",
    "    all_acts = set(act for acts in case_dict.values() for act in acts)\n",
    "    \n",
    "    # Create a binary matrix where rows represent case laws and columns represent acts\n",
    "    act_to_index = {act: idx for idx, act in enumerate(all_acts)}\n",
    "    case_matrix = np.zeros((len(case_laws), len(all_acts)))\n",
    "    \n",
    "    for i, case in enumerate(case_laws):\n",
    "        for act in case_dict[case]:\n",
    "            case_matrix[i, act_to_index[act]] = 1  # Mark act presence\n",
    "    \n",
    "    # Step 2: Use k-means to cluster the cases (with 2 fewer clusters than the total cases)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(case_matrix)\n",
    "    \n",
    "    # Step 3: Assign each case law to a cluster\n",
    "    clustered_cases = defaultdict(list)\n",
    "    for idx, label in enumerate(kmeans.labels_):\n",
    "        clustered_cases[label].append(case_laws[idx])\n",
    "    \n",
    "    # Step 4: Check the group sizes and split if necessary\n",
    "    final_clusters = defaultdict(list)\n",
    "    cluster_counter = 0  # Counter for new sub-clusters\n",
    "    for cluster, cases in clustered_cases.items():\n",
    "        while len(cases) > max_cases_per_group:\n",
    "            # Split the group into two smaller groups\n",
    "            split_group = cases[:max_cases_per_group]\n",
    "            cases = cases[max_cases_per_group:]\n",
    "            final_clusters[cluster_counter].extend(split_group)\n",
    "            cluster_counter += 1\n",
    "        \n",
    "        # Add the remaining cases (if any)\n",
    "        final_clusters[cluster_counter].extend(cases)\n",
    "        cluster_counter += 1\n",
    "\n",
    "    return final_clusters\n",
    "\n",
    "# Example usage\n",
    "case_dict = {\n",
    "    \"Case1\": [\"Act1\", \"Act2\", \"Act3\"],\n",
    "    \"Case2\": [\"Act1\", \"Act4\"],\n",
    "    \"Case3\": [\"Act2\", \"Act5\",\"Act4\"],\n",
    "    \"Case4\": [\"Act1\", \"Act2\"],\n",
    "    \"Case5\": [\"Act3\", \"Act4\", \"Act5\"]\n",
    "}\n",
    "\n",
    "n_clusters = 2  # Define the number of clusters\n",
    "grouped_cases = intelligent_grouping_with_kmeans(case_dict, n_clusters,4)\n",
    "\n",
    "for cluster, cases in grouped_cases.items():\n",
    "    print(f\"Cluster {cluster + 1}: {cases}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Cluster 1: ['ukftt_tc_2022_201', 'ewhc_admin_2011_2317', 'ewcop_2014_2', 'ewhc_ch_2012_4090', 'ewhc_ch_2009_1229', 'ewhc_ch_2021_324', 'ewhc_ch_2022_740', 'ewhc_admin_2009_3614', 'ewhc_fam_2014_6', 'ewhc_admin_2011_441', 'ewca_crim_2007_1165', 'ewhc_qb_2014_4729', 'ewca_crim_2019_281', 'ewhc_admin_2011_2855', 'ewhc_admin_2012_2806', 'ewhc_admin_2006_3346', 'ewhc_admin_2015_1603', 'ewhc_qb_2019_3340', 'ewhc_admin_2013_195', 'ewca_civ_2012_51', 'ewca_civ_2010_1468', 'ewhc_ch_2018_2344', 'ewca_crim_2004_3092', 'ewhc_admin_2024_1780', 'ewhc_ipec_2024_2478', 'ewca_civ_2017_2138', 'ewhc_scco_2022_3354', 'ukut_lc_2024_202', 'ewca_crim_2024_147', 'ewhc_ch_2012_28', 'ewhc_admin_2003_1837', 'ewhc_admin_2014_2900', 'ewhc_ch_2016_876', 'ewca_civ_2006_4', 'ewhc_admin_2014_2109', 'ewca_crim_2013_2406', 'ewhc_ch_2012_3266', 'ewhc_ch_2019_3339', 'ewhc_admin_2005_2977', 'ewca_crim_2008_1194', 'ewhc_ch_2011_3782', 'ewhc_qb_2022_1968', 'ewhc_scco_2022_1778', 'ukftt_tc_2022_171', 'ewca_civ_2005_1570', 'ewca_crim_2008_1745', 'ewcop_2019_10', 'ewhc_admin_2016_3693', 'ewca_civ_2015_718', 'ewhc_ch_2016_2759', 'ewhc_admin_2009_535', 'ewhc_admin_2017_576', 'ewca_crim_2005_1722', 'uksc_2013_37', 'ewca_civ_2023_652', 'ewca_civ_2003_167', 'ewhc_ch_2025_46', 'ewhc_qb_2018_1948', 'ewcop_2015_13', 'ewhc_ch_2020_1363', 'ewhc_admin_2023_62', 'ewhc_tcc_2004_2991', 'ewca_civ_2006_140', 'ewhc_admin_2014_4153', 'ewhc_ch_2015_3172', 'ewhc_admin_2009_3189', 'ewca_civ_2005_1770', 'ewhc_qb_2011_272', 'ewhc_ch_2018_2169', 'ewca_civ_2006_380', 'ewca_crim_2009_469', 'ewhc_fam_2018_3244', 'ewhc_admin_2009_3412', 'ewhc_fam_2021_1153', 'ewhc_admin_2024_2661', 'ewca_civ_2005_647', 'ewhc_admin_2019_62', 'ewca_crim_2011_2651', 'ewhc_ch_2014_1688', 'ewhc_ch_2017_769', 'ukpc_2022_47', 'ewhc_ch_2020_2624', 'ewhc_qb_2016_2355', 'ukut_tcc_2023_244', 'ukftt_grc_2025_287', 'ewca_crim_2013_1026', 'ewhc_admin_2014_3699', 'ewhc_admin_2013_240', 'ewhc_admin_2011_2943', 'ewca_civ_2013_1116', 'ewca_civ_2004_988', 'ukftt_grc_2025_251', 'ewhc_admin_2004_2240', 'ewhc_admin_2011_754', 'ewhc_admin_2004_3362', 'ukftt_grc_2022_415', 'ewhc_admin_2014_1106', 'ewhc_ch_2023_123', 'ewhc_ch_2022_2112', 'ewhc_comm_2015_3248']\n",
      "100\n",
      "Cluster 2: ['uksc_2011_41', 'ewhc_admin_2019_1809', 'ewca_crim_2015_1256', 'ukftt_grc_2025_284', 'ewhc_qb_2020_718', 'ewhc_admin_2009_590', 'ewhc_fam_2022_2120', 'ukftt_grc_2024_333', 'ewcop_2020_23', 'ewhc_admin_2020_2352', 'ewca_crim_2023_202', 'ewhc_kb_2023_1256', 'ewhc_qb_2022_977', 'ewhc_costs_2003_9050', 'ewfc_2022_34', 'ewhc_qb_2003_1814', 'ewhc_fam_2014_3135', 'ewhc_ch_2013_4630', 'ewca_civ_2018_764', 'ewfc_2016_25', 'ewhc_qb_2017_2554', 'ukftt_tc_2024_744', 'ewfc_2019_60', 'ewca_civ_2018_162', 'ukut_lc_2022_342', 'ewca_civ_2021_252', 'ewca_crim_2010_1474', 'ewhc_admin_2003_2779', 'ewhc_admin_2005_2278', 'ewhc_admin_2004_563', 'ewca_crim_2006_707', 'ukftt_grc_2025_282', 'ewca_crim_2009_2204', 'ewhc_fam_2018_3795', 'ewca_civ_2012_1660', 'ewca_crim_2016_1941', 'ewhc_ch_2006_2612', 'ewhc_ch_2024_560', 'ewhc_ch_2021_1275', 'ewhc_fam_2017_3164', 'ukut_aac_2016_355', 'ukut_iac_2015_268', 'ewhc_fam_2017_917', 'ukut_aac_2022_263', 'ewca_crim_2010_1450', 'ewhc_kb_2023_3472', 'ewhc_admin_2012_3226', 'ewhc_ch_2019_2517', 'ewca_crim_2009_201', 'ewhc_admin_2020_1522', 'ewhc_comm_2020_3334', 'ewca_civ_2017_20', 'ewhc_admin_2013_1009', 'ewhc_admin_2011_2962', 'ewhc_ch_2017_1770', 'ewhc_admin_2005_1393', 'ewhc_admin_2003_1321', 'ewhc_tcc_2009_1664', 'ewca_civ_2015_1230', 'ewfc_2014_9', 'ukftt_grc_2024_471', 'ewca_civ_2003_135', 'ukftt_grc_2024_1087', 'ewhc_ch_2022_1348', 'ewhc_ch_2020_3295', 'ewhc_ch_2010_938', 'ewca_civ_2015_414', 'ewca_crim_2017_2063', 'ewca_crim_2014_382', 'ewhc_admin_2008_2013', 'ewhc_fam_2017_1782', 'ewca_civ_2011_1172', 'ewhc_admin_2008_1043', 'ewhc_admin_2010_439', 'ewca_civ_2023_239', 'ewhc_fam_2014_4643', 'ewca_crim_2024_313', 'ewhc_admin_2013_1786', 'ewhc_ipec_2019_126', 'ewhc_admin_2020_3243', 'ewhc_comm_2023_2866', 'ewca_civ_2006_529', 'ewca_crim_2017_2314', 'ewhc_admin_2015_3084', 'ewhc_admin_2009_2348', 'ewhc_fam_2022_2677', 'ewhc_admin_2017_1818', 'ewhc_ch_2019_1839', 'ewhc_qb_2015_1760', 'ewhc_admin_2012_1033', 'ewhc_admin_2007_1304', 'ewca_civ_2010_805', 'ewca_crim_2017_1779', 'ewca_civ_2015_1419', 'ewca_crim_2004_621', 'ewhc_ch_2023_264', 'ewhc_fam_2005_1832', 'ukftt_grc_2025_289', 'ewca_civ_2004_1269', 'ewca_civ_2010_390']\n",
      "90\n",
      "Cluster 3: ['ewhc_qb_2022_1917', 'ewca_civ_2024_16', 'ewca_civ_2016_1267', 'uksc_2013_8', 'ukut_aac_2021_69', 'ukut_aac_2023_288', 'ewca_civ_2005_1479', 'ewhc_admin_2006_3048', 'ewca_crim_2017_2065', 'ewhc_tcc_2011_87', 'ewca_crim_2023_630', 'ewca_crim_2013_1764', 'ukftt_grc_2023_35', 'ewhc_pat_2005_2240', 'ewhc_pat_2014_4242', 'ukpc_2023_36', 'ewhc_ch_2009_74', 'ewhc_admin_2006_2784', 'ewcop_2020_14', 'ewhc_fam_2005_402', 'ewhc_comm_2011_1372', 'ewhc_admin_2019_84', 'ewhc_ch_2005_1508', 'ewfc_2024_6', 'ewhc_admin_2016_2186', 'ewhc_ch_2003_2845', 'ukait_2009_41', 'ewfc_b_2024_69', 'ewhc_ch_2022_2033', 'ewca_crim_2023_1520', 'ewhc_ch_2014_763', 'ewca_civ_2020_833', 'ukftt_tc_2024_462', 'ewca_crim_2007_3223', 'ewhc_qb_2018_2135', 'ewfc_b_2024_40', 'ewcop_2021_32', 'ewca_civ_2012_1395', 'ewca_civ_2014_935', 'ewhc_admin_2003_1578', 'ewca_civ_2023_1000', 'ewca_crim_2009_288', 'ukftt_tc_2023_959', 'ukpc_2009_42', 'ewhc_fam_2018_3841', 'ewhc_admin_2015_2868', 'ewhc_ch_2023_175', 'ewca_civ_2022_1943', 'ewca_civ_2011_1515', 'ewca_crim_2006_757', 'ewhc_ch_2023_4', 'ewhc_qb_2015_1060', 'ewca_civ_2012_543', 'ewhc_ch_2012_731', 'ewhc_comm_2008_1785', 'ewhc_ch_2016_1076', 'ukftt_tc_2024_139', 'ewca_crim_2004_492', 'ewca_civ_2015_1085', 'ewhc_ch_2017_2621', 'ewhc_qb_2013_797', 'ewca_crim_2007_3432', 'ewhc_admin_2008_470', 'ewhc_qb_2015_926', 'ewca_civ_2009_454', 'ewhc_fam_2010_1346', 'ewca_civ_2021_763', 'ewhc_comm_2006_777', 'ewca_civ_2007_826', 'ewhc_admin_2007_807', 'ewca_crim_2007_2847', 'ewhc_admin_2020_801', 'ewca_civ_2011_1539', 'ewhc_admin_2024_2864', 'ewhc_ch_2004_2947', 'ewhc_ch_2012_858', 'ewhc_tcc_2012_2593', 'ewca_crim_2023_1121', 'ewhc_fam_2004_1066', 'ewhc_admin_2004_1069', 'ewhc_qb_2020_1689', 'ewca_civ_2013_554', 'ewhc_ch_2020_3622', 'ewcc_2024_7', 'ewhc_fam_2023_1096', 'ewhc_admin_2008_738', 'ewca_crim_2008_854', 'ewfc_2025_41', 'ukpc_2013_21', 'ewca_civ_2003_151']\n",
      "17\n",
      "Cluster 4: ['ewca_civ_2009_856', 'ewhc_admin_2019_2233', 'ewhc_admin_2009_1404', 'ewca_civ_2011_1081', 'ewca_crim_2022_1428', 'ewca_civ_2005_385', 'ewhc_admin_2013_1818', 'ewhc_admin_2018_1371', 'ewca_civ_2014_163', 'ewhc_admin_2015_1641', 'ewca_civ_2021_113', 'ewca_civ_2014_826', 'ukait_2008_1', 'ewca_civ_2019_53', 'ukait_2009_40', 'ukait_2009_54', 'ukait_2009_43']\n",
      "22\n",
      "Cluster 5: ['ewca_civ_2014_1105', 'ewca_civ_2020_620', 'ewhc_qb_2009_1900', 'ewhc_ch_2024_11', 'ewca_civ_2011_10', 'ewhc_fam_2015_455', 'ewhc_qb_2010_100', 'ewhc_comm_2020_2736', 'ewhc_qb_2021_157', 'ewhc_admin_2020_2579', 'ewhc_pat_2015_1094', 'ewhc_admin_2020_1850', 'ewhc_qb_2003_3555', 'ewhc_admin_2018_2651', 'ewhc_ch_2018_2877', 'ewca_civ_2007_1175', 'ewhc_comm_2015_2748', 'ewhc_ch_2015_3910', 'uksc_2013_11', 'ewca_civ_2015_515', 'ewhc_ch_2010_180', 'ewhc_admin_2018_1092']\n",
      "27\n",
      "Cluster 6: ['ewhc_ch_2017_1727', 'ewca_crim_2008_468', 'ewca_crim_2012_1939', 'ewhc_comm_2023_1889', 'ewca_crim_2006_2136', 'ewca_crim_2009_651', 'ewca_crim_2017_1461', 'ewhc_admin_2013_218', 'ewca_crim_2008_2500', 'ewhc_admin_2012_1098', 'ewca_crim_2013_2499', 'ewca_crim_2006_3301', 'ewca_crim_2010_591', 'ewca_crim_2018_2895', 'ewca_crim_2009_379', 'ewca_crim_2022_1837', 'ewca_crim_2006_3335', 'ewca_crim_2017_1971', 'ewhc_admin_2012_882', 'ewhc_qb_2011_179', 'ewca_crim_2007_2548', 'ewca_crim_2005_3377', 'ewca_crim_2015_1933', 'ewca_crim_2007_36', 'ewca_crim_2007_3021', 'ewca_civ_2008_1097', 'ewca_crim_2009_1942']\n",
      "6\n",
      "Cluster 7: ['ewca_crim_2023_1106', 'ewca_crim_2018_1393', 'ewca_crim_2019_2056', 'ewca_crim_2019_466', 'ewca_crim_2008_894', 'ewca_crim_2022_483']\n",
      "14\n",
      "Cluster 8: ['ewhc_tcc_2019_2212', 'ewhc_comm_2023_910', 'ewhc_comm_2006_134', 'ewhc_comm_2021_286', 'ewhc_ch_2005_3438', 'ewhc_comm_2017_1430', 'ewhc_comm_2017_3430', 'ewhc_ch_2019_704', 'ewhc_comm_2023_391', 'ewhc_comm_2005_2115', 'ewhc_comm_2004_1752', 'ewhc_comm_2020_2012', 'ewhc_comm_2019_3292', 'ewhc_comm_2018_330']\n",
      "9\n",
      "Cluster 9: ['ukpc_2011_16', 'ewca_crim_2022_50', 'ewca_crim_2005_1681', 'ewca_crim_2003_190', 'ewca_civ_2018_1841', 'ewca_crim_2020_1455', 'ewca_crim_2010_1486', 'ewca_crim_2006_646', 'ewca_crim_2005_1881']\n",
      "24\n",
      "Cluster 10: ['ewca_civ_2018_1808', 'ewhc_admin_2006_1346', 'ewhc_ch_2013_47', 'ewhc_admin_2004_611', 'ewca_civ_2013_703', 'ewhc_qb_2017_294', 'ewhc_admin_2009_634', 'ewca_civ_2015_1189', 'ewhc_admin_2004_430', 'ewhc_admin_2009_771', 'ewhc_admin_2020_1905', 'ewhc_admin_2005_591', 'ewhc_admin_2017_3059', 'ewhc_admin_2003_1419', 'ewhc_admin_2005_2363', 'ewhc_admin_2010_2929', 'ewca_civ_2017_438', 'ewhc_ch_2010_1951', 'ewhc_admin_2009_2674', 'uksc_2022_30', 'ewhc_admin_2013_3250', 'ewhc_admin_2006_2643', 'ewhc_admin_2014_3627', 'ewhc_admin_2006_3014']\n",
      "16\n",
      "Cluster 11: ['uksc_2011_33', 'ewhc_ch_2023_568', 'eat_2023_162', 'eat_2024_17', 'ewca_civ_2018_1105', 'eat_2022_192', 'ewhc_admin_2013_512', 'ewhc_qb_2011_4', 'ewhc_admin_2022_967', 'ewhc_admin_2024_3016', 'eat_2024_56', 'ewca_civ_2016_775', 'ewhc_admin_2019_3480', 'eat_2023_70', 'ewhc_kb_2023_965', 'ewhc_admin_2012_1867']\n",
      "5\n",
      "Cluster 12: ['ewhc_ch_2005_1075', 'ewhc_admin_2024_3297', 'ewhc_qb_2017_3087', 'ewca_crim_2020_597', 'ewhc_admin_2003_2698']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle_file_path = '../data/final_test/case_csvs/cleaned_case_legislation_map.pkl'\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "        case_dict=  pickle.load(file)\n",
    "\n",
    "grouped_cases = intelligent_grouping_with_kmeans(case_dict,10, 100)\n",
    "i = 0\n",
    "for cluster, cases in grouped_cases.items():\n",
    "    print(len(cases))\n",
    "    i = i + len(cases)\n",
    "    print(f\"Cluster {cluster + 1}: {cases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/final_test/case_csvs/clusters', 'wb') as f:\n",
    "        pickle.dump(grouped_cases, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ewhc_ch_2005_1075',\n",
       " 'ewhc_admin_2024_3297',\n",
       " 'ewhc_qb_2017_3087',\n",
       " 'ewca_crim_2020_597',\n",
       " 'ewhc_admin_2003_2698']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_cases[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uri_to_filename(case_uri, base_url='https://caselaw.nationalarchives.gov.uk'):\n",
    "    \"\"\"Convert a case URI into a filename by removing the base URL and replacing '/' with '_'.\n",
    "    Args:\n",
    "    case_uri (str): The full URI of the case law.\n",
    "    base_url (str): The base URL to be removed from the case URI.\n",
    "    \n",
    "    Returns:\n",
    "    str: The generated filename.\n",
    "    \"\"\"\n",
    "    if case_uri.startswith(base_url):\n",
    "        # Remove base URL and leading slash\n",
    "        relative_path = case_uri[len(base_url):].lstrip('/')\n",
    "        # Replace slashes with underscores and return the filename\n",
    "        filename = relative_path.replace('/', '_')\n",
    "        return filename\n",
    "    else:\n",
    "        return None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_csv = \"../data/final_test/positve_cases.csv\"  \n",
    "df_input  = pd.read_csv(input_csv,index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['case_uri', 'para_id', 'paragraphs', 'references', 'if_law_applied',\n",
       "       'application_of_law_phrases', 'reason', 'if_law_applied_llama',\n",
       "       'application_of_law_phrases_llama', 'reason_llama',\n",
       "       'if_law_applied_claude', 'application_of_law_phrases_claude',\n",
       "       'reason_claude', 'confidence', 'agreement_with', 'final_annotation',\n",
       "       'case_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input['acts'] = df_input['case_uri'].apply(uri_to_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input.rename(columns ={'acts':'case_name'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input.to_csv(input_csv,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             case_uri  \\\n",
      "25  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "26  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "27  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "28  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "80  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "\n",
      "                         para_id  \\\n",
      "25  ewhc_admin_2017_3059#para_28   \n",
      "26  ewhc_admin_2017_3059#para_39   \n",
      "27  ewhc_admin_2017_3059#para_41   \n",
      "28  ewhc_admin_2017_3059#para_42   \n",
      "80  ewhc_admin_2003_1419#para_22   \n",
      "\n",
      "                                           paragraphs  \\\n",
      "25  28. In response, the Defendant relied on the c...   \n",
      "26  39. In my view, the Briefing Papers were out-o...   \n",
      "27  41. Mr Lockhart-Mummery QC, on behalf of the d...   \n",
      "28  42. The starting point is that there is no sta...   \n",
      "80  22. The Defendants answer that at paragraph 21...   \n",
      "\n",
      "                                           references if_law_applied  \\\n",
      "25  [{'text': 'section 77', 'href': 'http://www.le...           True   \n",
      "26                                                 []          False   \n",
      "27                                                 []          False   \n",
      "28                                                 []          False   \n",
      "80                                                 []          False   \n",
      "\n",
      "                           application_of_law_phrases  \\\n",
      "25  ['no duty to give reasons for a decision not t...   \n",
      "26                                                 []   \n",
      "27                                                 []   \n",
      "28                                                 []   \n",
      "80                                                 []   \n",
      "\n",
      "                                               reason if_law_applied_llama  \\\n",
      "25  The paragraph applies the principles of case l...                False   \n",
      "26  The paragraph discusses the reliability of evi...                 True   \n",
      "27  This paragraph discusses the arguments made by...                 True   \n",
      "28  This paragraph discusses the absence of a stat...                 True   \n",
      "80  This paragraph discusses procedural aspects an...                 True   \n",
      "\n",
      "                     application_of_law_phrases_llama  \\\n",
      "25                                                 []   \n",
      "26  ['I find the evidence of Mr Colbourne more rel...   \n",
      "27  ['the Defendant did not give reasons for his d...   \n",
      "28  ['there is no statutory duty to give reasons f...   \n",
      "80  ['he would have had to have made a finding tha...   \n",
      "\n",
      "                                         reason_llama if_law_applied_claude  \\\n",
      "25  This paragraph mentions a general legal princi...                  True   \n",
      "26  The judge applies the legal principle of evalu...                  True   \n",
      "27  The court applies the legal principle of provi...                  True   \n",
      "28  The court applies the principle of judicial re...                  True   \n",
      "80  The court applies principles of procedural fai...                  True   \n",
      "\n",
      "                    application_of_law_phrases_claude  \\\n",
      "25  ['no duty to give reasons for a decision not t...   \n",
      "26  ['I find the evidence of Mr Colbourne more rel...   \n",
      "27  ['the Defendant did not give reasons for his d...   \n",
      "28  ['there is no statutory duty to give reasons f...   \n",
      "80  ['he would have had to have made a finding tha...   \n",
      "\n",
      "                                        reason_claude confidence  \\\n",
      "25  Model A's analysis is more accurate. This para...       High   \n",
      "26  Model B's analysis is more accurate. While thi...       High   \n",
      "27  Model B's analysis is more accurate. This para...       High   \n",
      "28  Model B's analysis is more accurate. While thi...       High   \n",
      "80  Model B's analysis is more accurate. This para...       High   \n",
      "\n",
      "   agreement_with  final_annotation             case_name  \n",
      "25         OpenAI              True  ewhc_admin_2017_3059  \n",
      "26          Llama              True  ewhc_admin_2017_3059  \n",
      "27          Llama              True  ewhc_admin_2017_3059  \n",
      "28          Llama              True  ewhc_admin_2017_3059  \n",
      "80          Llama              True  ewhc_admin_2003_1419  \n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2017_3059.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2003_1419.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2009_2674.csv with 19 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2020_1905.csv with 23 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_ch_2013_47.csv with 35 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2006_2643.csv with 8 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2005_2363.csv with 31 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2009_771.csv with 27 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2005_591.csv with 13 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2014_3627.csv with 47 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2009_634.csv with 35 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2006_3014.csv with 25 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2006_1346.csv with 99 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_qb_2017_294.csv with 31 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_ch_2010_1951.csv with 47 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2004_611.csv with 15 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2004_430.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2013_3250.csv with 17 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewca_civ_2013_703.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewca_civ_2015_1189.csv with 28 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewca_civ_2018_1808.csv with 47 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/uksc_2022_30.csv with 56 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewhc_admin_2010_2929.csv with 21 rows\n",
      "Created ../data/final_test/case_csvs/cluster9/ewca_civ_2017_438.csv with 24 rows\n",
      "{'ewhc_admin_2017_3059': ['id/ukpga/1990/8', 'id/ukpga/1990/9'], 'ewhc_admin_2003_1419': ['id/ukpga/1990/8'], 'ewhc_admin_2009_2674': ['id/ukpga/1990/8'], 'ewhc_admin_2020_1905': ['id/ukpga/1990/8', 'id/ukpga/1991/34', 'id/ukpga/2011/20'], 'ewhc_ch_2013_47': ['id/ukpga/1990/8'], 'ewhc_admin_2006_2643': ['id/ukpga/1990/8', 'id/ukpga/2004/5'], 'ewhc_admin_2005_2363': ['id/ukpga/1980/66', 'id/ukpga/1990/8', 'id/ukpga/Will4/5-6/50'], 'ewhc_admin_2009_771': ['id/ukpga/1990/8'], 'ewhc_admin_2005_591': ['id/ukpga/1990/8'], 'ewhc_admin_2014_3627': ['id/ukpga/1988/4', 'id/ukpga/1990/8', 'id/ukpga/1990/9', 'id/ukpga/2004/5', 'id/ukpga/2006/16', 'id/ukpga/Geo6/12-13-14/97'], 'ewhc_admin_2009_634': ['id/ukpga/1968/72', 'id/ukpga/1971/78', 'id/ukpga/1978/30', 'id/ukpga/1990/11', 'id/ukpga/1990/8', 'id/ukpga/1991/34', 'id/ukpga/2004/5', 'id/ukpga/Eliz2/10-11/38', 'id/ukpga/Geo6/10-11/51'], 'ewhc_admin_2006_3014': ['id/ukpga/1990/8', 'id/ukpga/1998/42', 'id/ukpga/1999/22'], 'ewhc_admin_2006_1346': ['id/ukpga/1990/8', 'id/ukpga/1994/33', 'id/ukpga/Geo5and1Edw8/26/49'], 'ewhc_qb_2017_294': ['id/ukpga/1990/8', 'id/ukpga/1991/34', 'id/ukpga/1998/42'], 'ewhc_ch_2010_1951': ['id/ukpga/1990/8'], 'ewhc_admin_2004_611': ['id/ukpga/1990/8'], 'ewhc_admin_2004_430': ['id/ukpga/1990/8'], 'ewhc_admin_2013_3250': ['id/ukpga/1980/43', 'id/ukpga/1990/8'], 'ewca_civ_2013_703': ['id/ukpga/1990/8'], 'ewca_civ_2015_1189': ['id/ukpga/1988/4', 'id/ukpga/1990/8', 'id/ukpga/2004/5'], 'ewca_civ_2018_1808': ['id/ukpga/1990/8'], 'uksc_2022_30': ['id/ukpga/1990/8', 'id/ukpga/Geo6/10-11/51'], 'ewhc_admin_2010_2929': ['id/ukpga/1971/78', 'id/ukpga/1990/8'], 'ewca_civ_2017_438': ['id/ukpga/1990/8']}\n",
      "Saved cluster 9 cases and acts to ../data/final_test/case_csvs/cluster9/cluster_9_cases.pkl\n"
     ]
    }
   ],
   "source": [
    "def create_cluster_files(cluster_number, df_input, base_path='../data/final_test/case_csvs'):\n",
    "    \"\"\"\n",
    "    Create cluster directory and copy case files for a specific cluster number.\n",
    "    \n",
    "    Args:\n",
    "        cluster_number (int): The cluster number to process\n",
    "        filtered_df (DataFrame): Filtered dataframe containing cases for this cluster\n",
    "        base_path (str): Base path for the data directory\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = f'{base_path}/cluster{cluster_number}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Assuming your DataFrame 'df' has a column called 'case_uri'\n",
    "    filtered_df = df_input[df_input['case_name'].isin(grouped_cases[cluster_number])].copy()\n",
    "\n",
    "    # Now, filtered_df contains only rows corresponding to Cluster 1\n",
    "    print(filtered_df.head())\n",
    "    # Process each case in the filtered dataframe\n",
    "    for case_name in filtered_df['case_name'].unique():\n",
    "        # Get rows for this case\n",
    "        case_rows = filtered_df[filtered_df['case_name'] == case_name]\n",
    "        \n",
    "        # Create output file path\n",
    "        output_file = os.path.join(output_dir, f'{case_name}.csv')\n",
    "        \n",
    "        # Save rows to CSV\n",
    "        case_rows.to_csv(output_file, index=False)\n",
    "        print(f'Created {output_file} with {len(case_rows)} rows')\n",
    "    \n",
    "    # Create a dictionary mapping cases to their legislation acts\n",
    "    cluster_cases = {}\n",
    "    with open(f'{base_path}/cleaned_case_legislation_map.pkl', 'rb') as f:\n",
    "        case_legislation_dic = pickle.load(f)\n",
    "    \n",
    "    for case_name in list(filtered_df['case_name'].unique()):\n",
    "        cluster_cases[case_name] = case_legislation_dic[case_name]\n",
    "    \n",
    "    print(cluster_cases)\n",
    "    # Save the dictionary to a pickle file\n",
    "    output_pkl = f'{base_path}/cluster{cluster_number}/cluster_{cluster_number}_cases.pkl'\n",
    "    with open(output_pkl, 'wb') as f:\n",
    "        pickle.dump(cluster_cases, f)\n",
    "    print(f\"Saved cluster {cluster_number} cases and acts to {output_pkl}\")\n",
    "\n",
    "# Call the function for cluster 10\n",
    "create_cluster_files(9, df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 cases: 6\n",
      "Total unique legislation acts: 10\n",
      "Number of acts 10\n",
      "Act groups sizes: [5, 5]\n",
      "Group 0: 5 cases\n",
      "  Acts: 5\n",
      "  Sample cases: ['ewhc_ch_2016_2759', 'ewhc_admin_2012_1033', 'ewhc_admin_2013_1009', 'ewhc_ch_2021_1275', 'ukut_lc_2022_342']\n",
      "\n",
      "Group 1: 1 cases\n",
      "  Acts: 5\n",
      "  Sample cases: ['ewhc_admin_2014_2900']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Load cluster dictionaries\n",
    "cluster_033_dict = {}\n",
    "cluster_036_dict = {}\n",
    "\n",
    "\n",
    "# Load cluster 0 dictionary\n",
    "with open('../data/final_test/case_csvs/cluster034/cluster034_cases.pkl', 'rb') as f:\n",
    "    cluster_0_dict = pickle.load(f)\n",
    "\n",
    "# Load cluster 1 dictionary  \n",
    "# with open('../data/final_test/case_csvs/cluster1/cluster_1_cases.pkl', 'rb') as f:\n",
    "#     cluster_1_dict = pickle.load(f)\n",
    "\n",
    "print(\"Cluster 0 cases:\", len(cluster_0_dict))\n",
    "# print(\"Cluster 1 cases:\", len(cluster_1_dict))\n",
    "\n",
    "# Get full list of all legislation acts from both clusters\n",
    "all_acts = set()\n",
    "for case_acts in cluster_0_dict.values():\n",
    "    all_acts.update(case_acts)\n",
    "# for case_acts in cluster_1_dict.values():\n",
    "#     all_acts.update(case_acts)\n",
    "\n",
    "print(f\"Total unique legislation acts: {len(all_acts)}\")\n",
    "\n",
    "# Divide acts into 3 sets with minimum overlap\n",
    "# Use set partitioning to create balanced groups\n",
    "acts_list = list(all_acts)\n",
    "n_acts = len(acts_list)\n",
    "print(\"Number of acts\",n_acts)\n",
    "acts_per_group = n_acts // 2\n",
    "\n",
    "# Create 3 groups of acts\n",
    "act_groups = []\n",
    "for i in range(2):\n",
    "    start_idx = i * acts_per_group\n",
    "    end_idx = start_idx + acts_per_group if i < 2 else n_acts  # Last group gets remaining acts\n",
    "    act_groups.append(set(acts_list[start_idx:end_idx]))\n",
    "\n",
    "print(f\"Act groups sizes: {[len(group) for group in act_groups]}\")\n",
    "\n",
    "# Create a dictionary mapping acts to their groups\n",
    "act_to_group = {}\n",
    "for group_idx, group in enumerate(act_groups):\n",
    "    for act in group:\n",
    "        act_to_group[act] = group_idx\n",
    "\n",
    "# Divide cases into clusters based on their legislation acts\n",
    "# Assign each case to the group that has the most of its acts\n",
    "cluster_assignments = {}\n",
    "\n",
    "# Process cluster 0 cases\n",
    "for case_name, case_acts in cluster_0_dict.items():\n",
    "    group_scores = [0, 0]\n",
    "    for act in case_acts:\n",
    "        if act in act_to_group:\n",
    "            group_scores[act_to_group[act]] += 1\n",
    "    \n",
    "    # Assign to group with highest score\n",
    "    best_group = group_scores.index(max(group_scores))\n",
    "    cluster_assignments[case_name] = best_group\n",
    "\n",
    "\n",
    "\n",
    "# Print results\n",
    "for group_idx in range(2):\n",
    "    group_cases = [case for case, group in cluster_assignments.items() if group == group_idx]\n",
    "    print(f\"Group {group_idx}: {len(group_cases)} cases\")\n",
    "    print(f\"  Acts: {len(act_groups[group_idx])}\")\n",
    "    print(f\"  Sample cases: {group_cases[:5]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create three new cluster folders\n",
    "cluster_dirs = ['cluster033', 'cluster036']\n",
    "for cluster_dir in cluster_dirs:\n",
    "    os.makedirs(f'../data/final_test/case_csvs/{cluster_dir}', exist_ok=True)\n",
    "\n",
    "# Create dictionaries for each cluster\n",
    "cluster_0_cases = {}\n",
    "cluster_1_cases = {}\n",
    "\n",
    "\n",
    "\n",
    "# Populate cluster dictionaries based on assignments\n",
    "for case_name, group_idx in cluster_assignments.items():\n",
    "    if case_name in cluster_0_dict:\n",
    "        case_acts = cluster_0_dict[case_name]\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    if group_idx == 0:\n",
    "        cluster_0_cases[case_name] = case_acts\n",
    "    else:\n",
    "        cluster_1_cases[case_name] = case_acts\n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cluster0_cases.pkl to cluster0 folder with 5 cases\n",
      "Saved cluster_1_cases.pkl to cluster1 folder with 1 cases\n"
     ]
    }
   ],
   "source": [
    "# Save cluster dictionaries as pickle files in their respective folders\n",
    "import pickle\n",
    "\n",
    "# Save cluster_0_cases to cluster0 folder\n",
    "pickle.dump(cluster_0_cases, open('../data/final_test/case_csvs/cluster033/cluster033_cases.pkl', 'wb'))\n",
    "print(f\"Saved cluster0_cases.pkl to cluster0 folder with {len(cluster_0_cases)} cases\")\n",
    "\n",
    "# Save cluster_1_cases to cluster1 folder  \n",
    "pickle.dump(cluster_1_cases, open('../data/final_test/case_csvs/cluster036/cluster036_cases.pkl', 'wb'))\n",
    "print(f\"Saved cluster_1_cases.pkl to cluster1 folder with {len(cluster_1_cases)} cases\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cluster033: 5 cases\n",
      "  Created ewhc_ch_2016_2759.csv in cluster033/ with 20 rows\n",
      "  Created ewhc_admin_2012_1033.csv in cluster033/ with 26 rows\n",
      "  Created ewhc_admin_2013_1009.csv in cluster033/ with 32 rows\n",
      "  Created ewhc_ch_2021_1275.csv in cluster033/ with 19 rows\n",
      "  Created ukut_lc_2022_342.csv in cluster033/ with 5 rows\n",
      "Processing cluster036: 1 cases\n",
      "  Created ewhc_admin_2014_2900.csv in cluster036/ with 10 rows\n",
      "Finished copying cases to cluster folders\n"
     ]
    }
   ],
   "source": [
    "# Load each pickle file and copy cases from main folder to designated cluster folders\n",
    "import pickle\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define cluster folders\n",
    "cluster_folders = ['cluster033', 'cluster036']\n",
    "\n",
    "for cluster_folder in cluster_folders:\n",
    "    # Load the pickle file for this cluster\n",
    "    pickle_path = f'../data/final_test/case_csvs/{cluster_folder}/{cluster_folder}_cases.pkl'\n",
    "    \n",
    "    if os.path.exists(pickle_path):\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            cluster_cases = pickle.load(f)\n",
    "        \n",
    "        print(f\"Processing {cluster_folder}: {len(cluster_cases)} cases\")\n",
    "        \n",
    "        # Copy each case CSV file to the cluster folder\n",
    "        for case_name in list(cluster_cases.keys()):\n",
    "            # Filter df_input to get rows for this case\n",
    "            case_rows = df_input[df_input['case_name'] == case_name ]#.apply(convert_case_uri_to_key) == case_name]\n",
    "            \n",
    "            if not case_rows.empty:\n",
    "                # Save the filtered rows as CSV\n",
    "                dest_file = f'../data/final_test/case_csvs/{cluster_folder}/{case_name}.csv'\n",
    "                case_rows.to_csv(dest_file, index=False)\n",
    "                print(f\"  Created {case_name}.csv in {cluster_folder}/ with {len(case_rows)} rows\")\n",
    "            else:\n",
    "                print(f\"  Warning: No rows found for case {case_name} in df_input\")\n",
    "    else:\n",
    "        print(f\"Warning: {pickle_path} not found\")\n",
    "\n",
    "print(\"Finished copying cases to cluster folders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ewhc_admin_2014_2900': ['id/ukpga/2003/21']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_1_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied ewhc_ch_2021_324.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_crim_2007_1165.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_crim_2008_680.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_civ_2025_215.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_scco_2025_374.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_crim_2004_3092.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_scco_2022_3354.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_comm_2012_50.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_scco_2022_1778.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewcop_2019_10.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_ch_2025_46.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_ch_2013_47.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_admin_2015_1942.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_crim_2006_229.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_admin_2024_2661.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ukftt_grc_2025_287.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_admin_2014_3699.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ukftt_grc_2025_251.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied eat_2023_162.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_admin_2004_3362.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_comm_2015_3248.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ukftt_grc_2025_284.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_ch_2013_4630.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_civ_2018_162.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_crim_2009_2204.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_ch_2006_2612.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_ch_2024_560.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ukftt_grc_2025_283.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied eat_2022_192.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_crim_2009_201.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_ch_2017_1770.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_civ_2015_1230.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_crim_2019_2056.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_admin_2009_2348.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_fam_2022_2677.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_ch_2023_264.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_civ_2010_390.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_admin_2025_462.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_tcc_2011_87.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ukftt_tc_2022_326.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_civ_2009_650.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_ch_2003_2845.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ukait_2009_41.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_ch_2010_1951.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ukftt_tc_2023_959.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_comm_2008_1785.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_civ_2015_1085.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied eat_2025_29.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_qb_2013_797.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_civ_2011_1539.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_civ_2005_856.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ukftt_tc_2023_744.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewhc_qb_2020_1689.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ewca_crim_2008_854.csv to ../data/final_test/case_csvs/second_try\n",
      "Copied ukpc_2013_21.csv to ../data/final_test/case_csvs/second_try\n",
      "\n",
      "Copied 55 files to ../data/final_test/case_csvs/second_try\n"
     ]
    }
   ],
   "source": [
    "# Create the destination directory if it doesn't exist\n",
    "destination_dir = '../data/final_test/case_csvs/second_try'\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# Copy each missing case CSV file to the new directory\n",
    "for case_file in missing_cases:\n",
    "    source_path = os.path.join('../data/final_test/case_csvs', case_file)\n",
    "    dest_path = os.path.join(destination_dir, case_file)\n",
    "    shutil.copy2(source_path, dest_path)\n",
    "    print(f\"Copied {case_file} to {destination_dir}\")\n",
    "\n",
    "print(f\"\\nCopied {len(missing_cases)} files to {destination_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             case_uri  \\\n",
      "0   https://caselaw.nationalarchives.gov.uk/ewhc/c...   \n",
      "1   https://caselaw.nationalarchives.gov.uk/ewhc/c...   \n",
      "2   https://caselaw.nationalarchives.gov.uk/ewhc/c...   \n",
      "19  https://caselaw.nationalarchives.gov.uk/ewhc/f...   \n",
      "20  https://caselaw.nationalarchives.gov.uk/ewhc/f...   \n",
      "\n",
      "                       para_id  \\\n",
      "0    ewhc_ch_2009_1229#para_35   \n",
      "1    ewhc_ch_2022_2112#para_40   \n",
      "2    ewhc_ch_2022_2112#para_52   \n",
      "19  ewhc_fam_2018_3244#para_37   \n",
      "20  ewhc_fam_2018_3244#para_44   \n",
      "\n",
      "                                           paragraphs  \\\n",
      "0   35. It is relevant to bear in mind, I think, t...   \n",
      "1   40. Mr Ramel took me to a passage in the earli...   \n",
      "2   52. I reach this decision in the light of the ...   \n",
      "19  37. Once again, for the avoidance of any doubt...   \n",
      "20  44. Dr Summers was of the view that bone fract...   \n",
      "\n",
      "                                           references if_law_applied  \\\n",
      "0                                                  []          False   \n",
      "1   [{'text': 'Civil Evidence Act 1968', 'href': '...          False   \n",
      "2                                                  []          False   \n",
      "19                                                 []          False   \n",
      "20                                                 []           True   \n",
      "\n",
      "   application_of_law_phrases  \\\n",
      "0                          []   \n",
      "1                          []   \n",
      "2                          []   \n",
      "19                         []   \n",
      "20                         []   \n",
      "\n",
      "                                               reason if_law_applied_llama  \\\n",
      "0   This paragraph discusses the public interest a...                 True   \n",
      "1   This paragraph references previous case law an...                 True   \n",
      "2   This paragraph refers to the 'overriding objec...                 True   \n",
      "19  This paragraph discusses the judge's thoughts ...                 True   \n",
      "20  The judge applies the legal principle regardin...                False   \n",
      "\n",
      "                     application_of_law_phrases_llama  \\\n",
      "0   ['the public interest generally requires the p...   \n",
      "1   ['If it had been clear that the document in Ta...   \n",
      "2        ['in the light of the overriding objective']   \n",
      "19  ['I was minded to refuse to admit the report a...   \n",
      "20                                                 []   \n",
      "\n",
      "                                         reason_llama if_law_applied_claude  \\\n",
      "0   The judge applies the principle of open justic...                  True   \n",
      "1   The judge applies legal principles from previo...                  True   \n",
      "2   The judge implicitly applies the Civil Procedu...                  True   \n",
      "19  The judge applies the legal criteria for admit...                  True   \n",
      "20  This paragraph discusses medical evidence with...                  True   \n",
      "\n",
      "                    application_of_law_phrases_claude  \\\n",
      "0   ['the public interest generally requires the p...   \n",
      "1   ['If it had been clear that the document in Ta...   \n",
      "2   ['in the light of the overriding objective', '...   \n",
      "19  ['I was minded to refuse to admit the report a...   \n",
      "20  ['There is not a scintilla of evidence', 'or e...   \n",
      "\n",
      "                                        reason_claude confidence  \\\n",
      "0   Model B's analysis is more accurate. While thi...       High   \n",
      "1   Model B's analysis is more accurate. While thi...       High   \n",
      "2   Model B's analysis is more accurate. While the...       High   \n",
      "19  Model B's analysis is more accurate. This para...       High   \n",
      "20  This paragraph demonstrates the application of...       High   \n",
      "\n",
      "   agreement_with  final_annotation           case_name  \n",
      "0           Llama              True   ewhc_ch_2009_1229  \n",
      "1           Llama              True   ewhc_ch_2022_2112  \n",
      "2           Llama              True   ewhc_ch_2022_2112  \n",
      "19          Llama              True  ewhc_fam_2018_3244  \n",
      "20         OpenAI              True  ewhc_fam_2018_3244  \n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2009_1229.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2022_2112.csv with 22 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_fam_2018_3244.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2014_3699.csv with 2 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_scco_2022_3354.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2005_2977.csv with 11 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2023_123.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_scco_2022_1778.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2024_1780.csv with 23 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2024_2661.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_comm_2015_3248.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2011_3782.csv with 24 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2014_1688.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ipec_2024_2478.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2016_876.csv with 31 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_qb_2022_1968.csv with 24 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_qb_2011_272.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2012_4090.csv with 6 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2014_1106.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2022_740.csv with 52 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2015_3172.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2019_3339.csv with 24 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2009_3614.csv with 30 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2017_769.csv with 91 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2009_3412.csv with 8 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2013_240.csv with 13 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2019_62.csv with 30 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_qb_2018_1948.csv with 7 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2011_2317.csv with 34 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2012_28.csv with 54 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2003_1837.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2009_535.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2012_3266.csv with 13 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_fam_2014_6.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2025_46.csv with 24 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2018_2169.csv with 37 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2015_1603.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2013_195.csv with 23 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2014_2109.csv with 35 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2006_3346.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_qb_2019_3340.csv with 24 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_fam_2021_1153.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2011_2943.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2016_2759.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2018_2344.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2009_3189.csv with 7 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2020_2624.csv with 22 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2011_2855.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2016_3693.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2020_1363.csv with 19 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewcop_2019_10.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewcop_2015_13.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewcop_2014_2.csv with 28 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_civ_2023_652.csv with 15 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_crim_2019_281.csv with 34 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_crim_2007_1165.csv with 19 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_civ_2005_1570.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_civ_2012_51.csv with 42 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_civ_2013_1116.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_civ_2017_2138.csv with 15 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_civ_2010_1468.csv with 25 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_crim_2005_1722.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_crim_2013_2406.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_civ_2003_167.csv with 26 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_crim_2013_1026.csv with 24 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_civ_2006_380.csv with 26 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_crim_2011_2651.csv with 8 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_civ_2015_718.csv with 27 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ukftt_tc_2022_171.csv with 23 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ukftt_tc_2022_201.csv with 32 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ukftt_grc_2022_415.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/uksc_2013_37.csv with 19 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ukut_lc_2024_202.csv with 45 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ukpc_2022_47.csv with 43 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_ch_2021_324.csv with 19 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_civ_2006_4.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2017_576.csv with 19 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_civ_2005_647.csv with 11 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ukftt_grc_2025_287.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ukftt_grc_2025_251.csv with 17 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2014_2900.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2023_62.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2004_2240.csv with 4 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2011_754.csv with 9 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_tcc_2004_2991.csv with 11 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2011_441.csv with 4 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2012_2806.csv with 3 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2014_4153.csv with 2 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_admin_2004_3362.csv with 4 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_crim_2024_147.csv with 1 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_crim_2008_1194.csv with 7 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_crim_2009_469.csv with 17 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_crim_2008_1745.csv with 4 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_civ_2006_140.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_civ_2005_1770.csv with 3 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_crim_2004_3092.csv with 6 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ukut_tcc_2023_244.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_qb_2014_4729.csv with 3 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewhc_qb_2016_2355.csv with 17 rows\n",
      "Created ../data/final_test/case_csvs/cluster0/ewca_civ_2004_988.csv with 17 rows\n",
      "{'ewhc_ch_2009_1229': ['id/ukpga/1998/42'], 'ewhc_ch_2022_2112': ['id/ukpga/1968/64', 'id/ukpga/1986/45'], 'ewhc_fam_2018_3244': ['id/ukpga/2014/6'], 'ewhc_admin_2014_3699': ['id/ukpga/1989/41'], 'ewhc_scco_2022_3354': ['id/ukpga/1997/40'], 'ewhc_admin_2005_2977': ['id/ukpga/1972/20', 'id/ukpga/1988/52'], 'ewhc_ch_2023_123': ['id/ukpga/1986/45', 'id/ukpga/1986/46', 'id/ukpga/2009/1'], 'ewhc_scco_2022_1778': ['id/ukpga/2002/29'], 'ewhc_admin_2024_1780': ['id/ukpga/2018/16'], 'ewhc_admin_2024_2661': ['id/ukpga/1989/41', 'id/ukpga/1995/38'], 'ewhc_comm_2015_3248': ['id/ukpga/2002/29'], 'ewhc_ch_2011_3782': ['id/ukpga/1995/26'], 'ewhc_ch_2014_1688': ['id/ukpga/1988/48'], 'ewhc_ipec_2024_2478': ['id/ukpga/1988/48', 'id/ukpga/2023/28'], 'ewhc_ch_2016_876': ['id/ukpga/2007/3', 'id/ukpga/2011/25'], 'ewhc_qb_2022_1968': ['id/ukpga/1974/47', 'id/ukpga/2007/29'], 'ewhc_qb_2011_272': ['id/ukpga/1995/38', 'id/ukpga/Geo6and1Eliz2/15-16/66'], 'ewhc_ch_2012_4090': ['id/ukpga/1986/45', 'id/ukpga/Geo5/4-5/59'], 'ewhc_admin_2014_1106': ['id/ukpga/2003/41'], 'ewhc_ch_2022_740': ['id/ukpga/1985/6', 'id/ukpga/2006/46'], 'ewhc_ch_2015_3172': ['id/ukpga/1982/53'], 'ewhc_ch_2019_3339': ['id/ukpga/1986/45', 'id/ukpga/Geo5/15-16/19'], 'ewhc_admin_2009_3614': ['id/ukpga/1984/60', 'id/ukpga/2002/29'], 'ewhc_ch_2017_769': ['id/ukpga/1982/53', 'id/ukpga/1985/61', 'id/ukpga/1995/38', 'id/ukpga/Geo5/15-16/19', 'id/ukpga/Vict/59-60/35', 'id/ukpga/Will4and1Vict/7/26'], 'ewhc_admin_2009_3412': ['id/ukpga/1983/20', 'id/ukpga/Geo6/11-12/29'], 'ewhc_admin_2013_240': ['id/ukpga/1983/54', 'id/ukpga/1989/41'], 'ewhc_admin_2019_62': ['id/ukpga/1998/42', 'id/ukpga/2003/41'], 'ewhc_qb_2018_1948': ['id/ukpga/Geo6/8-9/28'], 'ewhc_admin_2011_2317': ['id/ukpga/1973/18', 'id/ukpga/1989/41', 'id/ukpga/1994/33', 'id/ukpga/1995/50', 'id/ukpga/1996/52', 'id/ukpga/2004/31', 'id/ukpga/2005/13', 'id/ukpga/2009/11'], 'ewhc_ch_2012_28': ['id/ukpga/1998/42'], 'ewhc_admin_2003_1837': ['id/ukpga/1988/50', 'id/ukpga/1992/53'], 'ewhc_admin_2009_535': ['id/ukpga/1983/54'], 'ewhc_ch_2012_3266': ['id/ukpga/1986/45', 'id/ukpga/Geo5/20-21/25'], 'ewhc_fam_2014_6': ['id/ukpga/1968/18', 'id/ukpga/1983/20', 'id/ukpga/1986/55', 'id/ukpga/1987/46', 'id/ukpga/1989/41', 'id/ukpga/Eliz2/8-9/65'], 'ewhc_ch_2025_46': ['id/ukpga/1998/41'], 'ewhc_ch_2018_2169': ['id/ukpga/1965/25', 'id/ukpga/1976/80', 'id/ukpga/1977/42', 'id/ukpga/1985/68', 'id/ukpga/1986/45', 'id/ukpga/1988/50', 'id/ukpga/1990/41', 'id/ukpga/1993/48', 'id/ukpga/1999/30', 'id/ukpga/Geo5/4-5/59'], 'ewhc_admin_2015_1603': ['id/ukpga/1968/60'], 'ewhc_admin_2013_195': ['id/ukpga/1986/64'], 'ewhc_admin_2014_2109': ['id/ukpga/1971/77', 'id/ukpga/1999/33'], 'ewhc_admin_2006_3346': ['id/ukpga/2003/41'], 'ewhc_qb_2019_3340': ['id/ukpga/1998/42', 'id/ukpga/Geo5/10-11/81', 'id/ukpga/Geo5/23-24/13'], 'ewhc_fam_2021_1153': ['id/ukpga/1969/46', 'id/ukpga/2002/38', 'id/ukpga/2006/35'], 'ewhc_admin_2011_2943': ['id/ukpga/1963/2', 'id/ukpga/1976/57', 'id/ukpga/1988/33'], 'ewhc_ch_2016_2759': ['id/ukpga/Geo5/15-16/20'], 'ewhc_ch_2018_2344': ['id/ukpga/Eliz2/6-7/53', 'id/ukpga/Geo5/15-16/18', 'id/ukpga/Geo5/15-16/19', 'id/ukpga/Geo5/15-16/20'], 'ewhc_admin_2009_3189': ['id/ukpga/1971/77', 'id/ukpga/1999/33', 'id/ukpga/2004/19'], 'ewhc_ch_2020_2624': ['id/ukpga/1985/6', 'id/ukpga/2006/46'], 'ewhc_admin_2011_2855': ['id/ukpga/1971/77'], 'ewhc_admin_2016_3693': ['id/ukpga/2006/35'], 'ewhc_ch_2020_1363': ['id/ukpga/2006/46'], 'ewcop_2019_10': ['id/ukpga/1983/19', 'id/ukpga/2005/9'], 'ewcop_2015_13': ['id/ukpga/Eliz2/8-9/65'], 'ewcop_2014_2': ['id/ukpga/1983/19', 'id/ukpga/1983/20', 'id/ukpga/1989/41', 'id/ukpga/2005/9'], 'ewca_civ_2023_652': ['id/ukpga/1995/50', 'id/ukpga/1996/17'], 'ewca_crim_2019_281': ['id/ukpga/1971/77', 'id/ukpga/1977/45', 'id/ukpga/1984/60'], 'ewca_crim_2007_1165': ['id/ukpga/1984/60'], 'ewca_civ_2005_1570': ['id/ukpga/1988/40', 'id/ukpga/1996/56', 'id/ukpga/Eliz2/5-6/31'], 'ewca_civ_2012_51': ['id/ukpga/1971/77', 'id/ukpga/2001/12', 'id/ukpga/2006/13'], 'ewca_civ_2013_1116': ['id/ukpga/1978/30', 'id/ukpga/1995/13'], 'ewca_civ_2017_2138': ['id/ukpga/1996/48', 'id/ukpga/Geo6/8-9/28'], 'ewca_civ_2010_1468': ['id/ukpga/1968/41', 'id/ukpga/1980/66', 'id/ukpga/1981/69', 'id/ukpga/2000/37', 'id/ukpga/2006/16'], 'ewca_crim_2005_1722': ['id/ukpga/1988/33'], 'ewca_crim_2013_2406': ['id/ukpga/1984/60'], 'ewca_civ_2003_167': ['id/ukpga/1999/33'], 'ewca_crim_2013_1026': ['id/ukpga/1977/45', 'id/ukpga/1984/60', 'id/ukpga/1985/56', 'id/ukpga/1990/18', 'id/ukpga/1996/25', 'id/ukpga/1998/29', 'id/ukpga/2000/23'], 'ewca_civ_2006_380': ['id/ukpga/1994/26'], 'ewca_crim_2011_2651': ['id/ukpga/1972/68'], 'ewca_civ_2015_718': ['id/ukpga/1967/9', 'id/ukpga/1988/41', 'id/ukpga/Edw7/8/64', 'id/ukpga/Eliz2/4-5/60', 'id/ukpga/Geo5/12-13/51', 'id/ukpga/Geo5/13-14/10', 'id/ukpga/Geo5/8-9/40'], 'ukftt_tc_2022_171': ['id/ukpga/1965/25', 'id/ukpga/1992/12'], 'ukftt_tc_2022_201': ['id/ukpga/1994/23'], 'ukftt_grc_2022_415': ['id/ukpga/1998/29', 'id/ukpga/2000/36', 'id/ukpga/2018/12'], 'uksc_2013_37': ['id/ukpga/1971/62', 'id/ukpga/1972/59', 'id/ukpga/1988/36', 'id/ukpga/2005/4', 'id/ukpga/Edw7/9/44', 'id/ukpga/Geo3/48/151', 'id/ukpga/Geo4/6/120'], 'ukut_lc_2024_202': ['id/ukpga/2004/34', 'id/ukpga/2016/22', 'id/ukpga/2019/4'], 'ukpc_2022_47': ['id/ukpga/Eliz2/6-7/53'], 'ewhc_ch_2021_324': ['id/ukpga/2011/25'], 'ewca_civ_2006_4': ['id/ukpga/1963/47', 'id/ukpga/1978/47', 'id/ukpga/1980/58', 'id/ukpga/Geo5/25-26/30', 'id/ukpga/Geo6/2-3/21'], 'ewhc_admin_2017_576': ['id/ukpga/1986/64'], 'ewca_civ_2005_647': ['id/ukpga/1983/2', 'id/ukpga/1984/60'], 'ukftt_grc_2025_287': ['id/ukpga/2000/36'], 'ukftt_grc_2025_251': ['id/ukpga/2000/36'], 'ewhc_admin_2014_2900': ['id/ukpga/2003/21'], 'ewhc_admin_2023_62': ['id/ukpga/1977/45', 'id/ukpga/2009/25'], 'ewhc_admin_2004_2240': ['id/ukpga/1980/43', 'id/ukpga/2000/6', 'id/ukpga/Eliz2/4-5/69', 'id/ukpga/Vict/24-25/100'], 'ewhc_admin_2011_754': ['id/ukpga/1968/73'], 'ewhc_tcc_2004_2991': ['id/ukpga/1980/58', 'id/ukpga/Geo6/2-3/21'], 'ewhc_admin_2011_441': ['id/ukpga/1971/77'], 'ewhc_admin_2012_2806': ['id/ukpga/2003/41'], 'ewhc_admin_2014_4153': ['id/ukpga/2003/41'], 'ewhc_admin_2004_3362': ['id/ukpga/1998/42'], 'ewca_crim_2024_147': ['id/ukpga/2006/35'], 'ewca_crim_2008_1194': ['id/ukpga/Vict/24-25/100'], 'ewca_crim_2009_469': ['id/ukpga/2008/4', 'id/ukpga/Vict/24-25/100'], 'ewca_crim_2008_1745': ['id/ukpga/1992/5'], 'ewca_civ_2006_140': ['id/ukpga/1996/52'], 'ewca_civ_2005_1770': ['id/ukpga/1973/18'], 'ewca_crim_2004_3092': ['id/ukpga/1971/38'], 'ukut_tcc_2023_244': ['id/ukpga/1970/9', 'id/ukpga/2007/11'], 'ewhc_qb_2014_4729': ['id/ukpga/2004/34'], 'ewhc_qb_2016_2355': ['id/ukpga/Eliz2/5-6/31'], 'ewca_civ_2004_988': ['id/ukpga/1980/58', 'id/ukpga/1986/45', 'id/ukpga/Vict/1-2/110']}\n",
      "Saved cluster 0 cases and acts to ../data/final_test/case_csvs/cluster0/cluster_0_cases.pkl\n",
      "                                             case_uri  \\\n",
      "5   https://caselaw.nationalarchives.gov.uk/ewhc/f...   \n",
      "6   https://caselaw.nationalarchives.gov.uk/ewhc/f...   \n",
      "74  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "75  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "76  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "\n",
      "                         para_id  \\\n",
      "5     ewhc_fam_2005_1832#para_14   \n",
      "6     ewhc_fam_2005_1832#para_36   \n",
      "74  ewhc_admin_2009_2348#para_22   \n",
      "75  ewhc_admin_2009_2348#para_58   \n",
      "76  ewhc_admin_2009_2348#para_62   \n",
      "\n",
      "                                           paragraphs references  \\\n",
      "5   14. The report concluded: “As G is likely to b...         []   \n",
      "6   36. Nor is it suggested, and certainly not est...         []   \n",
      "74  22. In relation to the “Gordon” request the Co...         []   \n",
      "75  58. At an early stage in the proceedings, whil...         []   \n",
      "76  62. In my view there are, as a matter of langu...         []   \n",
      "\n",
      "   if_law_applied                         application_of_law_phrases  \\\n",
      "5            True  ['it appears to me to be necessary for [the gr...   \n",
      "6           False                                                 []   \n",
      "74           True  [\"applying a 'dominant purpose' test\", 'the su...   \n",
      "75          False                                                 []   \n",
      "76          False                                                 []   \n",
      "\n",
      "                                               reason if_law_applied_llama  \\\n",
      "5   The report synthesizes legal principles regard...                False   \n",
      "6   This paragraph discusses the implications of p...                 True   \n",
      "74  The paragraph shows the Commissioner applying ...                False   \n",
      "75  This paragraph discusses an interpretation of ...                 True   \n",
      "76  This paragraph discusses the interpretation of...                 True   \n",
      "\n",
      "                     application_of_law_phrases_llama  \\\n",
      "5                                                  []   \n",
      "6   ['Nor is it suggested, and certainly not estab...   \n",
      "74                                                 []   \n",
      "75  ['It seems to me a possible, but not natural o...   \n",
      "76  ['The phrase ‘held for purposes other than’ ca...   \n",
      "\n",
      "                                         reason_llama if_law_applied_claude  \\\n",
      "5   This paragraph presents a report's recommendat...                  True   \n",
      "6   The judge applies legal criteria for granting ...                  True   \n",
      "74  This paragraph discusses the approach taken by...                  True   \n",
      "75  The judge applies principles of statutory inte...                  True   \n",
      "76  The judge applies principles of statutory inte...                  True   \n",
      "\n",
      "                    application_of_law_phrases_claude  \\\n",
      "5   ['it appears to me to be necessary for [the gr...   \n",
      "6   ['Nor is it suggested, and certainly not estab...   \n",
      "74  [\"applying a 'dominant purpose' test\", \"the in...   \n",
      "75  ['It seems to me a possible, but not natural o...   \n",
      "76  [\"The phrase 'held for purposes other than' ca...   \n",
      "\n",
      "                                        reason_claude confidence  \\\n",
      "5   The paragraph contains application of law thro...       High   \n",
      "6   The judge is applying the legal test for grant...       High   \n",
      "74  Model A's analysis is more accurate. While the...       High   \n",
      "75  Model B's analysis is more accurate. This para...       High   \n",
      "76  Model B's analysis is more accurate. This para...       High   \n",
      "\n",
      "   agreement_with  final_annotation             case_name  \n",
      "5          OpenAI              True    ewhc_fam_2005_1832  \n",
      "6           Llama              True    ewhc_fam_2005_1832  \n",
      "74         OpenAI              True  ewhc_admin_2009_2348  \n",
      "75          Llama              True  ewhc_admin_2009_2348  \n",
      "76          Llama              True  ewhc_admin_2009_2348  \n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_fam_2005_1832.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2009_2348.csv with 39 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2012_1033.csv with 26 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_fam_2018_3795.csv with 22 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2007_1304.csv with 13 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_kb_2023_1256.csv with 34 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_comm_2020_3334.csv with 33 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2015_3084.csv with 35 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_ch_2024_560.csv with 74 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_qb_2015_1760.csv with 33 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_ch_2022_1348.csv with 31 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_qb_2022_977.csv with 56 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_fam_2014_4643.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_fam_2017_917.csv with 4 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2017_1818.csv with 11 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_ch_2006_2612.csv with 55 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2004_563.csv with 6 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2013_1009.csv with 32 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_ch_2021_1275.csv with 19 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_kb_2023_3472.csv with 8 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_fam_2017_3164.csv with 29 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_fam_2014_3135.csv with 40 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2020_1522.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_ch_2020_3295.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2008_2013.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2003_1321.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2008_1043.csv with 24 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2020_2352.csv with 9 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_qb_2020_718.csv with 27 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2005_2278.csv with 13 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2013_1786.csv with 45 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_fam_2017_1782.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2019_1809.csv with 21 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_tcc_2009_1664.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2010_439.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2020_3243.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_ch_2010_938.csv with 23 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2012_3226.csv with 11 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_ch_2017_1770.csv with 31 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_ipec_2019_126.csv with 52 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2005_1393.csv with 26 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_ch_2023_264.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_ch_2019_2517.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_qb_2003_1814.csv with 21 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_qb_2017_2554.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_comm_2023_2866.csv with 37 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_ch_2019_1839.csv with 83 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_fam_2022_2677.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewcop_2020_23.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_civ_2011_1172.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_crim_2016_1941.csv with 27 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_civ_2015_1419.csv with 24 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_civ_2010_805.csv with 19 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_crim_2010_1474.csv with 13 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_crim_2017_2314.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_civ_2004_1269.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_crim_2009_2204.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_civ_2023_239.csv with 51 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_civ_2015_1230.csv with 15 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_civ_2010_390.csv with 39 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_civ_2003_135.csv with 27 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_civ_2018_162.csv with 15 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_civ_2006_529.csv with 31 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_crim_2017_2063.csv with 19 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_crim_2009_201.csv with 6 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_crim_2017_1779.csv with 8 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_civ_2017_20.csv with 37 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_crim_2006_707.csv with 2 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_civ_2021_252.csv with 21 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_civ_2012_1660.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_crim_2015_1256.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_crim_2014_382.csv with 15 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_crim_2023_202.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ukftt_tc_2024_744.csv with 53 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ukftt_grc_2024_471.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ukftt_grc_2024_1087.csv with 25 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewfc_2014_9.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewfc_2022_34.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewfc_2019_60.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewfc_2016_25.csv with 26 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ukut_lc_2022_342.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ukut_aac_2016_355.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/uksc_2011_41.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ukftt_grc_2025_284.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_ch_2013_4630.csv with 21 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_civ_2018_764.csv with 19 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2003_2779.csv with 9 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ukftt_grc_2025_282.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ukut_aac_2022_263.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_civ_2015_414.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ukftt_grc_2025_289.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_costs_2003_9050.csv with 11 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2009_590.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_admin_2011_2962.csv with 2 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewhc_fam_2022_2120.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_crim_2010_1450.csv with 13 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_crim_2024_313.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ewca_crim_2004_621.csv with 11 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ukftt_grc_2024_333.csv with 6 rows\n",
      "Created ../data/final_test/case_csvs/cluster1/ukut_iac_2015_268.csv with 2 rows\n",
      "{'ewhc_fam_2005_1832': ['id/ukpga/1989/41', 'id/ukpga/1998/42'], 'ewhc_admin_2009_2348': ['id/ukpga/1998/29'], 'ewhc_admin_2012_1033': ['id/ukpga/1992/14', 'id/ukpga/1992/20', 'id/ukpga/Geo5/15-16/20'], 'ewhc_fam_2018_3795': ['id/ukpga/1989/41'], 'ewhc_admin_2007_1304': ['id/ukpga/1985/6', 'id/ukpga/1988/33', 'id/ukpga/2002/29'], 'ewhc_kb_2023_1256': ['id/ukpga/1988/48', 'id/ukpga/1996/31', 'id/ukpga/2004/37', 'id/ukpga/2013/26', 'id/ukpga/2019/24'], 'ewhc_comm_2020_3334': ['id/ukpga/1979/54'], 'ewhc_admin_2015_3084': ['id/ukpga/1990/9', 'id/ukpga/2004/5'], 'ewhc_ch_2024_560': ['id/ukpga/1978/47'], 'ewhc_qb_2015_1760': ['id/ukpga/Eliz2/10-11/19', 'id/ukpga/Geo6/10-11/44', 'id/ukpga/Geo6/11-12/56'], 'ewhc_ch_2022_1348': ['id/ukpga/1985/61', 'id/ukpga/2006/46', 'id/ukpga/Geo6/11-12/38'], 'ewhc_qb_2022_977': ['id/ukpga/2015/15'], 'ewhc_fam_2014_4643': ['id/ukpga/1989/41', 'id/ukpga/1998/42'], 'ewhc_fam_2017_917': ['id/ukpga/1969/58', 'id/ukpga/1989/41', 'id/ukpga/2002/38'], 'ewhc_admin_2017_1818': ['id/ukpga/1990/9', 'id/ukpga/2011/20'], 'ewhc_ch_2006_2612': ['id/ukpga/2004/12'], 'ewhc_admin_2004_563': ['id/ukpga/1989/33', 'id/ukpga/Eliz2/8-9/65'], 'ewhc_admin_2013_1009': ['id/ukpga/1983/54', 'id/ukpga/1984/24', 'id/ukpga/1996/16', 'id/ukpga/1998/42', 'id/ukpga/2002/30', 'id/ukpga/2011/13'], 'ewhc_ch_2021_1275': ['id/ukpga/Geo5/15-16/20'], 'ewhc_kb_2023_3472': ['id/ukpga/1981/49'], 'ewhc_fam_2017_3164': ['id/ukpga/1973/18', 'id/ukpga/1989/41', 'id/ukpga/1991/48', 'id/ukpga/1996/27'], 'ewhc_fam_2014_3135': ['id/ukpga/1989/41', 'id/ukpga/1990/37', 'id/ukpga/1998/42', 'id/ukpga/2002/38', 'id/ukpga/2003/41', 'id/ukpga/2006/47', 'id/ukpga/2008/22'], 'ewhc_admin_2020_1522': ['id/ukpga/2003/41'], 'ewhc_ch_2020_3295': ['id/ukpga/1975/63', 'id/ukpga/1982/53', 'id/ukpga/2011/25'], 'ewhc_admin_2008_2013': ['id/ukpga/1980/43', 'id/ukpga/1992/14', 'id/ukpga/1992/20'], 'ewhc_admin_2003_1321': ['id/ukpga/1998/29', 'id/ukpga/1998/37', 'id/ukpga/1998/42'], 'ewhc_admin_2008_1043': ['id/ukpga/1974/47'], 'ewhc_admin_2020_2352': ['id/ukpga/1967/80'], 'ewhc_qb_2020_718': ['id/ukpga/1984/27', 'id/ukpga/Geo6/8-9/28'], 'ewhc_admin_2005_2278': ['id/ukpga/1968/73'], 'ewhc_admin_2013_1786': ['id/ukpga/1994/13', 'id/ukpga/2009/25', 'id/ukpga/2013/18'], 'ewhc_fam_2017_1782': ['id/ukpga/1986/55', 'id/ukpga/2008/22'], 'ewhc_admin_2019_1809': ['id/ukpga/2003/41'], 'ewhc_tcc_2009_1664': ['id/ukpga/1996/53'], 'ewhc_admin_2010_439': ['id/ukpga/1988/33'], 'ewhc_admin_2020_3243': ['id/ukpga/1971/38', 'id/ukpga/1976/63', 'id/ukpga/1980/43', 'id/ukpga/1985/23', 'id/ukpga/1999/23', 'id/ukpga/2002/29', 'id/ukpga/Eliz2/1-2/14'], 'ewhc_ch_2010_938': ['id/ukpga/1970/9', 'id/ukpga/1980/43', 'id/ukpga/2003/1', 'id/ukpga/2005/11'], 'ewhc_admin_2012_3226': ['id/ukpga/1998/37'], 'ewhc_ch_2017_1770': ['id/ukpga/1973/18', 'id/ukpga/1996/47'], 'ewhc_ipec_2019_126': ['id/ukpga/1977/37'], 'ewhc_admin_2005_1393': ['id/ukpga/1988/33', 'id/ukpga/1988/52', 'id/ukpga/1988/53'], 'ewhc_ch_2023_264': ['id/ukpga/2006/46'], 'ewhc_ch_2019_2517': ['id/ukpga/1974/39', 'id/ukpga/2000/8'], 'ewhc_qb_2003_1814': ['id/ukpga/1980/58'], 'ewhc_qb_2017_2554': ['id/ukpga/1986/32', 'id/ukpga/1988/33', 'id/ukpga/2002/29'], 'ewhc_comm_2023_2866': ['id/ukpga/1981/61', 'id/ukpga/1990/42', 'id/ukpga/2003/21', 'id/ukpga/2018/13', 'id/ukpga/Geo6/6-7/40'], 'ewhc_ch_2019_1839': ['id/ukpga/1974/39', 'id/ukpga/1986/45', 'id/ukpga/2006/14'], 'ewhc_fam_2022_2677': ['id/ukpga/1985/60'], 'ewcop_2020_23': ['id/ukpga/2005/9'], 'ewca_civ_2011_1172': ['id/ukpga/2004/19', 'id/ukpga/2004/31'], 'ewca_crim_2016_1941': ['id/ukpga/1994/33', 'id/ukpga/Eliz2/4-5/69'], 'ewca_civ_2015_1419': ['id/ukpga/1992/51', 'id/ukpga/1997/40', 'id/ukpga/1998/42'], 'ewca_civ_2010_805': ['id/ukpga/1974/47'], 'ewca_crim_2010_1474': ['id/ukpga/1974/37'], 'ewca_crim_2017_2314': ['id/ukpga/1994/33'], 'ewca_civ_2004_1269': ['id/ukpga/1986/60', 'id/ukpga/1999/22'], 'ewca_crim_2009_2204': ['id/ukpga/1982/16'], 'ewca_civ_2023_239': ['id/ukpga/1978/47', 'id/ukpga/1996/40', 'id/ukpga/Geo6/2-3/37'], 'ewca_civ_2015_1230': ['id/ukpga/1998/29'], 'ewca_civ_2010_390': ['id/ukpga/1988/48'], 'ewca_civ_2003_135': ['id/ukpga/1976/74'], 'ewca_civ_2018_162': ['id/ukpga/2002/29'], 'ewca_civ_2006_529': ['id/ukpga/1964/30', 'id/ukpga/1988/34', 'id/ukpga/1989/41', 'id/ukpga/1999/22'], 'ewca_crim_2017_2063': ['id/ukpga/1988/33'], 'ewca_crim_2009_201': ['id/ukpga/2002/29'], 'ewca_crim_2017_1779': ['id/ukpga/1972/70', 'id/ukpga/1985/23'], 'ewca_civ_2017_20': ['id/ukpga/1992/52', 'id/ukpga/1999/26'], 'ewca_crim_2006_707': ['id/ukpga/1981/45', 'id/ukpga/1981/47', 'id/ukpga/1999/33', 'id/ukpga/2000/6'], 'ewca_civ_2021_252': ['id/ukpga/1980/58', 'id/ukpga/1997/12', 'id/ukpga/2003/39'], 'ewca_civ_2012_1660': ['id/ukpga/1980/58', 'id/ukpga/2002/9'], 'ewca_crim_2015_1256': ['id/ukpga/1995/21'], 'ewca_crim_2014_382': ['id/ukpga/1968/60', 'id/ukpga/2002/29', 'id/ukpga/2006/35'], 'ewca_crim_2023_202': ['id/ukpga/1988/33'], 'ukftt_tc_2024_744': ['id/ukpga/1970/9', 'id/ukpga/1992/4', 'id/ukpga/2008/29', 'id/ukpga/2008/30', 'id/ukpga/2008/9', 'id/ukpga/2020/7'], 'ukftt_grc_2024_471': ['id/ukpga/2000/36'], 'ukftt_grc_2024_1087': ['id/ukpga/1998/29', 'id/ukpga/2000/36', 'id/ukpga/2018/12'], 'ewfc_2014_9': ['id/ukpga/1989/40', 'id/ukpga/2013/22'], 'ewfc_2022_34': ['id/ukpga/1986/55', 'id/ukpga/1998/42', 'id/ukpga/2002/38', 'id/ukpga/2008/22'], 'ewfc_2019_60': ['id/ukpga/1968/64', 'id/ukpga/1989/41', 'id/ukpga/Vict/24-25/100'], 'ewfc_2016_25': ['id/ukpga/1981/61', 'id/ukpga/2002/38'], 'ukut_lc_2022_342': ['id/ukpga/Geo5/15-16/20'], 'ukut_aac_2016_355': ['id/ukpga/1995/18', 'id/ukpga/1998/14', 'id/ukpga/2013/17'], 'uksc_2011_41': ['id/ukpga/1998/39'], 'ukftt_grc_2025_284': ['id/ukpga/2000/36'], 'ewhc_ch_2013_4630': ['id/ukpga/1986/45'], 'ewca_civ_2018_764': ['id/ukpga/1993/28', 'id/ukpga/Geo5/15-16/20'], 'ewhc_admin_2003_2779': ['id/ukpga/1970/9'], 'ukftt_grc_2025_282': ['id/ukpga/2008/30'], 'ukut_aac_2022_263': ['id/ukpga/1981/14'], 'ewca_civ_2015_414': ['id/ukpga/1985/6', 'id/ukpga/2006/46'], 'ukftt_grc_2025_289': ['id/ukpga/2018/12'], 'ewhc_costs_2003_9050': ['id/ukpga/1990/41'], 'ewhc_admin_2009_590': ['id/ukpga/1970/9', 'id/ukpga/1999/2'], 'ewhc_admin_2011_2962': ['id/ukpga/1988/52'], 'ewhc_fam_2022_2120': ['id/ukpga/1989/41'], 'ewca_crim_2010_1450': ['id/ukpga/Vict/24-25/100'], 'ewca_crim_2024_313': ['id/ukpga/1968/27', 'id/ukpga/1971/38', 'id/ukpga/1977/45', 'id/ukpga/1981/45'], 'ewca_crim_2004_621': ['id/ukpga/1968/20', 'id/ukpga/Eliz2/5-6/53'], 'ukftt_grc_2024_333': ['id/ukpga/2000/36'], 'ukut_iac_2015_268': ['id/ukpga/1999/33']}\n",
      "Saved cluster 1 cases and acts to ../data/final_test/case_csvs/cluster1/cluster_1_cases.pkl\n",
      "                                             case_uri  \\\n",
      "3   https://caselaw.nationalarchives.gov.uk/ewhc/c...   \n",
      "4   https://caselaw.nationalarchives.gov.uk/ewhc/c...   \n",
      "29  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "30  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "31  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "\n",
      "                         para_id  \\\n",
      "3     ewhc_comm_2006_777#para_25   \n",
      "4     ewhc_comm_2006_777#para_27   \n",
      "29   ewhc_admin_2015_2868#para_2   \n",
      "30   ewhc_admin_2015_2868#para_7   \n",
      "31  ewhc_admin_2015_2868#para_10   \n",
      "\n",
      "                                           paragraphs  \\\n",
      "3   25. Had it been necessary to decide the issue,...   \n",
      "4   27. In my judgment this court does not have ju...   \n",
      "29  2. Having regard to the urgency of the matter,...   \n",
      "30  7. A also was notified of the CPS decision. On...   \n",
      "31  10. On 15 June 2015, solicitors instructed by ...   \n",
      "\n",
      "                                           references if_law_applied  \\\n",
      "3                                                  []          False   \n",
      "4                                                  []          False   \n",
      "29  [{'text': 'Contempt of Court Act 1981', 'href'...          False   \n",
      "30                                                 []           True   \n",
      "31                                                 []           True   \n",
      "\n",
      "                           application_of_law_phrases  \\\n",
      "3                                                  []   \n",
      "4                                                  []   \n",
      "29                                                 []   \n",
      "30  ['exercised her right under the Victim Right t...   \n",
      "31  ['the magistrates’ court would have no jurisdi...   \n",
      "\n",
      "                                               reason if_law_applied_llama  \\\n",
      "3   This paragraph discusses jurisdiction and proc...                 True   \n",
      "4   The paragraph discusses the jurisdictional iss...                 True   \n",
      "29  This paragraph discusses procedural matters an...                 True   \n",
      "30  The paragraph applies the principles outlined ...                False   \n",
      "31  The paragraph applies principles of judicial r...                False   \n",
      "\n",
      "                     application_of_law_phrases_llama  \\\n",
      "3   ['I would hold that the time when the court mu...   \n",
      "4   ['In my judgment this court does not have juri...   \n",
      "29  ['on the basis that criminal proceedings are a...   \n",
      "30                                                 []   \n",
      "31                                                 []   \n",
      "\n",
      "                                         reason_llama if_law_applied_claude  \\\n",
      "3   The court applies legal principles regarding j...                  True   \n",
      "4   The judge applies legal principles related to ...                  True   \n",
      "29  The court applies the Contempt of Court Act 19...                  True   \n",
      "30  This paragraph is primarily background informa...                  True   \n",
      "31  This paragraph provides background procedural ...                  True   \n",
      "\n",
      "                    application_of_law_phrases_claude  \\\n",
      "3   ['I would hold that the time when the court mu...   \n",
      "4   [\"In my judgment this court does not have juri...   \n",
      "29  ['on the basis that criminal proceedings are a...   \n",
      "30  ['exercised her right under the Victim Right t...   \n",
      "31  [\"the magistrates' court would have no jurisdi...   \n",
      "\n",
      "                                        reason_claude confidence  \\\n",
      "3   Model B's analysis is more accurate. While thi...       High   \n",
      "4   Model B's analysis is more accurate. The parag...       High   \n",
      "29  Model B's analysis is more accurate. This para...       High   \n",
      "30  Model A's analysis is more accurate. While the...       High   \n",
      "31  Model A's analysis is more accurate. While the...       High   \n",
      "\n",
      "   agreement_with  final_annotation             case_name  \n",
      "3           Llama              True    ewhc_comm_2006_777  \n",
      "4           Llama              True    ewhc_comm_2006_777  \n",
      "29          Llama              True  ewhc_admin_2015_2868  \n",
      "30         OpenAI              True  ewhc_admin_2015_2868  \n",
      "31         OpenAI              True  ewhc_admin_2015_2868  \n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_comm_2006_777.csv with 11 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_admin_2015_2868.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_pat_2005_2240.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_ch_2014_763.csv with 26 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_ch_2023_175.csv with 41 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_ch_2023_4.csv with 59 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_ch_2020_3622.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_ch_2016_1076.csv with 41 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_admin_2024_2864.csv with 24 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_qb_2020_1689.csv with 7 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_ch_2012_858.csv with 9 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_ch_2009_74.csv with 28 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_qb_2013_797.csv with 76 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_ch_2022_2033.csv with 15 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_qb_2018_2135.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_admin_2004_1069.csv with 6 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_fam_2005_402.csv with 33 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_fam_2004_1066.csv with 11 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_admin_2003_1578.csv with 65 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_admin_2006_2784.csv with 60 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_admin_2019_84.csv with 29 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_qb_2015_1060.csv with 23 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_pat_2014_4242.csv with 25 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_tcc_2012_2593.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_fam_2023_1096.csv with 27 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_admin_2008_738.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_ch_2003_2845.csv with 21 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_admin_2007_807.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_qb_2015_926.csv with 47 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_comm_2011_1372.csv with 23 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_comm_2008_1785.csv with 19 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_admin_2016_2186.csv with 36 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_tcc_2011_87.csv with 23 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_fam_2018_3841.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_fam_2010_1346.csv with 17 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_ch_2017_2621.csv with 33 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_ch_2004_2947.csv with 15 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_ch_2012_731.csv with 45 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_qb_2022_1917.csv with 6 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewcop_2020_14.csv with 9 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewcop_2021_32.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2023_1000.csv with 32 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2016_1267.csv with 23 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_crim_2008_854.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_crim_2007_3432.csv with 4 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2022_1943.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_crim_2007_3223.csv with 13 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2009_454.csv with 17 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_crim_2017_2065.csv with 11 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2013_554.csv with 27 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2011_1539.csv with 6 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_crim_2023_630.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2012_1395.csv with 89 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2015_1085.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2020_833.csv with 31 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2014_935.csv with 17 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2003_151.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_crim_2023_1520.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2021_763.csv with 35 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2005_1479.csv with 6 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2011_1515.csv with 33 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2024_16.csv with 31 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ukftt_grc_2023_35.csv with 6 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ukftt_tc_2024_462.csv with 48 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ukftt_tc_2024_139.csv with 39 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewfc_b_2024_40.csv with 28 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewfc_2024_6.csv with 23 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewfc_b_2024_69.csv with 81 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/uksc_2013_8.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ukut_aac_2023_288.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ukpc_2023_36.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ukpc_2009_42.csv with 11 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ukpc_2013_21.csv with 17 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewcc_2024_7.csv with 42 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2007_826.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewfc_2025_41.csv with 17 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_admin_2020_801.csv with 23 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_ch_2005_1508.csv with 4 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_admin_2006_3048.csv with 4 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewhc_admin_2008_470.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_crim_2023_1121.csv with 11 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_crim_2013_1764.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_crim_2006_757.csv with 3 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_crim_2009_288.csv with 8 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_crim_2004_492.csv with 3 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_crim_2007_2847.csv with 7 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ukftt_tc_2023_959.csv with 4 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ukut_aac_2021_69.csv with 6 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ukait_2009_41.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster2/ewca_civ_2012_543.csv with 11 rows\n",
      "{'ewhc_comm_2006_777': ['id/ukpga/1982/27', 'id/ukpga/1999/31'], 'ewhc_admin_2015_2868': ['id/ukpga/1981/49', 'id/ukpga/1998/37', 'id/ukpga/1998/42'], 'ewhc_pat_2005_2240': ['id/ukpga/1977/37', 'id/ukpga/2004/16'], 'ewhc_ch_2014_763': ['id/ukpga/1967/7', 'id/ukpga/1977/50', 'id/ukpga/1986/45', 'id/ukpga/2002/40'], 'ewhc_ch_2023_175': ['id/ukpga/1985/6', 'id/ukpga/2006/46'], 'ewhc_ch_2023_4': ['id/ukpga/1968/64', 'id/ukpga/1972/30', 'id/ukpga/1995/38', 'id/ukpga/1997/12'], 'ewhc_ch_2020_3622': ['id/ukpga/1986/45', 'id/ukpga/2006/46'], 'ewhc_ch_2016_1076': ['id/ukpga/1985/6', 'id/ukpga/2006/46'], 'ewhc_admin_2024_2864': ['id/ukpga/1996/16', 'id/ukpga/2018/23', 'id/ukpga/Vict/24-25/100'], 'ewhc_qb_2020_1689': ['id/ukpga/2002/29'], 'ewhc_ch_2012_858': ['id/ukpga/1964/55'], 'ewhc_ch_2009_74': ['id/ukpga/1985/6', 'id/ukpga/2006/46'], 'ewhc_qb_2013_797': ['id/ukpga/1980/58', 'id/ukpga/1998/42', 'id/ukpga/2006/46', 'id/ukpga/Geo5/1-2/6'], 'ewhc_ch_2022_2033': ['id/ukpga/1996/47', 'id/ukpga/Geo5/15-16/23'], 'ewhc_qb_2018_2135': ['id/ukpga/1988/52'], 'ewhc_admin_2004_1069': ['id/ukpga/1992/12'], 'ewhc_fam_2005_402': ['id/ukpga/1973/18'], 'ewhc_fam_2004_1066': ['id/ukpga/1973/18'], 'ewhc_admin_2003_1578': ['id/ukpga/1965/64', 'id/ukpga/1980/66', 'id/ukpga/1998/42', 'id/ukpga/Geo5/22-23/45', 'id/ukpga/Vict/20-21/31'], 'ewhc_admin_2006_2784': ['id/ukpga/1983/54', 'id/ukpga/1984/60', 'id/ukpga/1999/22', 'id/ukpga/2002/17'], 'ewhc_admin_2019_84': ['id/ukpga/1980/43', 'id/ukpga/1998/37', 'id/ukpga/2014/12'], 'ewhc_qb_2015_1060': ['id/ukpga/1983/20', 'id/ukpga/2005/9'], 'ewhc_pat_2014_4242': ['id/ukpga/1972/68', 'id/ukpga/1988/48', 'id/ukpga/1994/26', 'id/ukpga/2014/18', 'id/ukpga/Geo6/12-13-14/88'], 'ewhc_tcc_2012_2593': ['id/ukpga/1991/57'], 'ewhc_fam_2023_1096': ['id/ukpga/1984/42'], 'ewhc_admin_2008_738': ['id/ukpga/1970/42', 'id/ukpga/1989/41', 'id/ukpga/2002/38'], 'ewhc_ch_2003_2845': ['id/ukpga/1994/23'], 'ewhc_admin_2007_807': ['id/ukpga/1971/23', 'id/ukpga/2005/4'], 'ewhc_qb_2015_926': ['id/ukpga/1982/27', 'id/ukpga/1987/43', 'id/ukpga/1995/42'], 'ewhc_comm_2011_1372': ['id/ukpga/1998/41', 'id/ukpga/1999/31'], 'ewhc_comm_2008_1785': ['id/ukpga/Edw7/7/24'], 'ewhc_admin_2016_2186': ['id/ukpga/1965/57', 'id/ukpga/1976/35', 'id/ukpga/1978/30', 'id/ukpga/1987/4', 'id/ukpga/1996/16', 'id/ukpga/2001/24', 'id/ukpga/2003/20', 'id/ukpga/2004/20', 'id/ukpga/2012/11', 'id/ukpga/2013/25', 'id/ukpga/Eliz2/2-3/32'], 'ewhc_tcc_2011_87': ['id/ukpga/1998/42', 'id/ukpga/2002/1'], 'ewhc_fam_2018_3841': ['id/ukpga/1984/42'], 'ewhc_fam_2010_1346': ['id/ukpga/1989/41'], 'ewhc_ch_2017_2621': ['id/ukpga/1980/22', 'id/ukpga/2006/46', 'id/ukpga/Geo6/11-12/38'], 'ewhc_ch_2004_2947': ['id/ukpga/1986/45', 'id/ukpga/Geo6/11-12/38'], 'ewhc_ch_2012_731': ['id/ukpga/1980/58'], 'ewhc_qb_2022_1917': ['id/ukpga/1981/49'], 'ewcop_2020_14': ['id/ukpga/2005/9'], 'ewcop_2021_32': ['id/ukpga/1983/20', 'id/ukpga/2005/9'], 'ewca_civ_2023_1000': ['id/ukpga/2013/26'], 'ewca_civ_2016_1267': ['id/ukpga/1977/50', 'id/ukpga/1990/36'], 'ewca_crim_2008_854': ['id/ukpga/2002/29'], 'ewca_crim_2007_3432': ['id/ukpga/2002/29'], 'ewca_civ_2022_1943': ['id/ukpga/1967/7'], 'ewca_crim_2007_3223': ['id/ukpga/1983/20', 'id/ukpga/2000/6'], 'ewca_civ_2009_454': ['id/ukpga/1976/74', 'id/ukpga/1978/44', 'id/ukpga/1996/17', 'id/ukpga/1996/18'], 'ewca_crim_2017_2065': ['id/ukpga/1988/52', 'id/ukpga/2009/25'], 'ewca_civ_2013_554': ['id/ukpga/1998/42'], 'ewca_civ_2011_1539': ['id/ukpga/Vict/1-2/110'], 'ewca_crim_2023_630': ['id/ukpga/1986/64', 'id/ukpga/1988/33'], 'ewca_civ_2012_1395': ['id/ukpga/1973/18', 'id/ukpga/1986/45', 'id/ukpga/2006/46'], 'ewca_civ_2015_1085': ['id/ukpga/1995/38'], 'ewca_civ_2020_833': ['id/ukpga/Eliz2/2-3/56'], 'ewca_civ_2014_935': ['id/ukpga/2006/46'], 'ewca_civ_2003_151': ['id/ukpga/1984/60'], 'ewca_crim_2023_1520': ['id/ukpga/1997/40', 'id/ukpga/2020/17'], 'ewca_civ_2021_763': ['id/ukpga/1983/54'], 'ewca_civ_2005_1479': ['id/ukpga/1998/42'], 'ewca_civ_2011_1515': ['id/ukpga/1992/4', 'id/ukpga/1992/5', 'id/ukpga/2000/14', 'id/ukpga/2006/41', 'id/ukpga/2007/12', 'id/ukpga/Geo6/11-12/29'], 'ewca_civ_2024_16': ['id/ukpga/1970/9', 'id/ukpga/1986/32', 'id/ukpga/1998/14', 'id/ukpga/2002/21', 'id/ukpga/2007/15', 'id/ukpga/2008/9', 'id/ukpga/2012/10', 'id/ukpga/2012/5'], 'ukftt_grc_2023_35': ['id/ukpga/2018/12'], 'ukftt_tc_2024_462': ['id/ukpga/2005/5', 'id/ukpga/2007/3', 'id/ukpga/2012/14'], 'ukftt_tc_2024_139': ['id/ukpga/1970/9', 'id/ukpga/2005/19', 'id/ukpga/2008/9'], 'ewfc_b_2024_40': ['id/ukpga/1989/41', 'id/ukpga/1998/42', 'id/ukpga/2021/17'], 'ewfc_2024_6': ['id/ukpga/1973/18'], 'ewfc_b_2024_69': ['id/ukpga/1989/41', 'id/ukpga/1996/27', 'id/ukpga/2018/12', 'id/ukpga/2021/17'], 'uksc_2013_8': ['id/ukpga/1989/41', 'id/ukpga/Vict/23-24/127'], 'ukut_aac_2023_288': ['id/ukpga/1992/5', 'id/ukpga/2007/15', 'id/ukpga/2012/5'], 'ukpc_2023_36': ['id/ukpga/1976/54', 'id/ukpga/2009/25'], 'ukpc_2009_42': ['id/ukpga/Eliz2/5-6/11'], 'ukpc_2013_21': ['id/ukpga/1971/77'], 'ewcc_2024_7': ['id/ukpga/1984/28', 'id/ukpga/1986/45', 'id/ukpga/2002/29', 'id/ukpga/2002/9', 'id/ukpga/Geo5/15-16/20'], 'ewca_civ_2007_826': ['id/ukpga/Eliz2/2-3/56'], 'ewfc_2025_41': ['id/ukpga/1984/42', 'id/ukpga/1986/55', 'id/ukpga/1989/41', 'id/ukpga/1990/37', 'id/ukpga/2008/22', 'id/ukpga/Eliz2/1-2/20'], 'ewhc_admin_2020_801': ['id/ukpga/1972/11', 'id/ukpga/1972/70', 'id/ukpga/1980/66', 'id/ukpga/1981/67', 'id/ukpga/1991/57', 'id/ukpga/1998/42', 'id/ukpga/2006/26', 'id/ukpga/2008/29', 'id/ukpga/2010/29', 'id/ukpga/2016/22', 'id/ukpga/2017/20', 'id/ukpga/Vict/20-21/31'], 'ewhc_ch_2005_1508': ['id/ukpga/1990/9'], 'ewhc_admin_2006_3048': ['id/ukpga/1999/33'], 'ewhc_admin_2008_470': ['id/ukpga/1968/60', 'id/ukpga/1989/33', 'id/ukpga/2002/29', 'id/ukpga/2003/41'], 'ewca_crim_2023_1121': ['id/ukpga/1971/77', 'id/ukpga/1981/47', 'id/ukpga/2010/40', 'id/ukpga/2022/36'], 'ewca_crim_2013_1764': ['id/ukpga/2000/6'], 'ewca_crim_2006_757': ['id/ukpga/1988/33'], 'ewca_crim_2009_288': ['id/ukpga/1968/60', 'id/ukpga/1985/23', 'id/ukpga/1988/33', 'id/ukpga/1996/49', 'id/ukpga/2002/29'], 'ewca_crim_2004_492': ['id/ukpga/1994/26'], 'ewca_crim_2007_2847': ['id/ukpga/1988/52', 'id/ukpga/Vict/24-25/100'], 'ukftt_tc_2023_959': ['id/ukpga/2003/14', 'id/ukpga/2020/14'], 'ukut_aac_2021_69': ['id/ukpga/1998/14', 'id/ukpga/1998/42', 'id/ukpga/2007/15', 'id/ukpga/2013/17'], 'ukait_2009_41': ['id/ukpga/1998/42'], 'ewca_civ_2012_543': ['id/ukpga/1996/18']}\n",
      "Saved cluster 2 cases and acts to ../data/final_test/case_csvs/cluster2/cluster_2_cases.pkl\n",
      "                                              case_uri  \\\n",
      "99   https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "100  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "101  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "102  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "103  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "\n",
      "                          para_id  \\\n",
      "99   ewhc_admin_2013_1818#para_29   \n",
      "100  ewhc_admin_2013_1818#para_39   \n",
      "101  ewhc_admin_2013_1818#para_46   \n",
      "102  ewhc_admin_2013_1818#para_60   \n",
      "103  ewhc_admin_2013_1818#para_63   \n",
      "\n",
      "                                            paragraphs  \\\n",
      "99   29. In Nirula , supra, the claimant relied upo...   \n",
      "100  39. However, there is no evidence before me, a...   \n",
      "101  46. About an hour later, at 3.32pm, the Secret...   \n",
      "102  60. Moreover, the FTT itself recognised that t...   \n",
      "103  63. Thirdly, the reference to s.8 8 (2)(d) of ...   \n",
      "\n",
      "                                            references if_law_applied  \\\n",
      "99                                                  []          False   \n",
      "100                                                 []          False   \n",
      "101  [{'text': 'the 2004 Act', 'href': 'http://www....           True   \n",
      "102                                                 []          False   \n",
      "103  [{'text': 'the 2002 Act', 'href': 'http://www....          False   \n",
      "\n",
      "                            application_of_law_phrases  \\\n",
      "99                                                  []   \n",
      "100                                                 []   \n",
      "101  [\"the Secretary of State certifying the Claima...   \n",
      "102                                                 []   \n",
      "103                                                 []   \n",
      "\n",
      "                                                reason if_law_applied_llama  \\\n",
      "99   This paragraph discusses case law and procedur...                 True   \n",
      "100  This paragraph discusses the lack of evidence ...                 True   \n",
      "101  The paragraph discusses the application of leg...                False   \n",
      "102  The paragraph discusses the procedural aspects...                 True   \n",
      "103  This paragraph analyzes and corrects an interp...                 True   \n",
      "\n",
      "                      application_of_law_phrases_llama  \\\n",
      "99   ['Longmore LJ rejected the submission that the...   \n",
      "100  ['It is common ground that a decision of third...   \n",
      "101                                                 []   \n",
      "102  [\"the conclusion, in paragraph 3, that there i...   \n",
      "103  ['Section s.8 8 (2)(d) is concerned with a par...   \n",
      "\n",
      "                                          reason_llama if_law_applied_claude  \\\n",
      "99   The judge applies the legal principles from th...                  True   \n",
      "100  The judge applies the legal principle regardin...                  True   \n",
      "101  This paragraph is primarily focused on providi...                  True   \n",
      "102  The judge applies principles related to the ri...                  True   \n",
      "103  The judge applies the provisions of the 2002 A...                  True   \n",
      "\n",
      "                     application_of_law_phrases_claude  \\\n",
      "99   ['Longmore LJ rejected the submission that the...   \n",
      "100  ['It is common ground that a decision of third...   \n",
      "101  [\"The letter stated that removal to Slovenia w...   \n",
      "102  [\"the conclusion, in paragraph 3, that there i...   \n",
      "103  ['Section s.8 8 (2)(d) is concerned with a par...   \n",
      "\n",
      "                                         reason_claude confidence  \\\n",
      "99   Model B's analysis is more accurate. This para...       High   \n",
      "100  Model B's analysis is more accurate. The judge...       High   \n",
      "101  Model A's analysis is more accurate. While thi...       High   \n",
      "102  Model B's analysis is more accurate. While the...       High   \n",
      "103  Model B's analysis is more accurate. While thi...       High   \n",
      "\n",
      "    agreement_with  final_annotation             case_name  \n",
      "99           Llama              True  ewhc_admin_2013_1818  \n",
      "100          Llama              True  ewhc_admin_2013_1818  \n",
      "101         OpenAI              True  ewhc_admin_2013_1818  \n",
      "102          Llama              True  ewhc_admin_2013_1818  \n",
      "103          Llama              True  ewhc_admin_2013_1818  \n",
      "Created ../data/final_test/case_csvs/cluster3/ewhc_admin_2013_1818.csv with 49 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ewhc_admin_2009_1404.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ewhc_admin_2018_1371.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ewhc_admin_2019_2233.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ewhc_admin_2015_1641.csv with 39 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ewca_civ_2011_1081.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ewca_civ_2014_826.csv with 33 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ewca_civ_2014_163.csv with 13 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ewca_civ_2009_856.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ewca_civ_2019_53.csv with 20 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ewca_civ_2005_385.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ewca_civ_2021_113.csv with 44 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ewca_crim_2022_1428.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ukait_2009_54.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ukait_2009_40.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ukait_2008_1.csv with 15 rows\n",
      "Created ../data/final_test/case_csvs/cluster3/ukait_2009_43.csv with 6 rows\n",
      "{'ewhc_admin_2013_1818': ['id/ukpga/1998/42', 'id/ukpga/2002/41', 'id/ukpga/2004/19'], 'ewhc_admin_2009_1404': ['id/ukpga/2002/41'], 'ewhc_admin_2018_1371': ['id/ukpga/1998/42', 'id/ukpga/2002/41'], 'ewhc_admin_2019_2233': ['id/ukpga/1998/42', 'id/ukpga/2002/41'], 'ewhc_admin_2015_1641': ['id/ukpga/1998/42', 'id/ukpga/1999/33', 'id/ukpga/2002/41', 'id/ukpga/2004/19'], 'ewca_civ_2011_1081': ['id/ukpga/2002/41', 'id/ukpga/2009/11'], 'ewca_civ_2014_826': ['id/ukpga/1989/41', 'id/ukpga/1998/42', 'id/ukpga/2002/41'], 'ewca_civ_2014_163': ['id/ukpga/1971/77', 'id/ukpga/2002/41'], 'ewca_civ_2009_856': ['id/ukpga/2002/41'], 'ewca_civ_2019_53': ['id/ukpga/2002/41'], 'ewca_civ_2005_385': ['id/ukpga/2002/41'], 'ewca_civ_2021_113': ['id/ukpga/1971/77', 'id/ukpga/1993/23', 'id/ukpga/1998/42', 'id/ukpga/2002/41', 'id/ukpga/2014/22'], 'ewca_crim_2022_1428': ['id/ukpga/1999/33', 'id/ukpga/2002/41', 'id/ukpga/2006/15', 'id/ukpga/2010/40'], 'ukait_2009_54': ['id/ukpga/2002/41'], 'ukait_2009_40': ['id/ukpga/1973/18', 'id/ukpga/2002/41'], 'ukait_2008_1': ['id/ukpga/2002/41'], 'ukait_2009_43': ['id/ukpga/1981/61', 'id/ukpga/2002/41']}\n",
      "Saved cluster 3 cases and acts to ../data/final_test/case_csvs/cluster3/cluster_3_cases.pkl\n",
      "                                             case_uri  \\\n",
      "12  https://caselaw.nationalarchives.gov.uk/ewhc/q...   \n",
      "13  https://caselaw.nationalarchives.gov.uk/ewhc/q...   \n",
      "14  https://caselaw.nationalarchives.gov.uk/ewhc/q...   \n",
      "15  https://caselaw.nationalarchives.gov.uk/ewhc/q...   \n",
      "16  https://caselaw.nationalarchives.gov.uk/ewhc/q...   \n",
      "\n",
      "                      para_id  \\\n",
      "12  ewhc_qb_2003_3555#para_18   \n",
      "13  ewhc_qb_2003_3555#para_19   \n",
      "14  ewhc_qb_2003_3555#para_20   \n",
      "15  ewhc_qb_2003_3555#para_30   \n",
      "16  ewhc_qb_2003_3555#para_36   \n",
      "\n",
      "                                           paragraphs references  \\\n",
      "12  18. In an effort to resolve the impasse the ow...         []   \n",
      "13  19. The owners have asserted that they are ent...         []   \n",
      "14  20. Vitol have entered an acknowledgement of s...         []   \n",
      "15  30. I would agree, as does Vitol, that owners’...         []   \n",
      "16  36. Mr Nigel Teare Q.C. for Euro Asian suggest...         []   \n",
      "\n",
      "   if_law_applied                         application_of_law_phrases  \\\n",
      "12           True  ['pursuant to CPR SC17 Rule 1(1a) and SC17 Rul...   \n",
      "13           True  ['entitled to serve this claim form on Vitol p...   \n",
      "14          False                                                 []   \n",
      "15          False                                                 []   \n",
      "16          False                                                 []   \n",
      "\n",
      "                                               reason if_law_applied_llama  \\\n",
      "12  The paragraph applies specific rules under the...                False   \n",
      "13  The judge applies the Civil Procedure Rules (C...                False   \n",
      "14  This paragraph discusses jurisdictional challe...                 True   \n",
      "15  The paragraph discusses the claims and dispute...                 True   \n",
      "16  This paragraph presents arguments made by coun...                 True   \n",
      "\n",
      "                     application_of_law_phrases_llama  \\\n",
      "12                                                 []   \n",
      "13                                                 []   \n",
      "14  ['by virtue of the charterparty jurisdiction c...   \n",
      "15  ['that deals with paragraph 2 of the details o...   \n",
      "16  ['whether or not Vitol is a company of a size ...   \n",
      "\n",
      "                                         reason_llama if_law_applied_claude  \\\n",
      "12  This paragraph merely sets out the owners' cla...                  True   \n",
      "13  This paragraph discusses a procedural issue an...                  True   \n",
      "14  The court applies jurisdiction principles to t...                  True   \n",
      "15  The judge applies the concept of a dispute 'ar...                  True   \n",
      "16  The judge applies the principle of requiring s...                  True   \n",
      "\n",
      "                    application_of_law_phrases_claude  \\\n",
      "12  ['pursuant to CPR SC17 Rule 1(1a) and SC17 Rul...   \n",
      "13  ['entitled to serve this claim form on Vitol p...   \n",
      "14  ['by virtue of the charterparty jurisdiction c...   \n",
      "15  [\"owners' claim that delivery up of the cargo ...   \n",
      "16  ['whether or not Vitol is a company of a size ...   \n",
      "\n",
      "                                        reason_claude confidence  \\\n",
      "12  Model A's analysis is more accurate. This para...       High   \n",
      "13  Model A's analysis is more accurate. This para...       High   \n",
      "14  Model B's analysis is more accurate. The parag...       High   \n",
      "15  Model B's analysis is more accurate. The judge...       High   \n",
      "16  Model B's analysis is more accurate. While the...       High   \n",
      "\n",
      "   agreement_with  final_annotation          case_name  \n",
      "12         OpenAI              True  ewhc_qb_2003_3555  \n",
      "13         OpenAI              True  ewhc_qb_2003_3555  \n",
      "14          Llama              True  ewhc_qb_2003_3555  \n",
      "15          Llama              True  ewhc_qb_2003_3555  \n",
      "16          Llama              True  ewhc_qb_2003_3555  \n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_qb_2003_3555.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_qb_2021_157.csv with 30 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_fam_2015_455.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_pat_2015_1094.csv with 23 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_ch_2024_11.csv with 40 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_ch_2018_2877.csv with 28 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_comm_2020_2736.csv with 13 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_qb_2009_1900.csv with 19 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_comm_2015_2748.csv with 30 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_admin_2020_2579.csv with 23 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_qb_2010_100.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_admin_2018_2651.csv with 34 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_admin_2018_1092.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_ch_2010_180.csv with 6 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_ch_2015_3910.csv with 5 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewca_civ_2011_10.csv with 8 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewca_civ_2020_620.csv with 22 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewca_civ_2015_515.csv with 124 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewca_civ_2007_1175.csv with 22 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewca_civ_2014_1105.csv with 22 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/uksc_2013_11.csv with 24 rows\n",
      "Created ../data/final_test/case_csvs/cluster4/ewhc_admin_2020_1850.csv with 22 rows\n",
      "{'ewhc_qb_2003_3555': ['id/ukpga/1981/54'], 'ewhc_qb_2021_157': ['id/ukpga/1981/49', 'id/ukpga/1981/54', 'id/ukpga/1998/42', 'id/ukpga/1999/23', 'id/ukpga/Geo5/23-24/12'], 'ewhc_fam_2015_455': ['id/ukpga/1973/18', 'id/ukpga/1981/54'], 'ewhc_pat_2015_1094': ['id/ukpga/1981/54'], 'ewhc_ch_2024_11': ['id/ukpga/1981/54'], 'ewhc_ch_2018_2877': ['id/ukpga/1980/58', 'id/ukpga/1981/54'], 'ewhc_comm_2020_2736': ['id/ukpga/1981/54'], 'ewhc_qb_2009_1900': ['id/ukpga/1981/54', 'id/ukpga/1990/36', 'id/ukpga/1995/42', 'id/ukpga/2000/8'], 'ewhc_comm_2015_2748': ['id/ukpga/1981/54', 'id/ukpga/1988/33'], 'ewhc_admin_2020_2579': ['id/ukpga/1981/54', 'id/ukpga/1990/8', 'id/ukpga/2000/37', 'id/ukpga/2008/29'], 'ewhc_qb_2010_100': ['id/ukpga/1981/54'], 'ewhc_admin_2018_2651': ['id/ukpga/1981/54', 'id/ukpga/1997/68', 'id/ukpga/2000/11', 'id/ukpga/2011/23', 'id/ukpga/2013/18', 'id/ukpga/2015/6'], 'ewhc_admin_2018_1092': ['id/ukpga/1981/54', 'id/ukpga/1991/65'], 'ewhc_ch_2010_180': ['id/ukpga/1981/54', 'id/ukpga/1985/6'], 'ewhc_ch_2015_3910': ['id/ukpga/1981/54'], 'ewca_civ_2011_10': ['id/ukpga/1981/54', 'id/ukpga/1998/20', 'id/ukpga/Vict/1-2/110'], 'ewca_civ_2020_620': ['id/ukpga/1974/39', 'id/ukpga/1977/42', 'id/ukpga/1981/54', 'id/ukpga/1997/12', 'id/ukpga/2005/4', 'id/ukpga/2020/7'], 'ewca_civ_2015_515': ['id/ukpga/1970/9', 'id/ukpga/1972/41', 'id/ukpga/1972/68', 'id/ukpga/1975/65', 'id/ukpga/1980/58', 'id/ukpga/1981/54', 'id/ukpga/1983/55', 'id/ukpga/1984/43', 'id/ukpga/1989/26', 'id/ukpga/1994/23', 'id/ukpga/1996/8', 'id/ukpga/2004/12', 'id/ukpga/2007/11'], 'ewca_civ_2007_1175': ['id/ukpga/1972/35', 'id/ukpga/1981/54'], 'ewca_civ_2014_1105': ['id/ukpga/1974/4', 'id/ukpga/1976/30', 'id/ukpga/1981/54', 'id/ukpga/1990/41', 'id/ukpga/2012/10', 'id/ukpga/2013/22', 'id/ukpga/Geo5/15-16/49', 'id/ukpga/Geo5/24-25/41'], 'uksc_2013_11': ['id/ukpga/1981/54', 'id/ukpga/1986/60', 'id/ukpga/1998/42', 'id/ukpga/2000/8', 'id/ukpga/2010/28', 'id/ukpga/Geo6/10-11/44', 'id/ukpga/Geo6/11-12/66'], 'ewhc_admin_2020_1850': ['id/ukpga/1980/43', 'id/ukpga/1981/54', 'id/ukpga/1985/23', 'id/ukpga/1998/42', 'id/ukpga/2012/10', 'id/ukpga/Geo6/11-12/58', 'id/ukpga/Vict/24-25/100']}\n",
      "Saved cluster 4 cases and acts to ../data/final_test/case_csvs/cluster4/cluster_4_cases.pkl\n",
      "                                              case_uri  \\\n",
      "186  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "187  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "188  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "189  https://caselaw.nationalarchives.gov.uk/ewhc/a...   \n",
      "237  https://caselaw.nationalarchives.gov.uk/ewhc/c...   \n",
      "\n",
      "                         para_id  \\\n",
      "186  ewhc_admin_2013_218#para_16   \n",
      "187  ewhc_admin_2013_218#para_27   \n",
      "188  ewhc_admin_2013_218#para_54   \n",
      "189  ewhc_admin_2013_218#para_62   \n",
      "237    ewhc_ch_2017_1727#para_26   \n",
      "\n",
      "                                            paragraphs references  \\\n",
      "186  16. The Claimant's minimum term (tariff) expir...         []   \n",
      "187  27. The panel then provided its decision. 8. C...         []   \n",
      "188  54. The final Manchester case is R (on the app...         []   \n",
      "189  62. It would have been open to the panel to id...         []   \n",
      "237  26. Those factors were approved by Lewison J (...         []   \n",
      "\n",
      "    if_law_applied                         application_of_law_phrases  \\\n",
      "186           True  ['the board was required by the referral to co...   \n",
      "187          False                                                 []   \n",
      "188           True  ['the panel was required to undertake', 'there...   \n",
      "189          False                                                 []   \n",
      "237           True  ['In that case the judge was sentencing the re...   \n",
      "\n",
      "                                                reason if_law_applied_llama  \\\n",
      "186  The paragraph applies legal provisions regardi...                False   \n",
      "187  This paragraph reports on the panel's conclusi...                 True   \n",
      "188  The paragraph discusses how legal principles r...                False   \n",
      "189  The paragraph discusses evidential considerati...                 True   \n",
      "237  The judge applies legal principles relating to...                False   \n",
      "\n",
      "                      application_of_law_phrases_llama  \\\n",
      "186                                                 []   \n",
      "187  ['it cannot direct your release', 'it remains ...   \n",
      "188                                                 []   \n",
      "189  ['It would have been open to the panel to iden...   \n",
      "237                                                 []   \n",
      "\n",
      "                                          reason_llama if_law_applied_claude  \\\n",
      "186  This paragraph primarily provides background i...                  True   \n",
      "187  The panel applies the legal criteria for parol...                  True   \n",
      "188  This paragraph discusses a previous case (R (o...                  True   \n",
      "189  The court applies the principles of administra...                  True   \n",
      "237  This paragraph discusses a previous case and t...                  True   \n",
      "\n",
      "                     application_of_law_phrases_claude  \\\n",
      "186  ['the board was required by the referral to co...   \n",
      "187  ['it cannot direct your release', 'It remains ...   \n",
      "188  ['there was no need for the panel to apply sep...   \n",
      "189  ['It would have been open to the panel to iden...   \n",
      "237  ['contempts of a similar nature to the charges...   \n",
      "\n",
      "                                         reason_claude confidence  \\\n",
      "186  Model A's analysis is more accurate. This para...       High   \n",
      "187  Model B's analysis is more accurate. This para...       High   \n",
      "188  While this paragraph does discuss a previous c...     Medium   \n",
      "189  Model B's analysis is more accurate. This para...       High   \n",
      "237  The judge applies legal sentencing principles ...       High   \n",
      "\n",
      "    agreement_with  final_annotation            case_name  \n",
      "186         OpenAI              True  ewhc_admin_2013_218  \n",
      "187          Llama              True  ewhc_admin_2013_218  \n",
      "188         OpenAI              True  ewhc_admin_2013_218  \n",
      "189          Llama              True  ewhc_admin_2013_218  \n",
      "237         OpenAI              True    ewhc_ch_2017_1727  \n",
      "Created ../data/final_test/case_csvs/cluster5/ewhc_admin_2013_218.csv with 33 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewhc_ch_2017_1727.csv with 13 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewhc_qb_2011_179.csv with 25 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewhc_comm_2023_1889.csv with 15 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewhc_admin_2012_882.csv with 17 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2009_651.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2007_3021.csv with 7 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2010_591.csv with 8 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2006_3335.csv with 13 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_civ_2008_1097.csv with 35 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2013_2499.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2006_2136.csv with 8 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2007_2548.csv with 9 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2006_3301.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2015_1933.csv with 28 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2007_36.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2008_2500.csv with 11 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2017_1461.csv with 10 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2018_2895.csv with 19 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2008_468.csv with 6 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2009_1942.csv with 30 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewhc_admin_2012_1098.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2005_3377.csv with 19 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2012_1939.csv with 14 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2009_379.csv with 7 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2022_1837.csv with 3 rows\n",
      "Created ../data/final_test/case_csvs/cluster5/ewca_crim_2017_1971.csv with 8 rows\n",
      "{'ewhc_admin_2013_218': ['id/ukpga/1991/53', 'id/ukpga/1997/43', 'id/ukpga/2003/44'], 'ewhc_ch_2017_1727': ['id/ukpga/1981/49', 'id/ukpga/2003/44'], 'ewhc_qb_2011_179': ['id/ukpga/1968/64', 'id/ukpga/1996/31', 'id/ukpga/2003/44', 'id/ukpga/Geo6and1Eliz2/15-16/66'], 'ewhc_comm_2023_1889': ['id/ukpga/2003/44'], 'ewhc_admin_2012_882': ['id/ukpga/1997/43', 'id/ukpga/2000/6', 'id/ukpga/2003/44'], 'ewca_crim_2009_651': ['id/ukpga/2003/44', 'id/ukpga/2008/4'], 'ewca_crim_2007_3021': ['id/ukpga/2000/6', 'id/ukpga/2003/44'], 'ewca_crim_2010_591': ['id/ukpga/2003/44'], 'ewca_crim_2006_3335': ['id/ukpga/1968/19', 'id/ukpga/1988/33', 'id/ukpga/2003/44', 'id/ukpga/Vict/24-25/100'], 'ewca_civ_2008_1097': ['id/ukpga/1991/53', 'id/ukpga/1998/37', 'id/ukpga/2000/6', 'id/ukpga/2003/44'], 'ewca_crim_2013_2499': ['id/ukpga/2003/44', 'id/ukpga/2007/27'], 'ewca_crim_2006_2136': ['id/ukpga/1968/27', 'id/ukpga/2003/44'], 'ewca_crim_2007_2548': ['id/ukpga/1991/65', 'id/ukpga/2003/44'], 'ewca_crim_2006_3301': ['id/ukpga/1984/60', 'id/ukpga/2003/44'], 'ewca_crim_2015_1933': ['id/ukpga/1968/19', 'id/ukpga/1970/31', 'id/ukpga/1980/43', 'id/ukpga/1982/48', 'id/ukpga/1985/23', 'id/ukpga/2000/6', 'id/ukpga/2003/39', 'id/ukpga/2003/44', 'id/ukpga/2010/40', 'id/ukpga/2015/2'], 'ewca_crim_2007_36': ['id/ukpga/1984/60', 'id/ukpga/2003/44'], 'ewca_crim_2008_2500': ['id/ukpga/2003/44', 'id/ukpga/Eliz2/5-6/11'], 'ewca_crim_2017_1461': ['id/ukpga/2003/44'], 'ewca_crim_2018_2895': ['id/ukpga/1988/33', 'id/ukpga/2003/44', 'id/ukpga/2009/25'], 'ewca_crim_2008_468': ['id/ukpga/1983/20', 'id/ukpga/2003/44'], 'ewca_crim_2009_1942': ['id/ukpga/1972/68', 'id/ukpga/1974/37', 'id/ukpga/2003/44'], 'ewhc_admin_2012_1098': ['id/ukpga/1999/23', 'id/ukpga/2003/41', 'id/ukpga/2003/44'], 'ewca_crim_2005_3377': ['id/ukpga/2003/44'], 'ewca_crim_2012_1939': ['id/ukpga/2000/6', 'id/ukpga/2003/44'], 'ewca_crim_2009_379': ['id/ukpga/2003/44'], 'ewca_crim_2022_1837': ['id/ukpga/1968/27', 'id/ukpga/1968/60', 'id/ukpga/2003/44'], 'ewca_crim_2017_1971': ['id/ukpga/2003/44']}\n",
      "Saved cluster 5 cases and acts to ../data/final_test/case_csvs/cluster5/cluster_5_cases.pkl\n",
      "                                              case_uri  \\\n",
      "980  https://caselaw.nationalarchives.gov.uk/ewca/c...   \n",
      "981  https://caselaw.nationalarchives.gov.uk/ewca/c...   \n",
      "982  https://caselaw.nationalarchives.gov.uk/ewca/c...   \n",
      "983  https://caselaw.nationalarchives.gov.uk/ewca/c...   \n",
      "984  https://caselaw.nationalarchives.gov.uk/ewca/c...   \n",
      "\n",
      "                        para_id  \\\n",
      "980  ewca_crim_2022_483#para_16   \n",
      "981  ewca_crim_2022_483#para_18   \n",
      "982  ewca_crim_2022_483#para_23   \n",
      "983  ewca_crim_2022_483#para_25   \n",
      "984  ewca_crim_2022_483#para_32   \n",
      "\n",
      "                                            paragraphs  \\\n",
      "980  16. The sentencing judge had had conduct of th...   \n",
      "981  18. The judge said that this was an exceptiona...   \n",
      "982  23. The first point that emerges from this mor...   \n",
      "983  25. The context of those decisions was paragra...   \n",
      "984  32. We must address the issue of exploitation....   \n",
      "\n",
      "                                            references if_law_applied  \\\n",
      "980                                                 []           True   \n",
      "981                                                 []           True   \n",
      "982  [{'text': 'section 5', 'href': 'http://www.leg...           True   \n",
      "983  [{'text': 'SOA 2003', 'href': 'http://www.legi...           True   \n",
      "984                                                 []           True   \n",
      "\n",
      "                            application_of_law_phrases  \\\n",
      "980  ['the basis upon which she was to sentence in ...   \n",
      "981  ['this was an exceptional case', 'factors whic...   \n",
      "982  ['the minimum starting point would be 4 years'...   \n",
      "983  ['We must consider whether the guideline as it...   \n",
      "984  ['exploitation involves the using of the victi...   \n",
      "\n",
      "                                                reason if_law_applied_llama  \\\n",
      "980  The judge applies legal principles related to ...                False   \n",
      "981  The judge applies legal principles regarding s...                  NaN   \n",
      "982  The paragraph applies sentencing guidelines an...                  NaN   \n",
      "983  The judge is analyzing how the specific provis...                False   \n",
      "984  The court applies the legal definition of expl...                  NaN   \n",
      "\n",
      "    application_of_law_phrases_llama  \\\n",
      "980                               []   \n",
      "981                              NaN   \n",
      "982                              NaN   \n",
      "983                               []   \n",
      "984                              NaN   \n",
      "\n",
      "                                          reason_llama if_law_applied_claude  \\\n",
      "980  This paragraph provides background information...                  True   \n",
      "981                                                NaN                  True   \n",
      "982                                                NaN                  True   \n",
      "983  This paragraph cites and discusses sentencing ...                  True   \n",
      "984                                                NaN                  True   \n",
      "\n",
      "                     application_of_law_phrases_claude  \\\n",
      "980  [\"the basis upon which she was to sentence in ...   \n",
      "981  ['this was an exceptional case', 'factors whic...   \n",
      "982  ['the minimum starting point would be 4 years'...   \n",
      "983  ['We must consider whether the guideline as it...   \n",
      "984  [\"exploitation involves the using of the victi...   \n",
      "\n",
      "                                         reason_claude confidence  \\\n",
      "980  Model A's analysis is more accurate. While thi...       High   \n",
      "981  This paragraph clearly demonstrates applicatio...       High   \n",
      "982  This paragraph demonstrates clear application ...       High   \n",
      "983  Model A's analysis is more accurate. While the...       High   \n",
      "984  This paragraph clearly demonstrates an applica...       High   \n",
      "\n",
      "    agreement_with  final_annotation           case_name  \n",
      "980         OpenAI              True  ewca_crim_2022_483  \n",
      "981         OpenAI              True  ewca_crim_2022_483  \n",
      "982         OpenAI              True  ewca_crim_2022_483  \n",
      "983         OpenAI              True  ewca_crim_2022_483  \n",
      "984         OpenAI              True  ewca_crim_2022_483  \n",
      "Created ../data/final_test/case_csvs/cluster6/ewca_crim_2022_483.csv with 24 rows\n",
      "Created ../data/final_test/case_csvs/cluster6/ewca_crim_2008_894.csv with 13 rows\n",
      "Created ../data/final_test/case_csvs/cluster6/ewca_crim_2018_1393.csv with 23 rows\n",
      "Created ../data/final_test/case_csvs/cluster6/ewca_crim_2019_466.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster6/ewca_crim_2023_1106.csv with 9 rows\n",
      "Created ../data/final_test/case_csvs/cluster6/ewca_crim_2019_2056.csv with 9 rows\n",
      "{'ewca_crim_2022_483': ['id/ukpga/1988/33', 'id/ukpga/2003/42', 'id/ukpga/2020/17'], 'ewca_crim_2008_894': ['id/ukpga/2003/42'], 'ewca_crim_2018_1393': ['id/ukpga/1992/34', 'id/ukpga/2003/42', 'id/ukpga/2003/44'], 'ewca_crim_2019_466': ['id/ukpga/1978/37', 'id/ukpga/1988/33', 'id/ukpga/1992/34', 'id/ukpga/2002/29', 'id/ukpga/2003/42', 'id/ukpga/2006/47', 'id/ukpga/2009/25'], 'ewca_crim_2023_1106': ['id/ukpga/1992/34', 'id/ukpga/2003/42'], 'ewca_crim_2019_2056': ['id/ukpga/1992/34', 'id/ukpga/2003/42']}\n",
      "Saved cluster 6 cases and acts to ../data/final_test/case_csvs/cluster6/cluster_6_cases.pkl\n",
      "                                              case_uri  \\\n",
      "169  https://caselaw.nationalarchives.gov.uk/ewhc/c...   \n",
      "403  https://caselaw.nationalarchives.gov.uk/ewhc/c...   \n",
      "404  https://caselaw.nationalarchives.gov.uk/ewhc/c...   \n",
      "405  https://caselaw.nationalarchives.gov.uk/ewhc/c...   \n",
      "406  https://caselaw.nationalarchives.gov.uk/ewhc/c...   \n",
      "\n",
      "                       para_id  \\\n",
      "169  ewhc_ch_2005_3438#para_19   \n",
      "403   ewhc_ch_2019_704#para_10   \n",
      "404   ewhc_ch_2019_704#para_15   \n",
      "405   ewhc_ch_2019_704#para_16   \n",
      "406   ewhc_ch_2019_704#para_53   \n",
      "\n",
      "                                            paragraphs  \\\n",
      "169  19. Thus, the position in the present case is ...   \n",
      "403  10. A procedural question which arises at the ...   \n",
      "404  15. A mistake in the construction of the lease...   \n",
      "405  16. Accordingly, there is no reason not to app...   \n",
      "406  53. Ground 4 is an allegation of irregularity ...   \n",
      "\n",
      "                                            references if_law_applied  \\\n",
      "169                                                 []          False   \n",
      "403  [{'text': 'section 69', 'href': 'http://www.le...           True   \n",
      "404  [{'text': 'section 69', 'href': 'http://www.le...          False   \n",
      "405  [{'text': 'section 69', 'href': 'http://www.le...          False   \n",
      "406  [{'text': 'section 69', 'href': 'http://www.le...          False   \n",
      "\n",
      "                            application_of_law_phrases  \\\n",
      "169                                                 []   \n",
      "403  ['section 69(5) provides a default position th...   \n",
      "404                                                 []   \n",
      "405                                                 []   \n",
      "406                                                 []   \n",
      "\n",
      "                                                reason if_law_applied_llama  \\\n",
      "169  The paragraph discusses the implications of an...                 True   \n",
      "403  The judge applies section 69(5) of the relevan...                False   \n",
      "404  The paragraph discusses procedural issues and ...                 True   \n",
      "405  The paragraph references established rules and...                 True   \n",
      "406  The paragraph discusses an allegation regardin...                 True   \n",
      "\n",
      "                      application_of_law_phrases_llama  \\\n",
      "169  ['The effect of the construction abducted by t...   \n",
      "403                                                 []   \n",
      "404  ['A mistake in the construction of the lease i...   \n",
      "405  ['as accepted by Colman J in Bulfract and emph...   \n",
      "406  ['for the purposes of the appeal under section...   \n",
      "\n",
      "                                          reason_llama if_law_applied_claude  \\\n",
      "169  The judge applies contractual interpretation p...                  True   \n",
      "403  This paragraph discusses procedural options an...                  True   \n",
      "404  The judge applies legal principles to distingu...                  True   \n",
      "405  The judge applies the normal rule regarding le...                  True   \n",
      "406  The judge applies the statutory provisions of ...                  True   \n",
      "\n",
      "                     application_of_law_phrases_claude  \\\n",
      "169  ['The effect of the construction abducted by t...   \n",
      "403  ['A procedural question which arises at the ou...   \n",
      "404  ['A mistake in the construction of the lease i...   \n",
      "405  ['as accepted by Colman J in Bulfract and emph...   \n",
      "406  ['for the purposes of the appeal under section...   \n",
      "\n",
      "                                         reason_claude confidence  \\\n",
      "169  Model B's analysis is more accurate. This para...       High   \n",
      "403  Model A's analysis is more accurate. The judge...       High   \n",
      "404  Model B's analysis is more accurate. While Mod...       High   \n",
      "405  Model B's analysis is more accurate. This para...       High   \n",
      "406  Model B's analysis is more accurate. The judge...       High   \n",
      "\n",
      "    agreement_with  final_annotation          case_name  \n",
      "169          Llama              True  ewhc_ch_2005_3438  \n",
      "403         OpenAI              True   ewhc_ch_2019_704  \n",
      "404          Llama              True   ewhc_ch_2019_704  \n",
      "405          Llama              True   ewhc_ch_2019_704  \n",
      "406          Llama              True   ewhc_ch_2019_704  \n",
      "Created ../data/final_test/case_csvs/cluster7/ewhc_ch_2005_3438.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster7/ewhc_ch_2019_704.csv with 19 rows\n",
      "Created ../data/final_test/case_csvs/cluster7/ewhc_comm_2020_2012.csv with 37 rows\n",
      "Created ../data/final_test/case_csvs/cluster7/ewhc_comm_2005_2115.csv with 43 rows\n",
      "Created ../data/final_test/case_csvs/cluster7/ewhc_tcc_2019_2212.csv with 29 rows\n",
      "Created ../data/final_test/case_csvs/cluster7/ewhc_comm_2018_330.csv with 15 rows\n",
      "Created ../data/final_test/case_csvs/cluster7/ewhc_comm_2023_391.csv with 27 rows\n",
      "Created ../data/final_test/case_csvs/cluster7/ewhc_comm_2006_134.csv with 13 rows\n",
      "Created ../data/final_test/case_csvs/cluster7/ewhc_comm_2021_286.csv with 16 rows\n",
      "Created ../data/final_test/case_csvs/cluster7/ewhc_comm_2004_1752.csv with 9 rows\n",
      "Created ../data/final_test/case_csvs/cluster7/ewhc_comm_2017_1430.csv with 12 rows\n",
      "Created ../data/final_test/case_csvs/cluster7/ewhc_comm_2019_3292.csv with 18 rows\n",
      "Created ../data/final_test/case_csvs/cluster7/ewhc_comm_2023_910.csv with 32 rows\n",
      "Created ../data/final_test/case_csvs/cluster7/ewhc_comm_2017_3430.csv with 15 rows\n",
      "{'ewhc_ch_2005_3438': ['id/ukpga/1996/23', 'id/ukpga/Eliz2/2-3/56'], 'ewhc_ch_2019_704': ['id/ukpga/1979/42', 'id/ukpga/1996/23'], 'ewhc_comm_2020_2012': ['id/ukpga/1996/23'], 'ewhc_comm_2005_2115': ['id/ukpga/1982/27', 'id/ukpga/1996/23'], 'ewhc_tcc_2019_2212': ['id/ukpga/1996/23'], 'ewhc_comm_2018_330': ['id/ukpga/1996/23'], 'ewhc_comm_2023_391': ['id/ukpga/1996/23'], 'ewhc_comm_2006_134': ['id/ukpga/1996/23'], 'ewhc_comm_2021_286': ['id/ukpga/1996/23'], 'ewhc_comm_2004_1752': ['id/ukpga/1996/23'], 'ewhc_comm_2017_1430': ['id/ukpga/1972/68', 'id/ukpga/1996/23'], 'ewhc_comm_2019_3292': ['id/ukpga/1996/23'], 'ewhc_comm_2023_910': ['id/ukpga/1996/23'], 'ewhc_comm_2017_3430': ['id/ukpga/1996/23']}\n",
      "Saved cluster 7 cases and acts to ../data/final_test/case_csvs/cluster7/cluster_7_cases.pkl\n",
      "                                               case_uri  \\\n",
      "973   https://caselaw.nationalarchives.gov.uk/ewca/c...   \n",
      "1006  https://caselaw.nationalarchives.gov.uk/ewca/c...   \n",
      "1007  https://caselaw.nationalarchives.gov.uk/ewca/c...   \n",
      "1008  https://caselaw.nationalarchives.gov.uk/ewca/c...   \n",
      "1009  https://caselaw.nationalarchives.gov.uk/ewca/c...   \n",
      "\n",
      "                          para_id  \\\n",
      "973    ewca_crim_2005_1681#para_8   \n",
      "1006  ewca_crim_2005_1881#para_15   \n",
      "1007  ewca_crim_2005_1881#para_35   \n",
      "1008  ewca_crim_2005_1881#para_41   \n",
      "1009  ewca_crim_2005_1881#para_45   \n",
      "\n",
      "                                             paragraphs references  \\\n",
      "973   8. Dr Lowenstein found the appellant to be an ...         []   \n",
      "1006  15. The defence had two psychiatric reports. N...         []   \n",
      "1007  35. Dr Mendelson’s first report is dated 15 Ap...         []   \n",
      "1008  41. When he gave evidence Dr Joseph explained ...         []   \n",
      "1009  45. Significantly, Dr Joseph said that there i...         []   \n",
      "\n",
      "     if_law_applied                         application_of_law_phrases  \\\n",
      "973            True  ['While this does not excuse his offences, it ...   \n",
      "1006           True  ['Neither suggested an abnormality of mind so ...   \n",
      "1007           True  ['an abnormality of mind of a nature and degre...   \n",
      "1008           True  ['a lifelong history of anxiety did not amount...   \n",
      "1009           True  ['that her mental responsibility for it was su...   \n",
      "\n",
      "                                                 reason if_law_applied_llama  \\\n",
      "973   The judge applies psychological evaluations an...                False   \n",
      "1006  The paragraph discusses how the psychiatric re...                False   \n",
      "1007  The paragraph applies legal principles regardi...                False   \n",
      "1008  The judge applies legal standards for evaluati...                False   \n",
      "1009  The paragraph discusses the application of men...                False   \n",
      "\n",
      "     application_of_law_phrases_llama  \\\n",
      "973                                []   \n",
      "1006                               []   \n",
      "1007                               []   \n",
      "1008                               []   \n",
      "1009                               []   \n",
      "\n",
      "                                           reason_llama if_law_applied_claude  \\\n",
      "973   This paragraph presents expert psychological a...                  True   \n",
      "1006  This paragraph discusses the existence of psyc...                  True   \n",
      "1007  This paragraph presents an expert's opinion on...                  True   \n",
      "1008  This paragraph discusses medical expert testim...                  True   \n",
      "1009  This paragraph is discussing expert opinion an...                  True   \n",
      "\n",
      "                      application_of_law_phrases_claude  \\\n",
      "973   ['While this does not excuse his offences, it ...   \n",
      "1006  [\"Neither suggested an abnormality of mind so ...   \n",
      "1007  ['an abnormality of mind of a nature and degre...   \n",
      "1008  ['a lifelong history of anxiety did not amount...   \n",
      "1009  ['that her mental responsibility for it was su...   \n",
      "\n",
      "                                          reason_claude confidence  \\\n",
      "973   Model A's analysis is more accurate. While the...       High   \n",
      "1006  Model A is correct. The paragraph does contain...       High   \n",
      "1007  Model A's analysis is more accurate. The parag...       High   \n",
      "1008  Model A's analysis is more accurate. While the...       High   \n",
      "1009  Model A is correct in identifying this as an a...       High   \n",
      "\n",
      "     agreement_with  final_annotation            case_name  \n",
      "973          OpenAI              True  ewca_crim_2005_1681  \n",
      "1006         OpenAI              True  ewca_crim_2005_1881  \n",
      "1007         OpenAI              True  ewca_crim_2005_1881  \n",
      "1008         OpenAI              True  ewca_crim_2005_1881  \n",
      "1009         OpenAI              True  ewca_crim_2005_1881  \n",
      "Created ../data/final_test/case_csvs/cluster8/ewca_crim_2005_1681.csv with 7 rows\n",
      "Created ../data/final_test/case_csvs/cluster8/ewca_crim_2005_1881.csv with 30 rows\n",
      "Created ../data/final_test/case_csvs/cluster8/ewca_crim_2020_1455.csv with 33 rows\n",
      "Created ../data/final_test/case_csvs/cluster8/ewca_civ_2018_1841.csv with 34 rows\n",
      "Created ../data/final_test/case_csvs/cluster8/ewca_crim_2010_1486.csv with 42 rows\n",
      "Created ../data/final_test/case_csvs/cluster8/ewca_crim_2022_50.csv with 8 rows\n",
      "Created ../data/final_test/case_csvs/cluster8/ewca_crim_2006_646.csv with 1 rows\n",
      "Created ../data/final_test/case_csvs/cluster8/ewca_crim_2003_190.csv with 15 rows\n",
      "Created ../data/final_test/case_csvs/cluster8/ukpc_2011_16.csv with 11 rows\n",
      "{'ewca_crim_2005_1681': ['id/ukpga/1968/19', 'id/ukpga/1984/60'], 'ewca_crim_2005_1881': ['id/ukpga/1968/19', 'id/ukpga/Eliz2/5-6/11'], 'ewca_crim_2020_1455': ['id/ukpga/1968/19', 'id/ukpga/2002/29'], 'ewca_civ_2018_1841': ['id/ukpga/1969/58', 'id/ukpga/1976/74', 'id/ukpga/1982/34', 'id/ukpga/1982/39', 'id/ukpga/1983/19', 'id/ukpga/1983/20', 'id/ukpga/1991/25', 'id/ukpga/1993/36', 'id/ukpga/1997/43', 'id/ukpga/1998/42', 'id/ukpga/2003/44', 'id/ukpga/2008/4', 'id/ukpga/2009/25', 'id/ukpga/Eliz2/5-6/11'], 'ewca_crim_2010_1486': ['id/ukpga/1968/19', 'id/ukpga/1971/23', 'id/ukpga/1972/68', 'id/ukpga/1984/39', 'id/ukpga/1998/42', 'id/ukpga/2010/1'], 'ewca_crim_2022_50': ['id/ukpga/1968/19', 'id/ukpga/1981/45'], 'ewca_crim_2006_646': ['id/ukpga/1968/19', 'id/ukpga/1995/35'], 'ewca_crim_2003_190': ['id/ukpga/1968/19', 'id/ukpga/1997/51', 'id/ukpga/1999/23'], 'ukpc_2011_16': ['id/ukpga/1968/19', 'id/ukpga/1977/45']}\n",
      "Saved cluster 8 cases and acts to ../data/final_test/case_csvs/cluster8/cluster_8_cases.pkl\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    create_cluster_files(i, df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " all_results = []\n",
    "    \n",
    "    # Step 2: Process each GROUP (not each row!)\n",
    "    for group_idx, group_data in enumerate(groups):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"📊 Processing Group {group_idx + 1}/{n_groups}\")\n",
    "        print(f\"   Legislation acts: {len(group_data['legislation_acts'])}\")\n",
    "        print(f\"   Cases in group: {len(group_data['cases'])}\")\n",
    "        \n",
    "        # Build vector store ONCE for this group's legislation\n",
    "        vectorstore = openAIHandler.BuildVectorDB(legislation_dir, group_data['legislation_acts'])\n",
    "        \n",
    "        # Get all rows for cases in this group\n",
    "        group_cases = set(group_data['cases'])\n",
    "        group_df = df_final[df_final['case_uri'].apply(convert_case_uri_to_key).isin(group_cases)].copy()\n",
    "        \n",
    "        print(f\"   Processing {len(group_df)} rows with SAME vector store\")\n",
    "        \n",
    "        # Process all rows in this group with the SAME vector store\n",
    "        for idx, row in group_df.iterrows():\n",
    "            try:\n",
    "                # Your section matching logic here using the SAME vectorstore\n",
    "                relevant_docs = get_relevant_sections_with_vectorstore(\n",
    "                    row['paragraphs'], \n",
    "                    group_data['legislation_acts'], \n",
    "                    row.get('references', []), \n",
    "                    vectorstore\n",
    "                )\n",
    "                \n",
    "                # Add sections to row\n",
    "                if relevant_docs:\n",
    "                    for doc in relevant_docs:\n",
    "                        new_row = row.copy()\n",
    "                        new_row['section_id'] = doc.metadata.get(\"id\", \"unknown\")\n",
    "                        new_row['section_text'] = str(doc.page_content)\n",
    "                        all_results.append(new_row)\n",
    "                else:\n",
    "                    all_results.append(row)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                all_results.append(row)\n",
    "        \n",
    "        # Clean up vector store\n",
    "        del vectorstore\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"✓ Group {group_idx + 1} completed\")\n",
    "    \n",
    "    # Save final results\n",
    "    final_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Clean text\n",
    "    for col in ['paragraphs', 'section_text']:\n",
    "        if col in final_df.columns:\n",
    "            final_df[col] = final_df[col].astype(str).str.replace('\\n', ' ', regex=False)\n",
    "            final_df[col] = final_df[col].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    \n",
    "    final_df.to_csv(output_csv_path, index=False, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    print(f\"🎉 DONE! Only {n_groups} vector stores built instead of thousands!\")\n",
    "    return output_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the csv with the positive values\n",
    "#or get a column name as input and filter the values\n",
    "#gropu the df in to sub groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Loading case-legislation mapping...\n",
      "🚀 Starting bulk processing...\n",
      "📁 Input: ../data/final_test/positve_cases.csv\n",
      "💾 Output: ../data/final_test/processed_with_sections_all.csv\n",
      "📚 Loading CSV data...\n",
      "🎯 Filtered to 9223 rows with law applied (from 9223 total)\n",
      "📄 Resuming from row 297\n",
      "🔍 Analyzing legislation patterns...\n",
      "⚠️  Warning: 181 case keys not found in legislation dict\n",
      "   Sample missing keys: ['ewhc_comm_2012_50', 'ewca_civ_2005_856', 'ewca_civ_2005_856']\n",
      "   Filtered from 8926 to 8745 rows\n",
      "📊 Pattern Analysis:\n",
      "   - Total rows with legislation: 8745\n",
      "   - Unique legislation combinations: 325\n",
      "   - Largest group: 271 rows\n",
      "   - This means 325 vector stores max!\n",
      "💾 Saving before expansion: 8745 rows\n",
      "✓ Saved pre-expansion file: ../data/final_test/processed_with_sections_all_before_expansion.csv\n",
      "📝 Grouping rows by legislation...\n",
      "📊 Processing 8745 rows with valid legislation (filtered from 8745)\n",
      "🎯 Processing order optimized:\n",
      "   1. 271 rows → 1 acts\n",
      "   2. 211 rows → 1 acts\n",
      "   3. 179 rows → 2 acts\n",
      "   4. 124 rows → 13 acts\n",
      "   5. 120 rows → 1 acts\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 1/325\n",
      "\n",
      "🔄 Processing 271 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1990/8']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 604\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 604\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 604\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 604\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 604\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 604\n",
      "Processing batch 600 to 604 (4 documents)\n",
      "Successfully embedded batch 600 to 604 of 604\n",
      "Creating FAISS index with 604 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 271 → 284 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 2/325\n",
      "\n",
      "🔄 Processing 211 rows with 1 acts\n",
      "📁 Loading cached vector store for 1 acts...\n",
      "✗ Failed to load cached vector store: Ran out of input\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1996/23']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 115\n",
      "Processing batch 100 to 115 (15 documents)\n",
      "Successfully embedded batch 100 to 115 of 115\n",
      "Creating FAISS index with 115 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 211 → 260 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 3/325\n",
      "\n",
      "🔄 Processing 179 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1985/6', 'id/ukpga/2006/46']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 2752\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 2752\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 2752\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 2752\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 2752\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 2752\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 2752\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 2752\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 2752\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 2752\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 2752\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 2752\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 2752\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 2752\n",
      "Processing batch 1400 to 1500 (100 documents)\n",
      "Successfully embedded batch 1400 to 1500 of 2752\n",
      "Processing batch 1500 to 1600 (100 documents)\n",
      "Successfully embedded batch 1500 to 1600 of 2752\n",
      "Processing batch 1600 to 1700 (100 documents)\n",
      "Successfully embedded batch 1600 to 1700 of 2752\n",
      "Processing batch 1700 to 1800 (100 documents)\n",
      "Successfully embedded batch 1700 to 1800 of 2752\n",
      "Processing batch 1800 to 1900 (100 documents)\n",
      "Successfully embedded batch 1800 to 1900 of 2752\n",
      "Processing batch 1900 to 2000 (100 documents)\n",
      "Successfully embedded batch 1900 to 2000 of 2752\n",
      "Processing batch 2000 to 2100 (100 documents)\n",
      "Successfully embedded batch 2000 to 2100 of 2752\n",
      "Processing batch 2100 to 2200 (100 documents)\n",
      "Successfully embedded batch 2100 to 2200 of 2752\n",
      "Processing batch 2200 to 2300 (100 documents)\n",
      "Successfully embedded batch 2200 to 2300 of 2752\n",
      "Processing batch 2300 to 2400 (100 documents)\n",
      "Successfully embedded batch 2300 to 2400 of 2752\n",
      "Processing batch 2400 to 2500 (100 documents)\n",
      "Successfully embedded batch 2400 to 2500 of 2752\n",
      "Processing batch 2500 to 2600 (100 documents)\n",
      "Successfully embedded batch 2500 to 2600 of 2752\n",
      "Processing batch 2600 to 2700 (100 documents)\n",
      "Successfully embedded batch 2600 to 2700 of 2752\n",
      "Processing batch 2700 to 2752 (52 documents)\n",
      "Successfully embedded batch 2700 to 2752 of 2752\n",
      "Creating FAISS index with 2752 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 179 → 364 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 4/325\n",
      "\n",
      "🔄 Processing 124 rows with 13 acts\n",
      "🔨 Building new vector store for 13 acts...\n",
      "   Acts: ['id/ukpga/1970/9', 'id/ukpga/1972/41', 'id/ukpga/1972/68', 'id/ukpga/1975/65', 'id/ukpga/1980/58', 'id/ukpga/1981/54', 'id/ukpga/1983/55', 'id/ukpga/1984/43', 'id/ukpga/1989/26', 'id/ukpga/1994/23', 'id/ukpga/1996/8', 'id/ukpga/2004/12', 'id/ukpga/2007/11']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 2566\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 2566\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 2566\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 2566\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 2566\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 2566\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 2566\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 2566\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 2566\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 2566\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 2566\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 2566\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 2566\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 2566\n",
      "Processing batch 1400 to 1500 (100 documents)\n",
      "Successfully embedded batch 1400 to 1500 of 2566\n",
      "Processing batch 1500 to 1600 (100 documents)\n",
      "Successfully embedded batch 1500 to 1600 of 2566\n",
      "Processing batch 1600 to 1700 (100 documents)\n",
      "Successfully embedded batch 1600 to 1700 of 2566\n",
      "Processing batch 1700 to 1800 (100 documents)\n",
      "Successfully embedded batch 1700 to 1800 of 2566\n",
      "Processing batch 1800 to 1900 (100 documents)\n",
      "Successfully embedded batch 1800 to 1900 of 2566\n",
      "Processing batch 1900 to 2000 (100 documents)\n",
      "Successfully embedded batch 1900 to 2000 of 2566\n",
      "Processing batch 2000 to 2100 (100 documents)\n",
      "Successfully embedded batch 2000 to 2100 of 2566\n",
      "Processing batch 2100 to 2200 (100 documents)\n",
      "Successfully embedded batch 2100 to 2200 of 2566\n",
      "Processing batch 2200 to 2300 (100 documents)\n",
      "Successfully embedded batch 2200 to 2300 of 2566\n",
      "Processing batch 2300 to 2400 (100 documents)\n",
      "Successfully embedded batch 2300 to 2400 of 2566\n",
      "Processing batch 2400 to 2500 (100 documents)\n",
      "Successfully embedded batch 2400 to 2500 of 2566\n",
      "Processing batch 2500 to 2566 (66 documents)\n",
      "Successfully embedded batch 2500 to 2566 of 2566\n",
      "Creating FAISS index with 2566 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 124 → 372 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 5/325\n",
      "\n",
      "🔄 Processing 120 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1998/42']\n",
      "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\n",
      "Processing batch 0 to 49 (49 documents)\n",
      "Successfully embedded batch 0 to 49 of 49\n",
      "Creating FAISS index with 49 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 120 → 123 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 6/325\n",
      "\n",
      "🔄 Processing 99 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1990/8', 'id/ukpga/1994/33', 'id/ukpga/Geo5and1Edw8/26/49']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1042\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1042\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1042\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1042\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1042\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1042\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1042\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1042\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1042\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1042\n",
      "Processing batch 1000 to 1042 (42 documents)\n",
      "Successfully embedded batch 1000 to 1042 of 1042\n",
      "Creating FAISS index with 1042 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 99 → 210 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 7/325\n",
      "\n",
      "🔄 Processing 91 rows with 6 acts\n",
      "🔨 Building new vector store for 6 acts...\n",
      "   Acts: ['id/ukpga/1982/53', 'id/ukpga/1985/61', 'id/ukpga/1995/38', 'id/ukpga/Geo5/15-16/19', 'id/ukpga/Vict/59-60/35', 'id/ukpga/Will4and1Vict/7/26']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 345\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 345\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 345\n",
      "Processing batch 300 to 345 (45 documents)\n",
      "Successfully embedded batch 300 to 345 of 345\n",
      "Creating FAISS index with 345 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 91 → 269 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 8/325\n",
      "\n",
      "🔄 Processing 89 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1973/18', 'id/ukpga/1986/45', 'id/ukpga/2006/46']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 2693\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 2693\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 2693\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 2693\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 2693\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 2693\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 2693\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 2693\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 2693\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 2693\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 2693\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 2693\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 2693\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 2693\n",
      "Processing batch 1400 to 1500 (100 documents)\n",
      "Successfully embedded batch 1400 to 1500 of 2693\n",
      "Processing batch 1500 to 1600 (100 documents)\n",
      "Successfully embedded batch 1500 to 1600 of 2693\n",
      "Processing batch 1600 to 1700 (100 documents)\n",
      "Successfully embedded batch 1600 to 1700 of 2693\n",
      "Processing batch 1700 to 1800 (100 documents)\n",
      "Successfully embedded batch 1700 to 1800 of 2693\n",
      "Processing batch 1800 to 1900 (100 documents)\n",
      "Successfully embedded batch 1800 to 1900 of 2693\n",
      "Processing batch 1900 to 2000 (100 documents)\n",
      "Successfully embedded batch 1900 to 2000 of 2693\n",
      "Processing batch 2000 to 2100 (100 documents)\n",
      "Successfully embedded batch 2000 to 2100 of 2693\n",
      "Processing batch 2100 to 2200 (100 documents)\n",
      "Successfully embedded batch 2100 to 2200 of 2693\n",
      "Processing batch 2200 to 2300 (100 documents)\n",
      "Successfully embedded batch 2200 to 2300 of 2693\n",
      "Processing batch 2300 to 2400 (100 documents)\n",
      "Successfully embedded batch 2300 to 2400 of 2693\n",
      "Processing batch 2400 to 2500 (100 documents)\n",
      "Successfully embedded batch 2400 to 2500 of 2693\n",
      "Processing batch 2500 to 2600 (100 documents)\n",
      "Successfully embedded batch 2500 to 2600 of 2693\n",
      "Processing batch 2600 to 2693 (93 documents)\n",
      "Successfully embedded batch 2600 to 2693 of 2693\n",
      "Creating FAISS index with 2693 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 89 → 232 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 9/325\n",
      "\n",
      "🔄 Processing 89 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1981/54']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 188\n",
      "Processing batch 100 to 188 (88 documents)\n",
      "Successfully embedded batch 100 to 188 of 188\n",
      "Creating FAISS index with 188 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 89 → 93 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 10/325\n",
      "\n",
      "🔄 Processing 83 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1974/39', 'id/ukpga/1986/45', 'id/ukpga/2006/14']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1134\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1134\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1134\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1134\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1134\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1134\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1134\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1134\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1134\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1134\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 1134\n",
      "Processing batch 1100 to 1134 (34 documents)\n",
      "Successfully embedded batch 1100 to 1134 of 1134\n",
      "Creating FAISS index with 1134 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 83 → 166 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 11/325\n",
      "\n",
      "🔄 Processing 81 rows with 4 acts\n",
      "🔨 Building new vector store for 4 acts...\n",
      "   Acts: ['id/ukpga/1989/41', 'id/ukpga/1996/27', 'id/ukpga/2018/12', 'id/ukpga/2021/17']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 729\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 729\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 729\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 729\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 729\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 729\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 729\n",
      "Processing batch 700 to 729 (29 documents)\n",
      "Successfully embedded batch 700 to 729 of 729\n",
      "Creating FAISS index with 729 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 81 → 200 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 12/325\n",
      "\n",
      "🔄 Processing 76 rows with 4 acts\n",
      "🔨 Building new vector store for 4 acts...\n",
      "   Acts: ['id/ukpga/1980/58', 'id/ukpga/1998/42', 'id/ukpga/2006/46', 'id/ukpga/Geo5/1-2/6']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1987\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1987\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1987\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1987\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1987\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1987\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1987\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1987\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1987\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1987\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 1987\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 1987\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 1987\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 1987\n",
      "Processing batch 1400 to 1500 (100 documents)\n",
      "Successfully embedded batch 1400 to 1500 of 1987\n",
      "Processing batch 1500 to 1600 (100 documents)\n",
      "Successfully embedded batch 1500 to 1600 of 1987\n",
      "Processing batch 1600 to 1700 (100 documents)\n",
      "Successfully embedded batch 1600 to 1700 of 1987\n",
      "Processing batch 1700 to 1800 (100 documents)\n",
      "Successfully embedded batch 1700 to 1800 of 1987\n",
      "Processing batch 1800 to 1900 (100 documents)\n",
      "Successfully embedded batch 1800 to 1900 of 1987\n",
      "Processing batch 1900 to 1987 (87 documents)\n",
      "Successfully embedded batch 1900 to 1987 of 1987\n",
      "Creating FAISS index with 1987 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 76 → 194 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 13/325\n",
      "\n",
      "🔄 Processing 76 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/2002/41']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 196\n",
      "Processing batch 100 to 196 (96 documents)\n",
      "Successfully embedded batch 100 to 196 of 196\n",
      "Creating FAISS index with 196 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 76 → 95 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 14/325\n",
      "\n",
      "🔄 Processing 75 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/2010/15']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 314\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 314\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 314\n",
      "Processing batch 300 to 314 (14 documents)\n",
      "Successfully embedded batch 300 to 314 of 314\n",
      "Creating FAISS index with 314 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 75 → 85 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 15/325\n",
      "\n",
      "🔄 Processing 70 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1973/18']\n",
      "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\n",
      "Processing batch 0 to 87 (87 documents)\n",
      "Successfully embedded batch 0 to 87 of 87\n",
      "Creating FAISS index with 87 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 70 → 76 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 16/325\n",
      "\n",
      "🔄 Processing 68 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/2003/41']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 296\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 296\n",
      "Processing batch 200 to 296 (96 documents)\n",
      "Successfully embedded batch 200 to 296 of 296\n",
      "Creating FAISS index with 296 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 68 → 78 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 17/325\n",
      "\n",
      "🔄 Processing 67 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/2003/44']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 496\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 496\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 496\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 496\n",
      "Processing batch 400 to 496 (96 documents)\n",
      "Successfully embedded batch 400 to 496 of 496\n",
      "Creating FAISS index with 496 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 67 → 71 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 18/325\n",
      "\n",
      "🔄 Processing 67 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/2000/36']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 117\n",
      "Processing batch 100 to 117 (17 documents)\n",
      "Successfully embedded batch 100 to 117 of 117\n",
      "Creating FAISS index with 117 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 67 → 68 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 19/325\n",
      "\n",
      "🔄 Processing 66 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1980/58']\n",
      "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\n",
      "Processing batch 0 to 65 (65 documents)\n",
      "Successfully embedded batch 0 to 65 of 65\n",
      "Creating FAISS index with 65 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 66 → 66 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 20/325\n",
      "\n",
      "🔄 Processing 66 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1996/18', 'id/ukpga/2010/15', 'id/ukpga/2010/27']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 810\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 810\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 810\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 810\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 810\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 810\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 810\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 810\n",
      "Processing batch 800 to 810 (10 documents)\n",
      "Successfully embedded batch 800 to 810 of 810\n",
      "Creating FAISS index with 810 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 66 → 136 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 21/325\n",
      "\n",
      "🔄 Processing 65 rows with 5 acts\n",
      "🔨 Building new vector store for 5 acts...\n",
      "   Acts: ['id/ukpga/1965/64', 'id/ukpga/1980/66', 'id/ukpga/1998/42', 'id/ukpga/Geo5/22-23/45', 'id/ukpga/Vict/20-21/31']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 558\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 558\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 558\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 558\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 558\n",
      "Processing batch 500 to 558 (58 documents)\n",
      "Successfully embedded batch 500 to 558 of 558\n",
      "Creating FAISS index with 558 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 65 → 177 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 22/325\n",
      "\n",
      "🔄 Processing 60 rows with 4 acts\n",
      "🔨 Building new vector store for 4 acts...\n",
      "   Acts: ['id/ukpga/1983/54', 'id/ukpga/1984/60', 'id/ukpga/1999/22', 'id/ukpga/2002/17']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 588\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 588\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 588\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 588\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 588\n",
      "Processing batch 500 to 588 (88 documents)\n",
      "Successfully embedded batch 500 to 588 of 588\n",
      "Creating FAISS index with 588 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 60 → 159 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 23/325\n",
      "\n",
      "🔄 Processing 57 rows with 7 acts\n",
      "🔨 Building new vector store for 7 acts...\n",
      "   Acts: ['id/ukpga/1983/20', 'id/ukpga/1998/42', 'id/ukpga/2005/12', 'id/ukpga/2005/9', 'id/ukpga/2008/14', 'id/ukpga/2010/15', 'id/ukpga/2015/2']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1124\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1124\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1124\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1124\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1124\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1124\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1124\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1124\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1124\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1124\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 1124\n",
      "Processing batch 1100 to 1124 (24 documents)\n",
      "Successfully embedded batch 1100 to 1124 of 1124\n",
      "Creating FAISS index with 1124 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 57 → 166 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 24/325\n",
      "\n",
      "🔄 Processing 56 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/2015/15']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 127\n",
      "Processing batch 100 to 127 (27 documents)\n",
      "Successfully embedded batch 100 to 127 of 127\n",
      "Creating FAISS index with 127 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 56 → 63 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 25/325\n",
      "\n",
      "🔄 Processing 56 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1990/8', 'id/ukpga/Geo6/10-11/51']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 604\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 604\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 604\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 604\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 604\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 604\n",
      "Processing batch 600 to 604 (4 documents)\n",
      "Successfully embedded batch 600 to 604 of 604\n",
      "Creating FAISS index with 604 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 56 → 65 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 26/325\n",
      "\n",
      "🔄 Processing 55 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/2004/12']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 563\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 563\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 563\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 563\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 563\n",
      "Processing batch 500 to 563 (63 documents)\n",
      "Successfully embedded batch 500 to 563 of 563\n",
      "Creating FAISS index with 563 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 55 → 55 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 27/325\n",
      "\n",
      "🔄 Processing 54 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/2006/46']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1853\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1853\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1853\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1853\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1853\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1853\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1853\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1853\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1853\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1853\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 1853\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 1853\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 1853\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 1853\n",
      "Processing batch 1400 to 1500 (100 documents)\n",
      "Successfully embedded batch 1400 to 1500 of 1853\n",
      "Processing batch 1500 to 1600 (100 documents)\n",
      "Successfully embedded batch 1500 to 1600 of 1853\n",
      "Processing batch 1600 to 1700 (100 documents)\n",
      "Successfully embedded batch 1600 to 1700 of 1853\n",
      "Processing batch 1700 to 1800 (100 documents)\n",
      "Successfully embedded batch 1700 to 1800 of 1853\n",
      "Processing batch 1800 to 1853 (53 documents)\n",
      "Successfully embedded batch 1800 to 1853 of 1853\n",
      "Creating FAISS index with 1853 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 54 → 60 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 28/325\n",
      "\n",
      "🔄 Processing 53 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1994/23']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 304\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 304\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 304\n",
      "Processing batch 300 to 304 (4 documents)\n",
      "Successfully embedded batch 300 to 304 of 304\n",
      "Creating FAISS index with 304 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 53 → 53 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 29/325\n",
      "\n",
      "🔄 Processing 53 rows with 6 acts\n",
      "🔨 Building new vector store for 6 acts...\n",
      "   Acts: ['id/ukpga/1970/9', 'id/ukpga/1992/4', 'id/ukpga/2008/29', 'id/ukpga/2008/30', 'id/ukpga/2008/9', 'id/ukpga/2020/7']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1584\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1584\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1584\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1584\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1584\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1584\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1584\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1584\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1584\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1584\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 1584\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 1584\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 1584\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 1584\n",
      "Processing batch 1400 to 1500 (100 documents)\n",
      "Successfully embedded batch 1400 to 1500 of 1584\n",
      "Processing batch 1500 to 1584 (84 documents)\n",
      "Successfully embedded batch 1500 to 1584 of 1584\n",
      "Creating FAISS index with 1584 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 53 → 145 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 30/325\n",
      "\n",
      "🔄 Processing 52 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1977/37']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 169\n",
      "Processing batch 100 to 169 (69 documents)\n",
      "Successfully embedded batch 100 to 169 of 169\n",
      "Creating FAISS index with 169 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 52 → 53 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 31/325\n",
      "\n",
      "🔄 Processing 51 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1978/47', 'id/ukpga/1996/40', 'id/ukpga/Geo6/2-3/37']\n",
      "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\n",
      "Processing batch 0 to 32 (32 documents)\n",
      "Successfully embedded batch 0 to 32 of 32\n",
      "Creating FAISS index with 32 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 51 → 123 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 32/325\n",
      "\n",
      "🔄 Processing 50 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/2002/29']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 831\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 831\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 831\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 831\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 831\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 831\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 831\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 831\n",
      "Processing batch 800 to 831 (31 documents)\n",
      "Successfully embedded batch 800 to 831 of 831\n",
      "Creating FAISS index with 831 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 50 → 50 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 33/325\n",
      "\n",
      "🔄 Processing 49 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1978/47']\n",
      "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\n",
      "Processing batch 0 to 10 (10 documents)\n",
      "Successfully embedded batch 0 to 10 of 10\n",
      "Creating FAISS index with 10 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 49 → 49 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 34/325\n",
      "\n",
      "🔄 Processing 48 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/2005/5', 'id/ukpga/2007/3', 'id/ukpga/2012/14']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 3649\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 3649\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 3649\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 3649\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 3649\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 3649\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 3649\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 3649\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 3649\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 3649\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 3649\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 3649\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 3649\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 3649\n",
      "Processing batch 1400 to 1500 (100 documents)\n",
      "Successfully embedded batch 1400 to 1500 of 3649\n",
      "Processing batch 1500 to 1600 (100 documents)\n",
      "Successfully embedded batch 1500 to 1600 of 3649\n",
      "Processing batch 1600 to 1700 (100 documents)\n",
      "Successfully embedded batch 1600 to 1700 of 3649\n",
      "Processing batch 1700 to 1800 (100 documents)\n",
      "Successfully embedded batch 1700 to 1800 of 3649\n",
      "Processing batch 1800 to 1900 (100 documents)\n",
      "Successfully embedded batch 1800 to 1900 of 3649\n",
      "Processing batch 1900 to 2000 (100 documents)\n",
      "Successfully embedded batch 1900 to 2000 of 3649\n",
      "Processing batch 2000 to 2100 (100 documents)\n",
      "Successfully embedded batch 2000 to 2100 of 3649\n",
      "Processing batch 2100 to 2200 (100 documents)\n",
      "Successfully embedded batch 2100 to 2200 of 3649\n",
      "Processing batch 2200 to 2300 (100 documents)\n",
      "Successfully embedded batch 2200 to 2300 of 3649\n",
      "Processing batch 2300 to 2400 (100 documents)\n",
      "Successfully embedded batch 2300 to 2400 of 3649\n",
      "Processing batch 2400 to 2500 (100 documents)\n",
      "Successfully embedded batch 2400 to 2500 of 3649\n",
      "Processing batch 2500 to 2600 (100 documents)\n",
      "Successfully embedded batch 2500 to 2600 of 3649\n",
      "Processing batch 2600 to 2700 (100 documents)\n",
      "Successfully embedded batch 2600 to 2700 of 3649\n",
      "Processing batch 2700 to 2800 (100 documents)\n",
      "Successfully embedded batch 2700 to 2800 of 3649\n",
      "Processing batch 2800 to 2900 (100 documents)\n",
      "Successfully embedded batch 2800 to 2900 of 3649\n",
      "Processing batch 2900 to 3000 (100 documents)\n",
      "Successfully embedded batch 2900 to 3000 of 3649\n",
      "Processing batch 3000 to 3100 (100 documents)\n",
      "Successfully embedded batch 3000 to 3100 of 3649\n",
      "Processing batch 3100 to 3200 (100 documents)\n",
      "Successfully embedded batch 3100 to 3200 of 3649\n",
      "Processing batch 3200 to 3300 (100 documents)\n",
      "Successfully embedded batch 3200 to 3300 of 3649\n",
      "Processing batch 3300 to 3400 (100 documents)\n",
      "Successfully embedded batch 3300 to 3400 of 3649\n",
      "Processing batch 3400 to 3500 (100 documents)\n",
      "Successfully embedded batch 3400 to 3500 of 3649\n",
      "Processing batch 3500 to 3600 (100 documents)\n",
      "Successfully embedded batch 3500 to 3600 of 3649\n",
      "Processing batch 3600 to 3649 (49 documents)\n",
      "Successfully embedded batch 3600 to 3649 of 3649\n",
      "Creating FAISS index with 3649 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 48 → 123 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 35/325\n",
      "\n",
      "🔄 Processing 48 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1998/29']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 113\n",
      "Processing batch 100 to 113 (13 documents)\n",
      "Successfully embedded batch 100 to 113 of 113\n",
      "Creating FAISS index with 113 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 48 → 48 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 36/325\n",
      "\n",
      "🔄 Processing 47 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1982/27', 'id/ukpga/1987/43', 'id/ukpga/1995/42']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 456\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 456\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 456\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 456\n",
      "Processing batch 400 to 456 (56 documents)\n",
      "Successfully embedded batch 400 to 456 of 456\n",
      "Creating FAISS index with 456 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 47 → 133 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 37/325\n",
      "\n",
      "🔄 Processing 47 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1984/60']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 247\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 247\n",
      "Processing batch 200 to 247 (47 documents)\n",
      "Successfully embedded batch 200 to 247 of 247\n",
      "Creating FAISS index with 247 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 47 → 52 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 38/325\n",
      "\n",
      "🔄 Processing 47 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1988/33']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 216\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 216\n",
      "Processing batch 200 to 216 (16 documents)\n",
      "Successfully embedded batch 200 to 216 of 216\n",
      "Creating FAISS index with 216 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 47 → 49 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 39/325\n",
      "\n",
      "🔄 Processing 47 rows with 6 acts\n",
      "🔨 Building new vector store for 6 acts...\n",
      "   Acts: ['id/ukpga/1988/4', 'id/ukpga/1990/8', 'id/ukpga/1990/9', 'id/ukpga/2004/5', 'id/ukpga/2006/16', 'id/ukpga/Geo6/12-13-14/97']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1219\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1219\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1219\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1219\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1219\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1219\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1219\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1219\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1219\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1219\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 1219\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 1219\n",
      "Processing batch 1200 to 1219 (19 documents)\n",
      "Successfully embedded batch 1200 to 1219 of 1219\n",
      "Creating FAISS index with 1219 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 47 → 131 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 40/325\n",
      "\n",
      "🔄 Processing 45 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1994/13', 'id/ukpga/2009/25', 'id/ukpga/2013/18']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 305\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 305\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 305\n",
      "Processing batch 300 to 305 (5 documents)\n",
      "Successfully embedded batch 300 to 305 of 305\n",
      "Creating FAISS index with 305 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 45 → 85 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 41/325\n",
      "\n",
      "🔄 Processing 45 rows with 4 acts\n",
      "🔨 Building new vector store for 4 acts...\n",
      "   Acts: ['id/ukpga/1968/64', 'id/ukpga/1972/30', 'id/ukpga/1995/38', 'id/ukpga/1997/12']\n",
      "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\n",
      "Processing batch 0 to 58 (58 documents)\n",
      "Successfully embedded batch 0 to 58 of 58\n",
      "Creating FAISS index with 58 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 45 → 134 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 42/325\n",
      "\n",
      "🔄 Processing 45 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/2004/34', 'id/ukpga/2016/22', 'id/ukpga/2019/4']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 611\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 611\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 611\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 611\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 611\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 611\n",
      "Processing batch 600 to 611 (11 documents)\n",
      "Successfully embedded batch 600 to 611 of 611\n",
      "Creating FAISS index with 611 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 45 → 132 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 43/325\n",
      "\n",
      "🔄 Processing 45 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1983/54']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 140\n",
      "Processing batch 100 to 140 (40 documents)\n",
      "Successfully embedded batch 100 to 140 of 140\n",
      "Creating FAISS index with 140 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 45 → 51 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 44/325\n",
      "\n",
      "🔄 Processing 44 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/Geo5/15-16/20']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 237\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 237\n",
      "Processing batch 200 to 237 (37 documents)\n",
      "Successfully embedded batch 200 to 237 of 237\n",
      "Creating FAISS index with 237 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 44 → 44 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 45/325\n",
      "\n",
      "🔄 Processing 44 rows with 5 acts\n",
      "🔨 Building new vector store for 5 acts...\n",
      "   Acts: ['id/ukpga/1971/77', 'id/ukpga/1993/23', 'id/ukpga/1998/42', 'id/ukpga/2002/41', 'id/ukpga/2014/22']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 487\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 487\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 487\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 487\n",
      "Processing batch 400 to 487 (87 documents)\n",
      "Successfully embedded batch 400 to 487 of 487\n",
      "Creating FAISS index with 487 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 44 → 132 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 46/325\n",
      "\n",
      "🔄 Processing 44 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1998/42', 'id/ukpga/2002/41', 'id/ukpga/2004/19']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 304\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 304\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 304\n",
      "Processing batch 300 to 304 (4 documents)\n",
      "Successfully embedded batch 300 to 304 of 304\n",
      "Creating FAISS index with 304 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 44 → 110 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 47/325\n",
      "\n",
      "🔄 Processing 43 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/Eliz2/2-3/56']\n",
      "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\n",
      "Processing batch 0 to 96 (96 documents)\n",
      "Successfully embedded batch 0 to 96 of 96\n",
      "Creating FAISS index with 96 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 43 → 44 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 48/325\n",
      "\n",
      "🔄 Processing 43 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1974/47']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 125\n",
      "Processing batch 100 to 125 (25 documents)\n",
      "Successfully embedded batch 100 to 125 of 125\n",
      "Creating FAISS index with 125 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 43 → 53 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 49/325\n",
      "\n",
      "🔄 Processing 43 rows with 2 acts\n",
      "📁 Loading cached vector store for 2 acts...\n",
      "✗ Failed to load cached vector store: Ran out of input\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1982/27', 'id/ukpga/1996/23']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 484\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 484\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 484\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 484\n",
      "Processing batch 400 to 484 (84 documents)\n",
      "Successfully embedded batch 400 to 484 of 484\n",
      "Creating FAISS index with 484 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 43 → 87 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 50/325\n",
      "\n",
      "🔄 Processing 43 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/Eliz2/6-7/53']\n",
      "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\n",
      "Processing batch 0 to 3 (3 documents)\n",
      "Successfully embedded batch 0 to 3 of 3\n",
      "Creating FAISS index with 3 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 43 → 43 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 51/325\n",
      "\n",
      "🔄 Processing 42 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1988/48']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 469\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 469\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 469\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 469\n",
      "Processing batch 400 to 469 (69 documents)\n",
      "Successfully embedded batch 400 to 469 of 469\n",
      "Creating FAISS index with 469 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 42 → 42 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 52/325\n",
      "\n",
      "🔄 Processing 42 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1971/77', 'id/ukpga/2001/12', 'id/ukpga/2006/13']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 198\n",
      "Processing batch 100 to 198 (98 documents)\n",
      "Successfully embedded batch 100 to 198 of 198\n",
      "Creating FAISS index with 198 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 42 → 109 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 53/325\n",
      "\n",
      "🔄 Processing 42 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1986/64']\n",
      "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\n",
      "Processing batch 0 to 66 (66 documents)\n",
      "Successfully embedded batch 0 to 66 of 66\n",
      "Creating FAISS index with 66 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 42 → 74 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 54/325\n",
      "\n",
      "🔄 Processing 42 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1989/41']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 244\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 244\n",
      "Processing batch 200 to 244 (44 documents)\n",
      "Successfully embedded batch 200 to 244 of 244\n",
      "Creating FAISS index with 244 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 42 → 44 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 55/325\n",
      "\n",
      "🔄 Processing 42 rows with 6 acts\n",
      "🔨 Building new vector store for 6 acts...\n",
      "   Acts: ['id/ukpga/1968/19', 'id/ukpga/1971/23', 'id/ukpga/1972/68', 'id/ukpga/1984/39', 'id/ukpga/1998/42', 'id/ukpga/2010/1']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 265\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 265\n",
      "Processing batch 200 to 265 (65 documents)\n",
      "Successfully embedded batch 200 to 265 of 265\n",
      "Creating FAISS index with 265 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 42 → 112 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 56/325\n",
      "\n",
      "🔄 Processing 42 rows with 5 acts\n",
      "🔨 Building new vector store for 5 acts...\n",
      "   Acts: ['id/ukpga/1984/28', 'id/ukpga/1986/45', 'id/ukpga/2002/29', 'id/ukpga/2002/9', 'id/ukpga/Geo5/15-16/20']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 2146\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 2146\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 2146\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 2146\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 2146\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 2146\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 2146\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 2146\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 2146\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 2146\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 2146\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 2146\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 2146\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 2146\n",
      "Processing batch 1400 to 1500 (100 documents)\n",
      "Successfully embedded batch 1400 to 1500 of 2146\n",
      "Processing batch 1500 to 1600 (100 documents)\n",
      "Successfully embedded batch 1500 to 1600 of 2146\n",
      "Processing batch 1600 to 1700 (100 documents)\n",
      "Successfully embedded batch 1600 to 1700 of 2146\n",
      "Processing batch 1700 to 1800 (100 documents)\n",
      "Successfully embedded batch 1700 to 1800 of 2146\n",
      "Processing batch 1800 to 1900 (100 documents)\n",
      "Successfully embedded batch 1800 to 1900 of 2146\n",
      "Processing batch 1900 to 2000 (100 documents)\n",
      "Successfully embedded batch 1900 to 2000 of 2146\n",
      "Processing batch 2000 to 2100 (100 documents)\n",
      "Successfully embedded batch 2000 to 2100 of 2146\n",
      "Processing batch 2100 to 2146 (46 documents)\n",
      "Successfully embedded batch 2100 to 2146 of 2146\n",
      "Creating FAISS index with 2146 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 42 → 122 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 57/325\n",
      "\n",
      "🔄 Processing 41 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1998/29', 'id/ukpga/2000/36', 'id/ukpga/2018/12']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 512\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 512\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 512\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 512\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 512\n",
      "Processing batch 500 to 512 (12 documents)\n",
      "Successfully embedded batch 500 to 512 of 512\n",
      "Creating FAISS index with 512 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 41 → 94 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 58/325\n",
      "\n",
      "🔄 Processing 40 rows with 7 acts\n",
      "🔨 Building new vector store for 7 acts...\n",
      "   Acts: ['id/ukpga/1989/41', 'id/ukpga/1990/37', 'id/ukpga/1998/42', 'id/ukpga/2002/38', 'id/ukpga/2003/41', 'id/ukpga/2006/47', 'id/ukpga/2008/22']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1015\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1015\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1015\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1015\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1015\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1015\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1015\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1015\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1015\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1015\n",
      "Processing batch 1000 to 1015 (15 documents)\n",
      "Successfully embedded batch 1000 to 1015 of 1015\n",
      "Creating FAISS index with 1015 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 40 → 118 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 59/325\n",
      "\n",
      "🔄 Processing 39 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1990/8', 'id/ukpga/1998/42', 'id/ukpga/2010/15']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 967\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 967\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 967\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 967\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 967\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 967\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 967\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 967\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 967\n",
      "Processing batch 900 to 967 (67 documents)\n",
      "Successfully embedded batch 900 to 967 of 967\n",
      "Creating FAISS index with 967 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 39 → 67 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 60/325\n",
      "\n",
      "🔄 Processing 39 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1970/9', 'id/ukpga/2005/19', 'id/ukpga/2008/9']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 980\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 980\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 980\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 980\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 980\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 980\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 980\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 980\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 980\n",
      "Processing batch 900 to 980 (80 documents)\n",
      "Successfully embedded batch 900 to 980 of 980\n",
      "Creating FAISS index with 980 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 39 → 90 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 61/325\n",
      "\n",
      "🔄 Processing 39 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1983/20', 'id/ukpga/2005/9']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 376\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 376\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 376\n",
      "Processing batch 300 to 376 (76 documents)\n",
      "Successfully embedded batch 300 to 376 of 376\n",
      "Creating FAISS index with 376 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 39 → 80 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 62/325\n",
      "\n",
      "🔄 Processing 39 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1984/42']\n",
      "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\n",
      "Processing batch 0 to 90 (90 documents)\n",
      "Successfully embedded batch 0 to 90 of 90\n",
      "Creating FAISS index with 90 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 39 → 40 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 63/325\n",
      "\n",
      "🔄 Processing 39 rows with 4 acts\n",
      "🔨 Building new vector store for 4 acts...\n",
      "   Acts: ['id/ukpga/1998/42', 'id/ukpga/1999/33', 'id/ukpga/2002/41', 'id/ukpga/2004/19']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 511\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 511\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 511\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 511\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 511\n",
      "Processing batch 500 to 511 (11 documents)\n",
      "Successfully embedded batch 500 to 511 of 511\n",
      "Creating FAISS index with 511 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 39 → 117 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 64/325\n",
      "\n",
      "🔄 Processing 38 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1990/43', 'id/ukpga/1995/25']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 521\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 521\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 521\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 521\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 521\n",
      "Processing batch 500 to 521 (21 documents)\n",
      "Successfully embedded batch 500 to 521 of 521\n",
      "Creating FAISS index with 521 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 38 → 81 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 65/325\n",
      "\n",
      "🔄 Processing 37 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1971/77']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 102\n",
      "Processing batch 100 to 102 (2 documents)\n",
      "Successfully embedded batch 100 to 102 of 102\n",
      "Creating FAISS index with 102 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 37 → 37 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 66/325\n",
      "\n",
      "🔄 Processing 37 rows with 5 acts\n",
      "🔨 Building new vector store for 5 acts...\n",
      "   Acts: ['id/ukpga/1981/61', 'id/ukpga/1990/42', 'id/ukpga/2003/21', 'id/ukpga/2018/13', 'id/ukpga/Geo6/6-7/40']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1248\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1248\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1248\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1248\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1248\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1248\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Token limit exceeded. Reducing batch size to 50 and retrying...\n",
      "Error creating vector store: name 'time' is not defined\n",
      "✗ Error processing group 66: name 'time' is not defined\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 67/325\n",
      "\n",
      "🔄 Processing 37 rows with 10 acts\n",
      "🔨 Building new vector store for 10 acts...\n",
      "   Acts: ['id/ukpga/1965/25', 'id/ukpga/1976/80', 'id/ukpga/1977/42', 'id/ukpga/1985/68', 'id/ukpga/1986/45', 'id/ukpga/1988/50', 'id/ukpga/1990/41', 'id/ukpga/1993/48', 'id/ukpga/1999/30', 'id/ukpga/Geo5/4-5/59']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 2488\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 2488\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 2488\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 2488\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 2488\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 2488\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 2488\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 2488\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 2488\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 2488\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 2488\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 2488\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 2488\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 2488\n",
      "Processing batch 1400 to 1500 (100 documents)\n",
      "Successfully embedded batch 1400 to 1500 of 2488\n",
      "Processing batch 1500 to 1600 (100 documents)\n",
      "Successfully embedded batch 1500 to 1600 of 2488\n",
      "Processing batch 1600 to 1700 (100 documents)\n",
      "Successfully embedded batch 1600 to 1700 of 2488\n",
      "Processing batch 1700 to 1800 (100 documents)\n",
      "Successfully embedded batch 1700 to 1800 of 2488\n",
      "Processing batch 1800 to 1900 (100 documents)\n",
      "Successfully embedded batch 1800 to 1900 of 2488\n",
      "Processing batch 1900 to 2000 (100 documents)\n",
      "Successfully embedded batch 1900 to 2000 of 2488\n",
      "Processing batch 2000 to 2100 (100 documents)\n",
      "Successfully embedded batch 2000 to 2100 of 2488\n",
      "Processing batch 2100 to 2200 (100 documents)\n",
      "Successfully embedded batch 2100 to 2200 of 2488\n",
      "Processing batch 2200 to 2300 (100 documents)\n",
      "Successfully embedded batch 2200 to 2300 of 2488\n",
      "Processing batch 2300 to 2400 (100 documents)\n",
      "Successfully embedded batch 2300 to 2400 of 2488\n",
      "Processing batch 2400 to 2488 (88 documents)\n",
      "Successfully embedded batch 2400 to 2488 of 2488\n",
      "Creating FAISS index with 2488 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 37 → 105 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 68/325\n",
      "\n",
      "🔄 Processing 37 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1992/52', 'id/ukpga/1999/26']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 489\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 489\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 489\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 489\n",
      "Processing batch 400 to 489 (89 documents)\n",
      "Successfully embedded batch 400 to 489 of 489\n",
      "Creating FAISS index with 489 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 37 → 71 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 69/325\n",
      "\n",
      "🔄 Processing 36 rows with 11 acts\n",
      "🔨 Building new vector store for 11 acts...\n",
      "   Acts: ['id/ukpga/1965/57', 'id/ukpga/1976/35', 'id/ukpga/1978/30', 'id/ukpga/1987/4', 'id/ukpga/1996/16', 'id/ukpga/2001/24', 'id/ukpga/2003/20', 'id/ukpga/2004/20', 'id/ukpga/2012/11', 'id/ukpga/2013/25', 'id/ukpga/Eliz2/2-3/32']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1026\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1026\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1026\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1026\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1026\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1026\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1026\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1026\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1026\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1026\n",
      "Processing batch 1000 to 1026 (26 documents)\n",
      "Successfully embedded batch 1000 to 1026 of 1026\n",
      "Creating FAISS index with 1026 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 36 → 107 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 70/325\n",
      "\n",
      "🔄 Processing 36 rows with 8 acts\n",
      "🔨 Building new vector store for 8 acts...\n",
      "   Acts: ['id/ukpga/1970/42', 'id/ukpga/1976/74', 'id/ukpga/1981/54', 'id/ukpga/1990/19', 'id/ukpga/1998/42', 'id/ukpga/1999/27', 'id/ukpga/2010/15', 'id/ukpga/Geo6/11-12/29']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 900\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 900\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 900\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 900\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 900\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 900\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 900\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 900\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 900\n",
      "Creating FAISS index with 900 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 36 → 102 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 71/325\n",
      "\n",
      "🔄 Processing 35 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1971/77', 'id/ukpga/1999/33']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 309\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 309\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 309\n",
      "Processing batch 300 to 309 (9 documents)\n",
      "Successfully embedded batch 300 to 309 of 309\n",
      "Creating FAISS index with 309 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 35 → 72 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 72/325\n",
      "\n",
      "🔄 Processing 35 rows with 9 acts\n",
      "🔨 Building new vector store for 9 acts...\n",
      "   Acts: ['id/ukpga/1968/72', 'id/ukpga/1971/78', 'id/ukpga/1978/30', 'id/ukpga/1990/11', 'id/ukpga/1990/8', 'id/ukpga/1991/34', 'id/ukpga/2004/5', 'id/ukpga/Eliz2/10-11/38', 'id/ukpga/Geo6/10-11/51']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1679\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1679\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1679\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1679\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1679\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1679\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1679\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1679\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1679\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1679\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 1679\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 1679\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 1679\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 1679\n",
      "Processing batch 1400 to 1500 (100 documents)\n",
      "Successfully embedded batch 1400 to 1500 of 1679\n",
      "Processing batch 1500 to 1600 (100 documents)\n",
      "Successfully embedded batch 1500 to 1600 of 1679\n",
      "Processing batch 1600 to 1679 (79 documents)\n",
      "Successfully embedded batch 1600 to 1679 of 1679\n",
      "Creating FAISS index with 1679 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 35 → 105 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 73/325\n",
      "\n",
      "🔄 Processing 35 rows with 4 acts\n",
      "🔨 Building new vector store for 4 acts...\n",
      "   Acts: ['id/ukpga/1991/53', 'id/ukpga/1998/37', 'id/ukpga/2000/6', 'id/ukpga/2003/44']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1051\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1051\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1051\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1051\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1051\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1051\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1051\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1051\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1051\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1051\n",
      "Processing batch 1000 to 1051 (51 documents)\n",
      "Successfully embedded batch 1000 to 1051 of 1051\n",
      "Creating FAISS index with 1051 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 35 → 102 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 74/325\n",
      "\n",
      "🔄 Processing 34 rows with 8 acts\n",
      "🔨 Building new vector store for 8 acts...\n",
      "   Acts: ['id/ukpga/1973/18', 'id/ukpga/1989/41', 'id/ukpga/1994/33', 'id/ukpga/1995/50', 'id/ukpga/1996/52', 'id/ukpga/2004/31', 'id/ukpga/2005/13', 'id/ukpga/2009/11']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1458\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1458\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1458\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1458\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1458\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1458\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1458\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1458\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1458\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1458\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 1458\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 1458\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 1458\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 1458\n",
      "Processing batch 1400 to 1458 (58 documents)\n",
      "Successfully embedded batch 1400 to 1458 of 1458\n",
      "Creating FAISS index with 1458 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 34 → 97 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 75/325\n",
      "\n",
      "🔄 Processing 34 rows with 14 acts\n",
      "🔨 Building new vector store for 14 acts...\n",
      "   Acts: ['id/ukpga/1969/58', 'id/ukpga/1976/74', 'id/ukpga/1982/34', 'id/ukpga/1982/39', 'id/ukpga/1983/19', 'id/ukpga/1983/20', 'id/ukpga/1991/25', 'id/ukpga/1993/36', 'id/ukpga/1997/43', 'id/ukpga/1998/42', 'id/ukpga/2003/44', 'id/ukpga/2008/4', 'id/ukpga/2009/25', 'id/ukpga/Eliz2/5-6/11']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1784\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1784\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1784\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1784\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1784\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1784\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1784\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1784\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1784\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1784\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 1784\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 1784\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 1784\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 1784\n",
      "Processing batch 1400 to 1500 (100 documents)\n",
      "Successfully embedded batch 1400 to 1500 of 1784\n",
      "Processing batch 1500 to 1600 (100 documents)\n",
      "Successfully embedded batch 1500 to 1600 of 1784\n",
      "Processing batch 1600 to 1700 (100 documents)\n",
      "Successfully embedded batch 1600 to 1700 of 1784\n",
      "Processing batch 1700 to 1784 (84 documents)\n",
      "Successfully embedded batch 1700 to 1784 of 1784\n",
      "Creating FAISS index with 1784 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 34 → 101 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 76/325\n",
      "\n",
      "🔄 Processing 34 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1984/60', 'id/ukpga/2003/44']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 743\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 743\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 743\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 743\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 743\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 743\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 743\n",
      "Processing batch 700 to 743 (43 documents)\n",
      "Successfully embedded batch 700 to 743 of 743\n",
      "Creating FAISS index with 743 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 34 → 81 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 77/325\n",
      "\n",
      "🔄 Processing 34 rows with 6 acts\n",
      "🔨 Building new vector store for 6 acts...\n",
      "   Acts: ['id/ukpga/1981/54', 'id/ukpga/1997/68', 'id/ukpga/2000/11', 'id/ukpga/2011/23', 'id/ukpga/2013/18', 'id/ukpga/2015/6']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 558\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 558\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 558\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 558\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 558\n",
      "Processing batch 500 to 558 (58 documents)\n",
      "Successfully embedded batch 500 to 558 of 558\n",
      "Creating FAISS index with 558 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 34 → 101 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 78/325\n",
      "\n",
      "🔄 Processing 34 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1971/77', 'id/ukpga/1977/45', 'id/ukpga/1984/60']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 452\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 452\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 452\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 452\n",
      "Processing batch 400 to 452 (52 documents)\n",
      "Successfully embedded batch 400 to 452 of 452\n",
      "Creating FAISS index with 452 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 34 → 96 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 79/325\n",
      "\n",
      "🔄 Processing 33 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1989/41', 'id/ukpga/1998/42', 'id/ukpga/2002/41']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 489\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 489\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 489\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 489\n",
      "Processing batch 400 to 489 (89 documents)\n",
      "Successfully embedded batch 400 to 489 of 489\n",
      "Creating FAISS index with 489 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 33 → 76 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 80/325\n",
      "\n",
      "🔄 Processing 33 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1968/19', 'id/ukpga/2002/29']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 901\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 901\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 901\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 901\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 901\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 901\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 901\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 901\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 901\n",
      "Processing batch 900 to 901 (1 documents)\n",
      "Successfully embedded batch 900 to 901 of 901\n",
      "Creating FAISS index with 901 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 33 → 64 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 81/325\n",
      "\n",
      "🔄 Processing 33 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1980/22', 'id/ukpga/2006/46', 'id/ukpga/Geo6/11-12/38']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 2362\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 2362\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 2362\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 2362\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 2362\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 2362\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 2362\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 2362\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 2362\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 2362\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 2362\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 2362\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 2362\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 2362\n",
      "Processing batch 1400 to 1500 (100 documents)\n",
      "Successfully embedded batch 1400 to 1500 of 2362\n",
      "Processing batch 1500 to 1600 (100 documents)\n",
      "Successfully embedded batch 1500 to 1600 of 2362\n",
      "Processing batch 1600 to 1700 (100 documents)\n",
      "Successfully embedded batch 1600 to 1700 of 2362\n",
      "Processing batch 1700 to 1800 (100 documents)\n",
      "Successfully embedded batch 1700 to 1800 of 2362\n",
      "Processing batch 1800 to 1900 (100 documents)\n",
      "Successfully embedded batch 1800 to 1900 of 2362\n",
      "Processing batch 1900 to 2000 (100 documents)\n",
      "Successfully embedded batch 1900 to 2000 of 2362\n",
      "Processing batch 2000 to 2100 (100 documents)\n",
      "Successfully embedded batch 2000 to 2100 of 2362\n",
      "Processing batch 2100 to 2200 (100 documents)\n",
      "Successfully embedded batch 2100 to 2200 of 2362\n",
      "Processing batch 2200 to 2300 (100 documents)\n",
      "Successfully embedded batch 2200 to 2300 of 2362\n",
      "Processing batch 2300 to 2362 (62 documents)\n",
      "Successfully embedded batch 2300 to 2362 of 2362\n",
      "Creating FAISS index with 2362 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 33 → 70 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 82/325\n",
      "\n",
      "🔄 Processing 33 rows with 6 acts\n",
      "🔨 Building new vector store for 6 acts...\n",
      "   Acts: ['id/ukpga/1992/4', 'id/ukpga/1992/5', 'id/ukpga/2000/14', 'id/ukpga/2006/41', 'id/ukpga/2007/12', 'id/ukpga/Geo6/11-12/29']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1607\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1607\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1607\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1607\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1607\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1607\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1607\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1607\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1607\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1607\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 1607\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 1607\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 1607\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 1607\n",
      "Processing batch 1400 to 1500 (100 documents)\n",
      "Successfully embedded batch 1400 to 1500 of 1607\n",
      "Processing batch 1500 to 1600 (100 documents)\n",
      "Successfully embedded batch 1500 to 1600 of 1607\n",
      "Processing batch 1600 to 1607 (7 documents)\n",
      "Successfully embedded batch 1600 to 1607 of 1607\n",
      "Creating FAISS index with 1607 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 33 → 97 rows\n",
      "💾 Saving intermediate results...\n",
      "✓ Saved 8785 rows to ../data/final_test/processed_with_sections_all.csv\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 83/325\n",
      "\n",
      "🔄 Processing 32 rows with 6 acts\n",
      "🔨 Building new vector store for 6 acts...\n",
      "   Acts: ['id/ukpga/1983/54', 'id/ukpga/1984/24', 'id/ukpga/1996/16', 'id/ukpga/1998/42', 'id/ukpga/2002/30', 'id/ukpga/2011/13']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 902\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 902\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 902\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 902\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 902\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 902\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 902\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 902\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 902\n",
      "Processing batch 900 to 902 (2 documents)\n",
      "Successfully embedded batch 900 to 902 of 902\n",
      "Creating FAISS index with 902 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 32 → 96 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 84/325\n",
      "\n",
      "🔄 Processing 32 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/2006/3', 'id/ukpga/2010/15']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 421\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 421\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 421\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 421\n",
      "Processing batch 400 to 421 (21 documents)\n",
      "Successfully embedded batch 400 to 421 of 421\n",
      "Creating FAISS index with 421 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 32 → 38 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 85/325\n",
      "\n",
      "🔄 Processing 32 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1998/42', 'id/ukpga/2002/41']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 245\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 245\n",
      "Processing batch 200 to 245 (45 documents)\n",
      "Successfully embedded batch 200 to 245 of 245\n",
      "Creating FAISS index with 245 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 32 → 56 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 86/325\n",
      "\n",
      "🔄 Processing 32 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1999/33']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 207\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 207\n",
      "Processing batch 200 to 207 (7 documents)\n",
      "Successfully embedded batch 200 to 207 of 207\n",
      "Creating FAISS index with 207 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 32 → 35 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 87/325\n",
      "\n",
      "🔄 Processing 32 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/2013/26']\n",
      "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\n",
      "Processing batch 0 to 18 (18 documents)\n",
      "Successfully embedded batch 0 to 18 of 18\n",
      "Creating FAISS index with 18 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 32 → 32 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 88/325\n",
      "\n",
      "🔄 Processing 31 rows with 8 acts\n",
      "🔨 Building new vector store for 8 acts...\n",
      "   Acts: ['id/ukpga/1970/9', 'id/ukpga/1986/32', 'id/ukpga/1998/14', 'id/ukpga/2002/21', 'id/ukpga/2007/15', 'id/ukpga/2008/9', 'id/ukpga/2012/10', 'id/ukpga/2012/5']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1431\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1431\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1431\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1431\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1431\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1431\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1431\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1431\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1431\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1431\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 1431\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 1431\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 1431\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 1431\n",
      "Processing batch 1400 to 1431 (31 documents)\n",
      "Successfully embedded batch 1400 to 1431 of 1431\n",
      "Creating FAISS index with 1431 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 31 → 92 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 89/325\n",
      "\n",
      "🔄 Processing 31 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1990/8', 'id/ukpga/1991/34', 'id/ukpga/1998/42']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 763\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 763\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 763\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 763\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 763\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 763\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 763\n",
      "Processing batch 700 to 763 (63 documents)\n",
      "Successfully embedded batch 700 to 763 of 763\n",
      "Creating FAISS index with 763 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 31 → 79 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 90/325\n",
      "\n",
      "🔄 Processing 31 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1973/18', 'id/ukpga/1996/47']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 119\n",
      "Processing batch 100 to 119 (19 documents)\n",
      "Successfully embedded batch 100 to 119 of 119\n",
      "Creating FAISS index with 119 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 31 → 55 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 91/325\n",
      "\n",
      "🔄 Processing 31 rows with 4 acts\n",
      "🔨 Building new vector store for 4 acts...\n",
      "   Acts: ['id/ukpga/1964/30', 'id/ukpga/1988/34', 'id/ukpga/1989/41', 'id/ukpga/1999/22']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 445\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 445\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 445\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 445\n",
      "Processing batch 400 to 445 (45 documents)\n",
      "Successfully embedded batch 400 to 445 of 445\n",
      "Creating FAISS index with 445 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 31 → 89 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 92/325\n",
      "\n",
      "🔄 Processing 31 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1985/61', 'id/ukpga/2006/46', 'id/ukpga/Geo6/11-12/38']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 2451\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 2451\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 2451\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 2451\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 2451\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 2451\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 2451\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 2451\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 2451\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 2451\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 2451\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 2451\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 2451\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 2451\n",
      "Processing batch 1400 to 1500 (100 documents)\n",
      "Successfully embedded batch 1400 to 1500 of 2451\n",
      "Processing batch 1500 to 1600 (100 documents)\n",
      "Successfully embedded batch 1500 to 1600 of 2451\n",
      "Processing batch 1600 to 1700 (100 documents)\n",
      "Successfully embedded batch 1600 to 1700 of 2451\n",
      "Processing batch 1700 to 1800 (100 documents)\n",
      "Successfully embedded batch 1700 to 1800 of 2451\n",
      "Processing batch 1800 to 1900 (100 documents)\n",
      "Successfully embedded batch 1800 to 1900 of 2451\n",
      "Processing batch 1900 to 2000 (100 documents)\n",
      "Successfully embedded batch 1900 to 2000 of 2451\n",
      "Processing batch 2000 to 2100 (100 documents)\n",
      "Successfully embedded batch 2000 to 2100 of 2451\n",
      "Processing batch 2100 to 2200 (100 documents)\n",
      "Successfully embedded batch 2100 to 2200 of 2451\n",
      "Processing batch 2200 to 2300 (100 documents)\n",
      "Successfully embedded batch 2200 to 2300 of 2451\n",
      "Processing batch 2300 to 2400 (100 documents)\n",
      "Successfully embedded batch 2300 to 2400 of 2451\n",
      "Processing batch 2400 to 2451 (51 documents)\n",
      "Successfully embedded batch 2400 to 2451 of 2451\n",
      "Creating FAISS index with 2451 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 31 → 71 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 93/325\n",
      "\n",
      "🔄 Processing 30 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1972/68', 'id/ukpga/1974/37', 'id/ukpga/2003/44']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 617\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 617\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 617\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 617\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 617\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 617\n",
      "Processing batch 600 to 617 (17 documents)\n",
      "Successfully embedded batch 600 to 617 of 617\n",
      "Creating FAISS index with 617 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 30 → 63 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 94/325\n",
      "\n",
      "🔄 Processing 30 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1984/60', 'id/ukpga/2002/29']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1078\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1078\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1078\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1078\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1078\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1078\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1078\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1078\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1078\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1078\n",
      "Processing batch 1000 to 1078 (78 documents)\n",
      "Successfully embedded batch 1000 to 1078 of 1078\n",
      "Creating FAISS index with 1078 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 30 → 65 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 95/325\n",
      "\n",
      "🔄 Processing 30 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1981/54', 'id/ukpga/1988/33']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 404\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 404\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 404\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 404\n",
      "Processing batch 400 to 404 (4 documents)\n",
      "Successfully embedded batch 400 to 404 of 404\n",
      "Creating FAISS index with 404 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 30 → 60 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 96/325\n",
      "\n",
      "🔄 Processing 30 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1998/42', 'id/ukpga/2003/41']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 345\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 345\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 345\n",
      "Processing batch 300 to 345 (45 documents)\n",
      "Successfully embedded batch 300 to 345 of 345\n",
      "Creating FAISS index with 345 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 30 → 33 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 97/325\n",
      "\n",
      "🔄 Processing 30 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1990/9', 'id/ukpga/2004/5']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 319\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 319\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 319\n",
      "Processing batch 300 to 319 (19 documents)\n",
      "Successfully embedded batch 300 to 319 of 319\n",
      "Creating FAISS index with 319 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 30 → 60 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 98/325\n",
      "\n",
      "🔄 Processing 30 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1968/19', 'id/ukpga/Eliz2/5-6/11']\n",
      "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\n",
      "Processing batch 0 to 86 (86 documents)\n",
      "Successfully embedded batch 0 to 86 of 86\n",
      "Creating FAISS index with 86 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 30 → 61 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 99/325\n",
      "\n",
      "🔄 Processing 29 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1991/53', 'id/ukpga/1997/43', 'id/ukpga/2003/44']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 714\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 714\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 714\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 714\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 714\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 714\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 714\n",
      "Processing batch 700 to 714 (14 documents)\n",
      "Successfully embedded batch 700 to 714 of 714\n",
      "Creating FAISS index with 714 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 29 → 79 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 100/325\n",
      "\n",
      "🔄 Processing 29 rows with 4 acts\n",
      "🔨 Building new vector store for 4 acts...\n",
      "   Acts: ['id/ukpga/1973/18', 'id/ukpga/1989/41', 'id/ukpga/1991/48', 'id/ukpga/1996/27']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 546\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 546\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 546\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 546\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 546\n",
      "Processing batch 500 to 546 (46 documents)\n",
      "Successfully embedded batch 500 to 546 of 546\n",
      "Creating FAISS index with 546 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 29 → 84 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 101/325\n",
      "\n",
      "🔄 Processing 29 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1980/43', 'id/ukpga/1998/37', 'id/ukpga/2014/12']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 641\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 641\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 641\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 641\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 641\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 641\n",
      "Processing batch 600 to 641 (41 documents)\n",
      "Successfully embedded batch 600 to 641 of 641\n",
      "Creating FAISS index with 641 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 29 → 84 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 102/325\n",
      "\n",
      "🔄 Processing 29 rows with 1 acts\n",
      "🔨 Building new vector store for 1 acts...\n",
      "   Acts: ['id/ukpga/1994/26']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 142\n",
      "Processing batch 100 to 142 (42 documents)\n",
      "Successfully embedded batch 100 to 142 of 142\n",
      "Creating FAISS index with 142 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 29 → 30 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 103/325\n",
      "\n",
      "🔄 Processing 29 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1980/66', 'id/ukpga/1990/8', 'id/ukpga/Will4/5-6/50']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1128\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1128\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1128\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1128\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1128\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1128\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1128\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1128\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1128\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1128\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 1128\n",
      "Processing batch 1100 to 1128 (28 documents)\n",
      "Successfully embedded batch 1100 to 1128 of 1128\n",
      "Creating FAISS index with 1128 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 29 → 74 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 104/325\n",
      "\n",
      "🔄 Processing 28 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1989/41', 'id/ukpga/1998/42', 'id/ukpga/2021/17']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 395\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 395\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 395\n",
      "Processing batch 300 to 395 (95 documents)\n",
      "Successfully embedded batch 300 to 395 of 395\n",
      "Creating FAISS index with 395 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 28 → 42 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 105/325\n",
      "\n",
      "🔄 Processing 28 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1989/41', 'id/ukpga/1998/42']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 293\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 293\n",
      "Processing batch 200 to 293 (93 documents)\n",
      "Successfully embedded batch 200 to 293 of 293\n",
      "Creating FAISS index with 293 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 28 → 44 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 106/325\n",
      "\n",
      "🔄 Processing 28 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/Eliz2/10-11/19', 'id/ukpga/Geo6/10-11/44', 'id/ukpga/Geo6/11-12/56']\n",
      "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\n",
      "Processing batch 0 to 79 (79 documents)\n",
      "Successfully embedded batch 0 to 79 of 79\n",
      "Creating FAISS index with 79 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 28 → 59 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 107/325\n",
      "\n",
      "🔄 Processing 28 rows with 10 acts\n",
      "🔨 Building new vector store for 10 acts...\n",
      "   Acts: ['id/ukpga/1968/19', 'id/ukpga/1970/31', 'id/ukpga/1980/43', 'id/ukpga/1982/48', 'id/ukpga/1985/23', 'id/ukpga/2000/6', 'id/ukpga/2003/39', 'id/ukpga/2003/44', 'id/ukpga/2010/40', 'id/ukpga/2015/2']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 1518\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 1518\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 1518\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 1518\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 1518\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 1518\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 1518\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 1518\n",
      "Processing batch 800 to 900 (100 documents)\n",
      "Successfully embedded batch 800 to 900 of 1518\n",
      "Processing batch 900 to 1000 (100 documents)\n",
      "Successfully embedded batch 900 to 1000 of 1518\n",
      "Processing batch 1000 to 1100 (100 documents)\n",
      "Successfully embedded batch 1000 to 1100 of 1518\n",
      "Processing batch 1100 to 1200 (100 documents)\n",
      "Successfully embedded batch 1100 to 1200 of 1518\n",
      "Processing batch 1200 to 1300 (100 documents)\n",
      "Successfully embedded batch 1200 to 1300 of 1518\n",
      "Processing batch 1300 to 1400 (100 documents)\n",
      "Successfully embedded batch 1300 to 1400 of 1518\n",
      "Processing batch 1400 to 1500 (100 documents)\n",
      "Successfully embedded batch 1400 to 1500 of 1518\n",
      "Processing batch 1500 to 1518 (18 documents)\n",
      "Successfully embedded batch 1500 to 1518 of 1518\n",
      "Creating FAISS index with 1518 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 28 → 84 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 108/325\n",
      "\n",
      "🔄 Processing 28 rows with 3 acts\n",
      "🔨 Building new vector store for 3 acts...\n",
      "   Acts: ['id/ukpga/1988/4', 'id/ukpga/1990/8', 'id/ukpga/2004/5']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 816\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 816\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 816\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 816\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 816\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 816\n",
      "Processing batch 600 to 700 (100 documents)\n",
      "Successfully embedded batch 600 to 700 of 816\n",
      "Processing batch 700 to 800 (100 documents)\n",
      "Successfully embedded batch 700 to 800 of 816\n",
      "Processing batch 800 to 816 (16 documents)\n",
      "Successfully embedded batch 800 to 816 of 816\n",
      "Creating FAISS index with 816 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 28 → 65 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 109/325\n",
      "\n",
      "🔄 Processing 28 rows with 4 acts\n",
      "🔨 Building new vector store for 4 acts...\n",
      "   Acts: ['id/ukpga/1983/19', 'id/ukpga/1983/20', 'id/ukpga/1989/41', 'id/ukpga/2005/9']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 636\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 636\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 636\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 636\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 636\n",
      "Processing batch 500 to 600 (100 documents)\n",
      "Successfully embedded batch 500 to 600 of 636\n",
      "Processing batch 600 to 636 (36 documents)\n",
      "Successfully embedded batch 600 to 636 of 636\n",
      "Creating FAISS index with 636 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 28 → 78 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 110/325\n",
      "\n",
      "🔄 Processing 27 rows with 5 acts\n",
      "🔨 Building new vector store for 5 acts...\n",
      "   Acts: ['id/ukpga/1981/49', 'id/ukpga/1981/54', 'id/ukpga/1998/42', 'id/ukpga/1999/23', 'id/ukpga/Geo5/23-24/12']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 451\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 451\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 451\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 451\n",
      "Processing batch 400 to 451 (51 documents)\n",
      "Successfully embedded batch 400 to 451 of 451\n",
      "Creating FAISS index with 451 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 27 → 80 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 111/325\n",
      "\n",
      "🔄 Processing 27 rows with 7 acts\n",
      "🔨 Building new vector store for 7 acts...\n",
      "   Acts: ['id/ukpga/1967/9', 'id/ukpga/1988/41', 'id/ukpga/Edw7/8/64', 'id/ukpga/Eliz2/4-5/60', 'id/ukpga/Geo5/12-13/51', 'id/ukpga/Geo5/13-14/10', 'id/ukpga/Geo5/8-9/40']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 520\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 520\n",
      "Processing batch 200 to 300 (100 documents)\n",
      "Successfully embedded batch 200 to 300 of 520\n",
      "Processing batch 300 to 400 (100 documents)\n",
      "Successfully embedded batch 300 to 400 of 520\n",
      "Processing batch 400 to 500 (100 documents)\n",
      "Successfully embedded batch 400 to 500 of 520\n",
      "Processing batch 500 to 520 (20 documents)\n",
      "Successfully embedded batch 500 to 520 of 520\n",
      "Creating FAISS index with 520 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n",
      "✓ Processed 27 → 81 rows\n",
      "\n",
      "============================================================\n",
      "📊 Processing group 112/325\n",
      "\n",
      "🔄 Processing 27 rows with 2 acts\n",
      "🔨 Building new vector store for 2 acts...\n",
      "   Acts: ['id/ukpga/1994/33', 'id/ukpga/Eliz2/4-5/69']\n",
      "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\n",
      "Processing batch 0 to 100 (100 documents)\n",
      "Successfully embedded batch 0 to 100 of 285\n",
      "Processing batch 100 to 200 (100 documents)\n",
      "Successfully embedded batch 100 to 200 of 285\n",
      "Processing batch 200 to 285 (85 documents)\n",
      "Successfully embedded batch 200 to 285 of 285\n",
      "Creating FAISS index with 285 embeddings\n",
      "✗ Failed to cache vector store: cannot pickle '_thread.RLock' object\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 588\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎯 Final result saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;66;03m# Uncomment to run:\u001b[39;00m\n\u001b[0;32m--> 588\u001b[0m \u001b[43mrun_bulk_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 Bulk CSV Processor ready!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m💡 Features:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[56], line 579\u001b[0m, in \u001b[0;36mrun_bulk_processing\u001b[0;34m()\u001b[0m\n\u001b[1;32m    576\u001b[0m processor \u001b[38;5;241m=\u001b[39m BulkCachedProcessor()\n\u001b[1;32m    578\u001b[0m \u001b[38;5;66;03m# Process the bulk CSV\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m result_path \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_bulk_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcase_legislation_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Process in chunks\u001b[39;49;00m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Save progress every 5000 rows\u001b[39;49;00m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎯 Final result saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[56], line 464\u001b[0m, in \u001b[0;36mBulkCachedProcessor.process_bulk_csv\u001b[0;34m(self, input_csv_path, output_csv_path, case_legislation_dict, chunk_size, save_every)\u001b[0m\n\u001b[1;32m    461\u001b[0m legislation_acts \u001b[38;5;241m=\u001b[39m group_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlegislation_acts\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Process this group\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m processed_group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_legislation_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegislation_acts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# Collect results\u001b[39;00m\n\u001b[1;32m    467\u001b[0m all_processed_rows\u001b[38;5;241m.\u001b[39mappend(processed_group)\n",
      "Cell \u001b[0;32mIn[56], line 298\u001b[0m, in \u001b[0;36mBulkCachedProcessor.process_legislation_group\u001b[0;34m(self, group_df, legislation_acts)\u001b[0m\n\u001b[1;32m    295\u001b[0m     references \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Get relevant sections\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m relevant_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_sections_safe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparagraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegislation_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorstore\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# SAVE ORIGINAL ROW FIRST (before expansion)\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m relevant_docs:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;66;03m# Keep original row if no sections found\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[56], line 340\u001b[0m, in \u001b[0;36mBulkCachedProcessor._get_relevant_sections_safe\u001b[0;34m(self, query, legislation_list, references, vectorstore)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m legislation \u001b[38;5;129;01min\u001b[39;00m legislation_list:\n\u001b[0;32m--> 340\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlegislation_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegislation\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m results:\n\u001b[1;32m    344\u001b[0m             \u001b[38;5;66;03m# Take top result per legislation\u001b[39;00m\n\u001b[1;32m    345\u001b[0m             docs\u001b[38;5;241m.\u001b[39mextend([doc \u001b[38;5;28;01mfor\u001b[39;00m doc, score \u001b[38;5;129;01min\u001b[39;00m results[:\u001b[38;5;241m1\u001b[39m]])\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:515\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msimilarity_search_with_score\u001b[39m(\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    493\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    498\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    499\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m        L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score_by_vector(\n\u001b[1;32m    517\u001b[0m         embedding,\n\u001b[1;32m    518\u001b[0m         k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:266\u001b[0m, in \u001b[0;36mFAISS._embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_embed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function, Embeddings):\n\u001b[0;32m--> 266\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function(text)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/langchain_community/embeddings/openai.py:700\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    692\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;124;03m        Embedding for the text.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/langchain_community/embeddings/openai.py:671\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/langchain_community/embeddings/openai.py:497\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    495\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 497\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43membed_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    503\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/langchain_community/embeddings/openai.py:120\u001b[0m, in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/openai/resources/embeddings.py:125\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    119\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    120\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/openai/_base_client.py:996\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    993\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 996\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1002\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/ssl.py:1295\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1294\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/Odyssey/lib/python3.11/ssl.py:1168\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openAIHandler\n",
    "import pickle\n",
    "import util\n",
    "import json \n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import ast\n",
    "from langchain.docstore.document import Document\n",
    "import csv\n",
    "import gc\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class BulkCachedProcessor:\n",
    "    def __init__(self, legislation_dir=\"../data/final_test/case_csvs/legislation\", cache_dir=\"../data/final_test/case_csvs/vector_cache\", checkpoint_dir=\"../data/final_test/case_csvs/checkpoints\"):\n",
    "        self.legislation_dir = legislation_dir\n",
    "        self.cache_dir = cache_dir\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        # Create directories\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Caching system\n",
    "        self.current_vectorstore = None\n",
    "        self.current_legislation_set = None\n",
    "        \n",
    "        # Progress tracking\n",
    "        self.progress_file = os.path.join(checkpoint_dir, \"bulk_progress.json\")\n",
    "        self.load_progress()\n",
    "    \n",
    "    def load_progress(self):\n",
    "        \"\"\"Load processing progress from checkpoint\"\"\"\n",
    "        if os.path.exists(self.progress_file):\n",
    "            with open(self.progress_file, 'r') as f:\n",
    "                self.progress = json.load(f)\n",
    "        else:\n",
    "            self.progress = {\n",
    "                'processed_rows': 0,\n",
    "                'completed_groups': [],\n",
    "                'failed_groups': [],\n",
    "                'last_checkpoint': None\n",
    "            }\n",
    "    \n",
    "    def save_progress(self, processed_rows=None):\n",
    "        \"\"\"Save current progress to checkpoint\"\"\"\n",
    "        if processed_rows is not None:\n",
    "            self.progress['processed_rows'] = processed_rows\n",
    "        self.progress['last_checkpoint'] = time.time()\n",
    "        \n",
    "        with open(self.progress_file, 'w') as f:\n",
    "            json.dump(self.progress, f, indent=2)\n",
    "    \n",
    "    def get_legislation_hash(self, legislation_list):\n",
    "        \"\"\"Create a hash for a set of legislation acts\"\"\"\n",
    "        # Remove pd.isna() - just check for None and basic types\n",
    "        if legislation_list is None:\n",
    "            return \"no_legislation\"\n",
    "        \n",
    "        if not isinstance(legislation_list, (list, tuple)):\n",
    "            return \"invalid_legislation\"\n",
    "            \n",
    "        if len(legislation_list) == 0:\n",
    "            return \"empty_legislation\"\n",
    "            \n",
    "        sorted_legislation = sorted(legislation_list)\n",
    "        legislation_string = \"|\".join(sorted_legislation)\n",
    "        return hashlib.md5(legislation_string.encode()).hexdigest()\n",
    "\n",
    "    def get_cached_vectorstore_path(self, legislation_list):\n",
    "        \"\"\"Get the file path for cached vector store\"\"\"\n",
    "        legislation_hash = self.get_legislation_hash(legislation_list)\n",
    "        return os.path.join(self.cache_dir, f\"vectorstore_{legislation_hash}.pkl\")\n",
    "    \n",
    "    def save_vectorstore_to_cache(self, vectorstore, legislation_list):\n",
    "        \"\"\"Save vector store to disk cache\"\"\"\n",
    "        try:\n",
    "            cache_path = self.get_cached_vectorstore_path(legislation_list)\n",
    "            \n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(vectorstore, f)\n",
    "            \n",
    "            # Save metadata\n",
    "            metadata_path = cache_path.replace('.pkl', '_metadata.json')\n",
    "            metadata = {\n",
    "                'legislation_acts': sorted(legislation_list),\n",
    "                'created_at': time.time(),\n",
    "                'hash': self.get_legislation_hash(legislation_list)\n",
    "            }\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "            \n",
    "            print(f\"✓ Cached vector store for {len(legislation_list)} acts\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to cache vector store: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_vectorstore_from_cache(self, legislation_list):\n",
    "        \"\"\"Load vector store from disk cache\"\"\"\n",
    "        try:\n",
    "            cache_path = self.get_cached_vectorstore_path(legislation_list)\n",
    "            \n",
    "            if not os.path.exists(cache_path):\n",
    "                return None\n",
    "            \n",
    "            print(f\"📁 Loading cached vector store for {len(legislation_list)} acts...\")\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                vectorstore = pickle.load(f)\n",
    "            \n",
    "            print(\"✓ Loaded from cache successfully\")\n",
    "            return vectorstore\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to load cached vector store: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_or_create_vectorstore(self, legislation_list):\n",
    "        \"\"\"Get vector store from cache or create new one\"\"\"\n",
    "        legislation_set = set(legislation_list)\n",
    "        \n",
    "        # Check if we already have this exact combination loaded\n",
    "        if (self.current_vectorstore is not None and \n",
    "            self.current_legislation_set == legislation_set):\n",
    "            print(\"♻️  Reusing current vector store\")\n",
    "            return self.current_vectorstore\n",
    "        \n",
    "        # Try to load from cache\n",
    "        vectorstore = self.load_vectorstore_from_cache(legislation_list)\n",
    "        \n",
    "        if vectorstore is None:\n",
    "            # Create new vector store\n",
    "            print(f\"🔨 Building new vector store for {len(legislation_list)} acts...\")\n",
    "            print(f\"   Acts: {legislation_list}\")\n",
    "            vectorstore = openAIHandler.BuildVectorDB(self.legislation_dir, legislation_list)\n",
    "            \n",
    "            if vectorstore is not None:\n",
    "                # Cache the new vector store\n",
    "                self.save_vectorstore_to_cache(vectorstore, legislation_list)\n",
    "            else:\n",
    "                print(\"✗ Failed to build vector store\")\n",
    "                return None\n",
    "        \n",
    "        # Update current vectorstore\n",
    "        if self.current_vectorstore is not None:\n",
    "            del self.current_vectorstore\n",
    "            gc.collect()\n",
    "        \n",
    "        self.current_vectorstore = vectorstore\n",
    "        self.current_legislation_set = legislation_set\n",
    "        \n",
    "        return vectorstore\n",
    "    \n",
    "    def convert_case_uri_to_key(self,case_uri):\n",
    "        \"\"\"Convert case URI URL to key format\"\"\"\n",
    "        if not isinstance(case_uri, str):\n",
    "            return None\n",
    "        \n",
    "        # Remove base URL\n",
    "        base_url = 'https://caselaw.nationalarchives.gov.uk/'\n",
    "        if case_uri.startswith(base_url):\n",
    "            relative_path = case_uri[len(base_url):]\n",
    "            # Replace slashes with underscores\n",
    "            key = relative_path.replace('/', '_')\n",
    "            return key\n",
    "        return None\n",
    "    # And add this method to save before expansion:\n",
    "    def save_before_expansion(self, df, output_path):\n",
    "        \"\"\"Save dataframe before section expansion\"\"\"\n",
    "        print(f\"💾 Saving before expansion: {len(df)} rows\")\n",
    "        \n",
    "        # Clean text columns\n",
    "        df_clean = df.copy()\n",
    "        text_columns = ['paragraphs']\n",
    "        for col in text_columns:\n",
    "            if col in df_clean.columns:\n",
    "                df_clean[col] = df_clean[col].astype(str).str.replace('\\n', ' ', regex=False)\n",
    "                df_clean[col] = df_clean[col].str.replace('\\r', ' ', regex=False)\n",
    "                df_clean[col] = df_clean[col].str.replace('\\t', ' ', regex=False)\n",
    "                df_clean[col] = df_clean[col].str.replace(r'\\s+', ' ', regex=True)\n",
    "                df_clean[col] = df_clean[col].str.strip()\n",
    "        \n",
    "        # Remove temporary columns\n",
    "        columns_to_drop = ['legislation_acts', 'legislation_hash', 'case_key']\n",
    "        df_clean = df_clean.drop(columns=[col for col in columns_to_drop if col in df_clean.columns])\n",
    "        \n",
    "        # Save\n",
    "        df_clean.to_csv(output_path, index=False, \n",
    "                    quoting=csv.QUOTE_ALL,\n",
    "                    escapechar='\\\\', \n",
    "                    doublequote=True, \n",
    "                    encoding='utf-8')\n",
    "        \n",
    "        print(f\"✓ Saved pre-expansion file: {output_path}\")\n",
    "    def analyze_legislation_patterns(self, df, case_legislation_dict):\n",
    "        \"\"\"Analyze patterns in the data to optimize processing\"\"\"\n",
    "        print(\"🔍 Analyzing legislation patterns...\")\n",
    "        \n",
    "        # Convert case URIs to keys for mapping\n",
    "        df['case_key'] = df['case_uri'].apply(self.convert_case_uri_to_key)\n",
    "        \n",
    "        # Add legislation info to dataframe using converted keys\n",
    "        df['legislation_acts'] = df['case_key'].map(case_legislation_dict)\n",
    "        \n",
    "        # Check for missing mappings\n",
    "        missing_cases = df[df['legislation_acts'].isna()]\n",
    "        if len(missing_cases) > 0:\n",
    "            print(f\"⚠️  Warning: {len(missing_cases)} case keys not found in legislation dict\")\n",
    "            print(f\"   Sample missing keys: {missing_cases['case_key'].head(3).tolist()}\")\n",
    "            \n",
    "            # Filter out rows without legislation mapping\n",
    "            original_len = len(df)\n",
    "            df = df[df['legislation_acts'].notna()].copy()\n",
    "            print(f\"   Filtered from {original_len} to {len(df)} rows\")\n",
    "        \n",
    "        # # Create legislation hash\n",
    "        # df['legislation_hash'] = df['legislation_acts'].apply(\n",
    "        #     lambda x: self.get_legislation_hash(x) if pd.notna(x) and x is not None else \"no_legislation\"\n",
    "        # )\n",
    "        # With this:\n",
    "        df['legislation_hash'] = df['legislation_acts'].apply(\n",
    "            lambda x: self.get_legislation_hash(x) if x is not None else \"no_legislation\"\n",
    "        )\n",
    "        # Count patterns\n",
    "        pattern_counts = df['legislation_hash'].value_counts()\n",
    "        unique_patterns = len(pattern_counts)\n",
    "        \n",
    "        print(f\"📊 Pattern Analysis:\")\n",
    "        print(f\"   - Total rows with legislation: {len(df)}\")\n",
    "        print(f\"   - Unique legislation combinations: {unique_patterns}\")\n",
    "        if len(pattern_counts) > 0:\n",
    "            print(f\"   - Largest group: {pattern_counts.iloc[0]} rows\")\n",
    "        print(f\"   - This means {unique_patterns} vector stores max!\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def group_dataframe_by_legislation(self, df):\n",
    "        \"\"\"Group dataframe rows by legislation combination for efficient processing\"\"\"\n",
    "        print(\"📝 Grouping rows by legislation...\")\n",
    "        \n",
    "        # Remove rows without valid legislation\n",
    "        df_valid = df[~df['legislation_hash'].isin(['no_legislation', 'invalid_legislation', 'empty_legislation'])].copy()\n",
    "        \n",
    "        if len(df_valid) == 0:\n",
    "            print(\"❌ No valid legislation found in any rows!\")\n",
    "            return [], None\n",
    "        \n",
    "        print(f\"📊 Processing {len(df_valid)} rows with valid legislation (filtered from {len(df)})\")\n",
    "        \n",
    "        # Group by legislation hash\n",
    "        grouped = df_valid.groupby('legislation_hash')\n",
    "        \n",
    "        # Sort groups by size (largest first for efficiency)\n",
    "        group_sizes = [(name, len(group)) for name, group in grouped]\n",
    "        group_sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"🎯 Processing order optimized:\")\n",
    "        for i, (hash_val, size) in enumerate(group_sizes[:5]):  # Show top 5\n",
    "            sample_acts = df_valid[df_valid['legislation_hash'] == hash_val]['legislation_acts'].iloc[0]\n",
    "            print(f\"   {i+1}. {size} rows → {len(sample_acts)} acts\")\n",
    "        \n",
    "        return group_sizes, grouped\n",
    "    \n",
    "    def process_legislation_group(self, group_df, legislation_acts):\n",
    "        \"\"\"Process all rows for a specific legislation combination\"\"\"\n",
    "        print(f\"\\n🔄 Processing {len(group_df)} rows with {len(legislation_acts)} acts\")\n",
    "        \n",
    "        # Get vector store for this legislation combination\n",
    "        vectorstore = self.get_or_create_vectorstore(legislation_acts)\n",
    "        if vectorstore is None:\n",
    "            print(\"✗ Failed to get vector store\")\n",
    "            return group_df  # Return original dataframe\n",
    "        \n",
    "        # Initialize new columns if they don't exist\n",
    "        if 'section_id' not in group_df.columns:\n",
    "            group_df = group_df.copy()\n",
    "            group_df['section_id'] = '0'\n",
    "            group_df['section_text'] = ''\n",
    "        \n",
    "        # Process each row in the group\n",
    "        processed_rows = []\n",
    "        \n",
    "        for idx, row in group_df.iterrows():\n",
    "            try:\n",
    "                paragraph = row['paragraphs']\n",
    "                references = row.get('references', [])\n",
    "                \n",
    "                if isinstance(references, str) and references:\n",
    "                    references = ast.literal_eval(references)\n",
    "                elif not references:\n",
    "                    references = []\n",
    "                \n",
    "                # Get relevant sections\n",
    "                relevant_docs = self._get_relevant_sections_safe(\n",
    "                    paragraph, legislation_acts, references, vectorstore\n",
    "                )\n",
    "                \n",
    "                # SAVE ORIGINAL ROW FIRST (before expansion)\n",
    "                if not relevant_docs:\n",
    "                    # Keep original row if no sections found\n",
    "                    processed_rows.append(row)\n",
    "                else:\n",
    "                    # Create multiple rows if multiple relevant sections found\n",
    "                    for relevant_doc in relevant_docs:\n",
    "                        new_row = row.copy()\n",
    "                        new_row['section_id'] = relevant_doc.metadata.get(\"id\", \"unknown\")\n",
    "                        new_row['section_text'] = str(relevant_doc.page_content)\n",
    "                        processed_rows.append(new_row)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Error processing row {idx}: {e}\")\n",
    "                # Keep original row on error\n",
    "                processed_rows.append(row)\n",
    "                continue\n",
    "        \n",
    "        # Create result dataframe\n",
    "        result_df = pd.DataFrame(processed_rows)\n",
    "        print(f\"✓ Processed {len(group_df)} → {len(result_df)} rows\")\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def _get_relevant_sections_safe(self, query, legislation_list, references, vectorstore):\n",
    "        \"\"\"Get relevant sections using provided vectorstore\"\"\"\n",
    "        docs = []\n",
    "        \n",
    "        try:\n",
    "            # Get explicit sections first\n",
    "            if references:\n",
    "                docs_explicit = self._get_explicit_sections(references)\n",
    "                if docs_explicit:\n",
    "                    docs.extend(docs_explicit)\n",
    "            \n",
    "            # Get from vectorstore\n",
    "            try:\n",
    "                for legislation in legislation_list:\n",
    "                    results = vectorstore.similarity_search_with_score(\n",
    "                        query=query, k=2, filter={\"legislation_id\": legislation}\n",
    "                    )\n",
    "                    if results:\n",
    "                        # Take top result per legislation\n",
    "                        docs.extend([doc for doc, score in results[:1]])\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Vector search failed: {e}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error getting relevant sections: {e}\")\n",
    "        \n",
    "        return docs[:3]  # Limit to top 3 total\n",
    "    \n",
    "    def _get_explicit_sections(self, references):\n",
    "        \"\"\"Get explicitly mentioned sections from references\"\"\"\n",
    "        docs = []\n",
    "        \n",
    "        for ref in references:\n",
    "            if not isinstance(ref, dict) or 'href' not in ref:\n",
    "                continue\n",
    "                \n",
    "            url = ref['href']\n",
    "            keywords = ['section', 'schedule', 'regulation', 'chapter', 'article']\n",
    "            \n",
    "            for keyword in keywords:\n",
    "                if keyword not in url.lower():\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    if 'legislation.gov.uk' in url:\n",
    "                        legislation_parts = url.split('legislation.gov.uk/', 1)[-1].split('/')\n",
    "                        \n",
    "                        # Extract act and section parts\n",
    "                        act_parts = []\n",
    "                        section_parts = []\n",
    "                        found_keyword = False\n",
    "                        \n",
    "                        for part in legislation_parts:\n",
    "                            if keyword in part.lower():\n",
    "                                found_keyword = True\n",
    "                                section_parts.append(part)\n",
    "                            elif found_keyword:\n",
    "                                section_parts.append(part)\n",
    "                            else:\n",
    "                                act_parts.append(part)\n",
    "                        \n",
    "                        if not found_keyword:\n",
    "                            continue\n",
    "                        \n",
    "                        # Construct file path\n",
    "                        directory_path = os.path.join(self.legislation_dir, *act_parts)\n",
    "                        section_file_path = '-'.join(section_parts)\n",
    "                        file_name = f\"{section_file_path}.txt\"\n",
    "                        file_path = os.path.join(directory_path, file_name)\n",
    "                        \n",
    "                        # Read file if exists\n",
    "                        if os.path.exists(file_path):\n",
    "                            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                                content = file.read()\n",
    "                            \n",
    "                            doc = Document(content)\n",
    "                            doc.metadata['id'] = f\"{'_'.join(act_parts)}_{keyword}_{section_parts[-1]}\"\n",
    "                            doc.metadata['legislation_id'] = url\n",
    "                            docs.append(doc)\n",
    "                            break\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️  Could not process reference {url}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        return docs\n",
    "    \n",
    "    def process_bulk_csv(self, input_csv_path, output_csv_path, case_legislation_dict, \n",
    "                        chunk_size=1000, save_every=5000):\n",
    "        \"\"\"Process the entire CSV efficiently with smart caching\"\"\"\n",
    "        \n",
    "        print(f\"🚀 Starting bulk processing...\")\n",
    "        print(f\"📁 Input: {input_csv_path}\")\n",
    "        print(f\"💾 Output: {output_csv_path}\")\n",
    "        \n",
    "        # Load data\n",
    "        print(\"📚 Loading CSV data...\")\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "        \n",
    "        # Filter to only rows that need processing (have law applied)\n",
    "        if 'final_annotation' in df.columns:\n",
    "            original_len = len(df)\n",
    "            df = df[df['final_annotation'] == True].copy()\n",
    "            print(f\"🎯 Filtered to {len(df)} rows with law applied (from {original_len} total)\")\n",
    "        \n",
    "        # Skip already processed rows if resuming\n",
    "        if self.progress['processed_rows'] > 0:\n",
    "            df = df.iloc[self.progress['processed_rows']:].copy()\n",
    "            print(f\"📄 Resuming from row {self.progress['processed_rows']}\")\n",
    "        \n",
    "        # Analyze patterns\n",
    "        df = self.analyze_legislation_patterns(df, case_legislation_dict)\n",
    "        \n",
    "        # SAVE BEFORE EXPANSION\n",
    "        pre_expansion_path = output_csv_path.replace('.csv', '_before_expansion.csv')\n",
    "        self.save_before_expansion(df, pre_expansion_path)\n",
    "\n",
    "        # Group by legislation for efficient processing\n",
    "        group_sizes, grouped = self.group_dataframe_by_legislation(df)\n",
    "        \n",
    "        if not group_sizes:\n",
    "            print(\"❌ No valid groups found. Check your case_legislation_dict!\")\n",
    "            return output_csv_path\n",
    "        \n",
    "        # Process each group\n",
    "        all_processed_rows = []\n",
    "        total_processed = self.progress['processed_rows']\n",
    "        \n",
    "        for i, (legislation_hash, group_size) in enumerate(group_sizes):\n",
    "            try:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"📊 Processing group {i+1}/{len(group_sizes)}\")\n",
    "                \n",
    "                # Get group data\n",
    "                group_df = grouped.get_group(legislation_hash)\n",
    "                legislation_acts = group_df['legislation_acts'].iloc[0]\n",
    "                \n",
    "                # Process this group\n",
    "                processed_group = self.process_legislation_group(group_df, legislation_acts)\n",
    "                \n",
    "                # Collect results\n",
    "                all_processed_rows.append(processed_group)\n",
    "                total_processed += len(group_df)\n",
    "                \n",
    "                # Save progress periodically\n",
    "                if total_processed % save_every == 0 or i == len(group_sizes) - 1:\n",
    "                    print(f\"💾 Saving intermediate results...\")\n",
    "                    \n",
    "                    # Combine all processed data so far\n",
    "                    combined_df = pd.concat(all_processed_rows, ignore_index=True)\n",
    "                    \n",
    "                    # Clean text columns - remove newlines and extra whitespace\n",
    "                    text_columns = ['paragraphs', 'section_text']\n",
    "                    for col in text_columns:\n",
    "                        if col in combined_df.columns:\n",
    "                            combined_df[col] = combined_df[col].astype(str).str.replace('\\n', ' ', regex=False)\n",
    "                            combined_df[col] = combined_df[col].str.replace('\\r', ' ', regex=False)\n",
    "                            combined_df[col] = combined_df[col].str.replace('\\t', ' ', regex=False)\n",
    "                            combined_df[col] = combined_df[col].str.replace(r'\\s+', ' ', regex=True)\n",
    "                            combined_df[col] = combined_df[col].str.strip()\n",
    "                    \n",
    "                    # Remove temporary columns\n",
    "                    columns_to_drop = ['legislation_acts', 'legislation_hash', 'case_key']\n",
    "                    combined_df = combined_df.drop(columns=[col for col in columns_to_drop if col in combined_df.columns])\n",
    "                    \n",
    "                    # Save to output file with proper quoting\n",
    "                    combined_df.to_csv(output_csv_path, index=False, \n",
    "                                    quoting=csv.QUOTE_ALL,\n",
    "                                    escapechar='\\\\', \n",
    "                                    doublequote=True, \n",
    "                                    encoding='utf-8')\n",
    "                    \n",
    "                    # Update progress\n",
    "                    self.save_progress(total_processed)\n",
    "                    print(f\"✓ Saved {len(combined_df)} rows to {output_csv_path}\")\n",
    "                # Memory cleanup\n",
    "                if i % 3 == 0:\n",
    "                    gc.collect()\n",
    "                \n",
    "                # Mark group as completed\n",
    "                self.progress['completed_groups'].append(legislation_hash)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error processing group {i+1}: {e}\")\n",
    "                self.progress['failed_groups'].append({\n",
    "                    'group': legislation_hash,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "                continue\n",
    "        \n",
    "        # Final save and summary\n",
    "        # Final save and summary\n",
    "        if all_processed_rows:\n",
    "            print(f\"\\n💾 Final save...\")\n",
    "            combined_df = pd.concat(all_processed_rows, ignore_index=True)\n",
    "            \n",
    "            # Clean text columns\n",
    "            text_columns = ['paragraphs', 'section_text']\n",
    "            for col in text_columns:\n",
    "                if col in combined_df.columns:\n",
    "                    combined_df[col] = combined_df[col].astype(str).str.replace('\\n', ' ', regex=False)\n",
    "                    combined_df[col] = combined_df[col].str.replace('\\r', ' ', regex=False)\n",
    "                    combined_df[col] = combined_df[col].str.replace('\\t', ' ', regex=False)\n",
    "                    combined_df[col] = combined_df[col].str.replace(r'\\s+', ' ', regex=True)\n",
    "                    combined_df[col] = combined_df[col].str.strip()\n",
    "            \n",
    "            # Clean up columns\n",
    "            columns_to_drop = ['legislation_acts', 'legislation_hash', 'case_key']\n",
    "            combined_df = combined_df.drop(columns=[col for col in columns_to_drop if col in combined_df.columns])\n",
    "            \n",
    "            combined_df.to_csv(output_csv_path, index=False, \n",
    "                            quoting=csv.QUOTE_ALL,\n",
    "                            escapechar='\\\\', \n",
    "                            doublequote=True, \n",
    "                            encoding='utf-8')\n",
    "        self.print_summary(len(df), len(combined_df) if all_processed_rows else 0)\n",
    "        \n",
    "        return output_csv_path\n",
    "    \n",
    "    def print_summary(self, input_rows, output_rows):\n",
    "        \"\"\"Print processing summary\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"🎉 BULK PROCESSING COMPLETE\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"📥 Input rows processed: {input_rows:,}\")\n",
    "        print(f\"📤 Output rows generated: {output_rows:,}\")\n",
    "        print(f\"📊 Expansion ratio: {output_rows/input_rows:.2f}x\" if input_rows > 0 else \"\")\n",
    "        print(f\"✅ Completed groups: {len(self.progress['completed_groups'])}\")\n",
    "        print(f\"❌ Failed groups: {len(self.progress['failed_groups'])}\")\n",
    "        \n",
    "        # Cache statistics\n",
    "        cache_files = [f for f in os.listdir(self.cache_dir) if f.endswith('.pkl')]\n",
    "        print(f\"💾 Vector stores cached: {len(cache_files)}\")\n",
    "\n",
    "# Usage function\n",
    "def run_bulk_processing():\n",
    "    \"\"\"Main function to run bulk CSV processing\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    case_legislation_pickle = \"../data/final_test/case_csvs/cleaned_case_legislation_map.pkl\"  # Your pickle file\n",
    "    input_csv = '../data/final_test/positve_cases.csv'  # Your big CSV\n",
    "    output_csv = \"../data/final_test/processed_with_sections_all.csv\"  # Output CSV\n",
    "    legislation_dir = \"../data/final_test/case_csvs/legislation\"  # Your legislation directory\n",
    "    \n",
    "    # Load case-legislation mapping\n",
    "    print(\"📚 Loading case-legislation mapping...\")\n",
    "    with open(case_legislation_pickle, 'rb') as f:\n",
    "        case_legislation_dict = pickle.load(f)\n",
    "    \n",
    "    # Create processor (using your existing legislation folder)\n",
    "    processor = BulkCachedProcessor()\n",
    "    \n",
    "    # Process the bulk CSV\n",
    "    result_path = processor.process_bulk_csv(\n",
    "        input_csv, output_csv, case_legislation_dict,\n",
    "        chunk_size=1000,  # Process in chunks\n",
    "        save_every=5000   # Save progress every 5000 rows\n",
    "    )\n",
    "    \n",
    "    print(f\"🎯 Final result saved to: {result_path}\")\n",
    "\n",
    "# Uncomment to run:\n",
    "run_bulk_processing()\n",
    "\n",
    "print(\"📊 Bulk CSV Processor ready!\")\n",
    "print(\"💡 Features:\")\n",
    "print(\"   ✅ Processes one big CSV file\")\n",
    "print(\"   ✅ Groups rows by legislation for efficiency\") \n",
    "print(\"   ✅ Caches vector stores - no repeated OpenAI calls\")\n",
    "print(\"   ✅ Saves progress every 5000 rows\")\n",
    "print(\"   ✅ Can resume from interruption\")\n",
    "print(\"   ✅ Memory efficient with cleanup\")\n",
    "print(\"   ✅ Expands rows when multiple sections found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Odyssey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
