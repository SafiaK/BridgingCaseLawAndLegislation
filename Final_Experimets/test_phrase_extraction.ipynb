{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Phrase Extraction with Fine-tuned Model\n",
    "\n",
    "This notebook tests the new phrase extraction function with section text and section_id tracking using only 5 rows and your fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import pandas as pd\n",
    "from make_batch_jsonl_law_application import create_batch_jsonl_for_phrase_extraction\n",
    "from phrase_validator import PhraseValidator\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src folder to Python path\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Load environment variables from src/.env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(os.path.dirname(os.getcwd()), 'src', '.env'))\n",
    "import openAIHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17470\n"
     ]
    }
   ],
   "source": [
    "# 1. Load data with only 5 rows for testing\n",
    "csv_path = '../data/final_test/final/withsectionpositvefinal_cleaned.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_text = df.paragraphs[1]\n",
    "section_text = df.section_text[1]\n",
    "para_id = df.para_id[1]\n",
    "section_id = df.section_id[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validation Results:\n",
      "Total paragraphs: 7\n",
      "\n",
      "üìà Validation Summary:\n",
      "Total paragraphs: 2\n",
      "Valid paragraphs: 0\n",
      "Invalid paragraphs: 2\n",
      "Success rate: 0.0%\n",
      "\n",
      "‚ùå Failure reasons:\n",
      "  No extracted phrases found: 4\n",
      "  No case law excerpt extracted: 3\n"
     ]
    }
   ],
   "source": [
    "# Run validation on the results\n",
    "validation_results = PhraseValidator.validate_extractions_df(merged_df)\n",
    "\n",
    "print(\"üîç Validation Results:\")\n",
    "print(f\"Total paragraphs: {len(validation_results)}\")\n",
    "\n",
    "# Show validation summary\n",
    "summary = PhraseValidator.get_validation_summary(validation_results)\n",
    "print(f\"\\nüìà Validation Summary:\")\n",
    "print(f\"Total paragraphs: {summary['total_paragraphs']}\")\n",
    "print(f\"Valid paragraphs: {summary['valid_paragraphs']}\")\n",
    "print(f\"Invalid paragraphs: {summary['invalid_paragraphs']}\")\n",
    "print(f\"Success rate: {summary['success_rate']:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚ùå Failure reasons:\")\n",
    "for reason, count in summary['failure_reasons'].items():\n",
    "    print(f\"  {reason}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openAIHandler\n",
    "extraction_chain = openAIHandler.getPhraseExtractionChain()\n",
    "\n",
    "interpretations = openAIHandler.getInterPretations(section_text, para_text, para_id, section_id, extraction_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\n    {\\n        \"caselaw_term\": \"claimant ticked the relevant box to state he had a disability\",\\n        \"legislation_term\": \"interested disabled person has a disability and is likely to be placed at the disadvantage referred to in the first, second or third requirement\",\\n        \"key_phrases\": [\"interested disabled person has a disability\"],\\n        \"reasoning\": \"Case law identifies the claimant as an interested disabled person, aligning with the legislative definition of disability.\",\\n        \"confidence\": \"High\"\\n    },\\n    {\\n        \"caselaw_term\": \"no apparent disadvantage or need for adjustments\",\\n        \"legislation_term\": \"A is not subject to a duty to make reasonable adjustments if A does not know, and could not reasonably be expected to know\",\\n        \"key_phrases\": [\"duty to make reasonable adjustments\"],\\n        \"reasoning\": \"Case law indicates that the claimant did not experience a disadvantage, which relates to the lack of duty for reasonable adjustments under the legislation.\",\\n        \"confidence\": \"High\"\\n    }\\n]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import ast \n",
    "interpretations_list = ast.literal_eval(interpretations)\n",
    "for interpretation in interpretations_list:\n",
    "    case_law_term = interpretation['caselaw_term']\n",
    "    legislation_term = interpretation['legislation_term']\n",
    "    confidence = interpretation['confidence']\n",
    "    print(case_law_term in para_text)\n",
    "    print(legislation_term in section_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt from prompt_file:\n",
      "\n",
      "You are a legal expert tasked with extracting exact phrases from case law paragraphs that correspond to specific legislation sections.\n",
      "\n",
      "Your task is to:\n",
      "1. Analyze the case law paragraph\n",
      "2. Compare it with the provided legislation section text\n",
      "3. Extract exact phrases that show how the case law applies or interprets the legislation\n",
      "4. Provide confidence levels for each extraction\n",
      "5. Include the section_id for tracking purposes\n",
      "\n",
      "Return your response in this JSON format:\n",
      "{\n",
      "    \"para_id\": \"paragraph_identifier\",\n",
      "    \"section_id\": \"legislation_section_identifier\",\n",
      "    \"extracted_phrases\": [\n",
      "        {\n",
      "            \"case_law_term\": \"exact phrase from case law\",\n",
      "            \"legislation_term\": \"corresponding phrase from legislation\",\n",
      "            \"confidence\": \"High/Medium/Low\",\n",
      "            \"reasoning\": \"explanation of the extraction\"\n",
      "        }\n",
      "    ],\n",
      "    \"reason\": \"overall reasoning for the extractions\"\n",
      "}\n",
      "\n",
      "Focus on finding exact or near-exact matches between the case law and legislation text.\n",
      "Always include the section_id in your response for proper tracking. \n"
     ]
    }
   ],
   "source": [
    "# Print the prompt from the prompt_file\n",
    "try:\n",
    "    with open(prompt_file, 'r', encoding='utf-8') as f:\n",
    "        prompt_content = f.read()\n",
    "    print(\"Prompt from prompt_file:\\n\")\n",
    "    print(prompt_content)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not read prompt file '{prompt_file}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected provider: OpenAI\n",
      "Limits: 50,000 requests, 100MB per file\n",
      "Building phrase extraction JSONL requests...\n",
      "\n",
      "Analysis:\n",
      "  Total requests: 5\n",
      "  Estimated size: 0.0 MB\n",
      "  Splits needed: 1 (file within limits)\n",
      "\n",
      "‚úÖ Creating single file: ../data/final_test/test_phrase_extraction.jsonl\n",
      "   Actual size: 0.0 MB\n",
      "   Requests: 5\n",
      "\n",
      "üìã Summary:\n",
      "   Provider: OpenAI\n",
      "   Model: ft:gpt-4o-mini-2024-07-18:swansea-university::B3pbF9HD\n",
      "   Files created: 1\n",
      "   Total requests: 5\n",
      "   Total size: 0.0 MB\n",
      "\n",
      "üìÅ Output files:\n",
      "   1. ../data/final_test/test_phrase_extraction.jsonl\n",
      "‚úÖ Successfully created JSONL files:\n",
      "   - ../data/final_test/test_phrase_extraction.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 6. Create the batch JSONL for phrase extraction\n",
    "try:\n",
    "    created_files = create_batch_jsonl_for_phrase_extraction(\n",
    "        model_name=model_name,\n",
    "        prompt_file=prompt_file,\n",
    "        examples_file=examples_file,\n",
    "        df=test_df,\n",
    "        output_path=output_path\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Successfully created JSONL files:\")\n",
    "    for file_path in created_files:\n",
    "        print(f\"   - {file_path}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating JSONL files: {e}\")\n",
    "    print(\"\\nThis might be because the prompt or examples files don't exist yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch job submitted successfully!\n",
      "Batch ID: batch_687f851578108190a8fb55deaa561831\n",
      "Status: validating\n",
      "Input file ID: file-Qa5pJtPZPkKYAhazM9XGyq\n",
      "Completion window: 24h\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import openAIHandler\n",
    "\n",
    "# Execute the JSONL file using batch API\n",
    "input_file = '../data/final_test/test_phrase_extraction.jsonl'\n",
    "batch_job = openAIHandler.get_batch_job(input_file)\n",
    "\n",
    "print(\"‚úÖ Batch job submitted successfully!\")\n",
    "print(f\"Batch ID: {batch_job.id}\")\n",
    "print(f\"Status: {batch_job.status}\")\n",
    "print(f\"Input file ID: {batch_job.input_file_id}\")\n",
    "print(f\"Completion window: {batch_job.completion_window}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status: completed\n"
     ]
    }
   ],
   "source": [
    "# You can check the status later with:\n",
    "import openai\n",
    "client = openai.OpenAI()\n",
    "batch_status = client.batches.retrieve(batch_job.id)\n",
    "print(f\"Current status: {batch_status.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_687f851578108190a8fb55deaa561831\n"
     ]
    }
   ],
   "source": [
    "print(batch_job.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent batches:\n",
      "ID: batch_687f851578108190a8fb55deaa561831, Status: completed\n",
      "  ‚úÖ Completed batch found: batch_687f851578108190a8fb55deaa561831\n",
      "  Output file ID: file-7fCcDKU37Zi5xDmNhcPayY\n",
      "  üì• Results downloaded to: ../data/final_test/batch_output_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('src/.env')\n",
    "load_dotenv()\n",
    "\n",
    "# Set up client\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# List recent batches to find the completed one\n",
    "batches = client.batches.list(limit=5)\n",
    "print('Recent batches:')\n",
    "for batch in batches.data:\n",
    "    print(f'ID: {batch.id}, Status: {batch.status}')\n",
    "    if batch.status == 'completed':\n",
    "        print(f'  ‚úÖ Completed batch found: {batch.id}')\n",
    "        print(f'  Output file ID: {batch.output_file_id}')\n",
    "        \n",
    "        # Download the results\n",
    "        result = client.files.content(batch.output_file_id)\n",
    "        output_path = '../data/final_test/batch_output_results.jsonl'\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(result.read())\n",
    "        \n",
    "        print(f'  üì• Results downloaded to: {output_path}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parsed 5 results\n",
      "\n",
      "üìã Sample result:\n",
      "{\n",
      "  \"para_id\": \"ewhc_ch_2009_1229#para_35\",\n",
      "  \"section_id\": \"id/ukpga/1998/42_section-8\",\n",
      "  \"extracted_phrases\": [\n",
      "    {\n",
      "      \"case_law_term\": \"public interest generally requires the precise facts relevant to the decision to be a matter of public record\",\n",
      "      \"legislation_term\": \"any act (or proposed act) of a public authority which the court finds is (or would be) unlawful, it may grant such relief or remedy\",\n",
      "      \"confidence\": \"Medium\",\n",
      "      \"reasoning\": \"The case law reflects the complexity and sensitivity of public authority decisions, aligning with the legislation's framework for judicial remedies concerning public authorities.\"\n",
      "    }\n",
      "  ],\n",
      "  \"reason\": \"The case law emphasizes the public interest and the importance of public record in tax disputes, which relates to judicial remedies against public authority actions deemed unlawful.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from phrase_validator import PhraseValidator\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Parse the JSONL results\n",
    "results = []\n",
    "with open('../data/final_test/batch_output_results.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        # Extract the response\n",
    "        try:\n",
    "            response_content = data['response']['body']['choices'][0]['message']['content']\n",
    "            # Parse the JSON response\n",
    "            extracted_data = json.loads(response_content)\n",
    "            results.append(extracted_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"‚úÖ Parsed {len(results)} results\")\n",
    "\n",
    "# Show sample result\n",
    "if results:\n",
    "    print(\"\\nüìã Sample result:\")\n",
    "    print(json.dumps(results[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validation Results:\n",
      "Total paragraphs: 7\n",
      "\n",
      "üìà Validation Summary:\n",
      "Total paragraphs: 2\n",
      "Valid paragraphs: 0\n",
      "Invalid paragraphs: 2\n",
      "Success rate: 0.0%\n",
      "\n",
      "‚ùå Failure reasons:\n",
      "  No extracted phrases found: 4\n",
      "  No case law excerpt extracted: 3\n"
     ]
    }
   ],
   "source": [
    "# Run validation on the results\n",
    "validation_results = PhraseValidator.validate_extractions_df(merged_df)\n",
    "\n",
    "print(\"üîç Validation Results:\")\n",
    "print(f\"Total paragraphs: {len(validation_results)}\")\n",
    "\n",
    "# Show validation summary\n",
    "summary = PhraseValidator.get_validation_summary(validation_results)\n",
    "print(f\"\\nüìà Validation Summary:\")\n",
    "print(f\"Total paragraphs: {summary['total_paragraphs']}\")\n",
    "print(f\"Valid paragraphs: {summary['valid_paragraphs']}\")\n",
    "print(f\"Invalid paragraphs: {summary['invalid_paragraphs']}\")\n",
    "print(f\"Success rate: {summary['success_rate']:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚ùå Failure reasons:\")\n",
    "for reason, count in summary['failure_reasons'].items():\n",
    "    print(f\"  {reason}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run validation on the results\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m validation_results = PhraseValidator.validate_extractions_df(\u001b[43mmerged_df\u001b[49m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîç Validation Results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal paragraphs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(validation_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Run validation on the results\n",
    "validation_results = PhraseValidator.validate_extractions_df(merged_df)\n",
    "\n",
    "print(\"üîç Validation Results:\")\n",
    "print(f\"Total paragraphs: {len(validation_results)}\")\n",
    "\n",
    "# Show validation summary\n",
    "summary = PhraseValidator.get_validation_summary(validation_results)\n",
    "print(f\"\\nüìà Validation Summary:\")\n",
    "print(f\"Total paragraphs: {summary['total_paragraphs']}\")\n",
    "print(f\"Valid paragraphs: {summary['valid_paragraphs']}\")\n",
    "print(f\"Invalid paragraphs: {summary['invalid_paragraphs']}\")\n",
    "print(f\"Success rate: {summary['success_rate']:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚ùå Failure reasons:\")\n",
    "for reason, count in summary['failure_reasons'].items():\n",
    "    print(f\"  {reason}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validation Results:\n",
      "Total paragraphs: 7\n",
      "\n",
      "üìà Validation Summary:\n",
      "Total paragraphs: 2\n",
      "Valid paragraphs: 0\n",
      "Invalid paragraphs: 2\n",
      "Success rate: 0.0%\n",
      "\n",
      "‚ùå Failure reasons:\n",
      "  No extracted phrases found: 4\n",
      "  No case law excerpt extracted: 3\n"
     ]
    }
   ],
   "source": [
    "# Run validation on the results\n",
    "validation_results = PhraseValidator.validate_extractions_df(merged_df)\n",
    "\n",
    "print(\"üîç Validation Results:\")\n",
    "print(f\"Total paragraphs: {len(validation_results)}\")\n",
    "\n",
    "# Show validation summary\n",
    "summary = PhraseValidator.get_validation_summary(validation_results)\n",
    "print(f\"\\nüìà Validation Summary:\")\n",
    "print(f\"Total paragraphs: {summary['total_paragraphs']}\")\n",
    "print(f\"Valid paragraphs: {summary['valid_paragraphs']}\")\n",
    "print(f\"Invalid paragraphs: {summary['invalid_paragraphs']}\")\n",
    "print(f\"Success rate: {summary['success_rate']:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚ùå Failure reasons:\")\n",
    "for reason, count in summary['failure_reasons'].items():\n",
    "    print(f\"  {reason}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Created DataFrame with 5 rows\n",
      "Columns: ['para_id', 'section_id', 'extracted_phrases']\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame format for validation\n",
    "validation_data = []\n",
    "\n",
    "for result in results:\n",
    "    para_id = result.get('para_id', '')\n",
    "    section_id = result.get('section_id', '')\n",
    "    extracted_phrases = result.get('extracted_phrases', [])\n",
    "    \n",
    "    # Convert extracted_phrases back to string for validator\n",
    "    validation_data.append({\n",
    "        'para_id': para_id,\n",
    "        'section_id': section_id,\n",
    "        'extracted_phrases': json.dumps(extracted_phrases)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(validation_data)\n",
    "print(f\"üìä Created DataFrame with {len(results_df)} rows\")\n",
    "print(f\"Columns: {list(results_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing phrase validator...\n",
      "Original: 'The court applied the principle of natural justice.'\n",
      "Cleaned: 'the court applied the principle of natural justice'\n",
      "\n",
      "Validation result: True - Both terms found in respective texts\n"
     ]
    }
   ],
   "source": [
    "# 7. Test the validator function\n",
    "print(\"Testing phrase validator...\")\n",
    "\n",
    "# Create a sample validation test\n",
    "sample_text = \"The court applied the principle of natural justice.\"\n",
    "sample_section = \"Natural justice requires fair procedures.\"\n",
    "sample_case_law_term = \"principle of natural justice\"\n",
    "sample_legislation_term = \"natural justice\"\n",
    "\n",
    "# Test text cleaning\n",
    "cleaned_text = PhraseValidator.clean_text(sample_text)\n",
    "print(f\"Original: '{sample_text}'\")\n",
    "print(f\"Cleaned: '{cleaned_text}'\")\n",
    "\n",
    "# Test phrase validation\n",
    "is_valid, reason = PhraseValidator.validate_phrase_match(\n",
    "    sample_case_law_term, sample_legislation_term, sample_text, sample_section\n",
    ")\n",
    "print(f\"\\nValidation result: {is_valid} - {reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Now try creating the JSONL again with the files created\n",
    "try:\n",
    "    created_files = create_batch_jsonl_for_phrase_extraction(\n",
    "        model_name=model_name,\n",
    "        prompt_file=prompt_file,\n",
    "        examples_file=examples_file,\n",
    "        df=test_df,\n",
    "        output_path=output_path\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Successfully created JSONL files:\")\n",
    "    for file_path in created_files:\n",
    "        print(f\"   - {file_path}\")\n",
    "        \n",
    "        # Show sample content\n",
    "        with open(file_path, 'r') as f:\n",
    "            first_line = f.readline().strip()\n",
    "            sample_data = json.loads(first_line)\n",
    "            user_content = sample_data['body']['messages'][-1]['content']\n",
    "            print(f\"   Sample user content: {user_content[:150]}...\")\n",
    "            \n",
    "            # Check if section_id is included\n",
    "            if 'section_id:' in user_content:\n",
    "                print(f\"   ‚úÖ section_id is included in the prompt\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå section_id is missing from the prompt\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating JSONL files: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
